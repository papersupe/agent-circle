[
  {
    "paper_id": "Agent0VL",
    "chunk_id": 0,
    "text": "Agent0-VL: Exploring Self-Evolving Agent for\nTool-Integrated Vision-Language Reasoning\nJiaqiLiu1 KaiwenXiong1 PengXia1 YiyangZhou1 HaonianJi1\nLuFeng1 SiweiHan1 MingyuDing1 HuaxiuYao1\n1UNC-ChapelHill\nAbstract and Vision-Language Models (VLMs) are trained using\nhuman-annotated preference data [20, 48, 57] or external-\nVision-languageagentshaveachievedremarkableprogress reward signals. However, such training paradigms are in-\ninavarietyofmultimodalreasoningtasks;however,their herently constrained by the limitations of human anno-\nlearningremainsconstrainedbythelimitationsofhuman- tators‚Äô preferences and the sparsity or incompleteness of\nannotatedsupervision. Recentself-rewardingapproaches environment-generatedfeedback,ultimatelycappingtheup-\nattempttoovercomethisconstraintbyallowingmodelsto per bound of the agent‚Äôs capabilities [54, 55]. To enable\nactastheirowncriticsorrewardproviders. Yet,purelytext- agentstomovebeyondstatichumansupervisionandachieve\nbasedself-evaluationstrugglestoverifycomplexvisualrea- continuousself-evolution,recentresearchhasexploredself-\nsoningstepsandoftensuffersfromevaluationhallucinations. rewarding learning [63, 81], in which the agent or model\nToaddressthesechallenges,inspiredbyrecentadvancesin itselfactsasaCriticorRewardModel,providingfeedback\ntool-integrated reasoning, we propose Agent0-VL, a self- andrewardsignalsforitsownlearning[9,55].\nevolvingvision-languageagentthatachievescontinualim- Nevertheless,formanycomplexvisualreasoningtasks,",
    "char_length": 1490
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 1,
    "text": "provementwithtool-integratedreasoning. Agent0-VLincor- purelytext-basedself-evaluationfacestwokeylimitations.\nporatestoolusagenotonlyintoreasoningbutalsointoself- First,limitedevaluationcapability: whenrelyingsolelyon\nevaluationandself-repair,enablingthemodeltointrospect, textualreflection,modelsstruggletoverifycomplexmulti-\nverify,andrefineitsreasoningthroughevidence-grounded stepcomputations,spatialreasoning,orprecisephysicaland\nanalysis. It unifies two synergistic roles within a single geometric calculations, abilities that are critical for tasks\nLVLM:aSolverthatperformsmulti-turntool-integratedrea- such as visual geometry and chart analysis [64]. Second,\nsoning,andaVerifierthatgeneratesstructuredfeedbackand unreliableevaluationprocess: excessivetextualreasoning\nfine-grained self-rewards through tool-grounded critique. causesmodelstorelyonlanguageshortcuts,bypassingfine-\nTheserolesinteractthroughaSelf-EvolvingReasoningCy- grainedvisualunderstandinganddependinginsteadonlin-\ncle,wheretool-basedverificationandreinforcementlearn- guisticpriorsorcontextualbias[27,81]. Thisoftenleads\ningjointlyalignthereasoningandevaluationdistributions toevaluationhallucination,wherethemodelincorrectlyre-\nfor stable self-improvement. Through this zero-external- wardsalinguisticallyplausibleyetvisuallyincorrectanswer,\nrewardevolution,Agent0-VLalignsitsreasoningandveri- orpenalizesavisuallycorrectanswerthatfailstoalignwith",
    "char_length": 1426
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 2,
    "text": "ficationbehaviorswithoutanyhumanannotationorexter- itslanguage-basedexpectations.\nnalrewardmodels, achievingcontinualself-improvement. Toaddressthesechallenges,inspiredbyrecentadvances\nExperimentsongeometricproblemsolvingandvisualsci- in tool-integrated reasoning [13, 48, 65, 76], we propose\nentific analysis show that Agent0-VL achieves an 12.5% a simple yet effective idea: enable the model to employ\nimprovementoverthebasemodel. Ourcodeisavailableat external tools not only during reasoning, but also during\nhttps://github.com/aiming-lab/Agent0. its own self-evaluation and self-repair. By doing so, the\nmodelcanperformclosed-loopself-improvementunderzero\nexternal reward supervision, learning to analyze, critique,\n1.Introduction and refine its reasoning in a verifiable manner. Building\nonthisidea,wepresentAgent0-VL,aself-evolvingvision-\nVision-languageagentshaveshownremarkablecapabilities languageagentframeworkthatintegratestool-useparadigms\nintacklingcomplexmultimodaltasks[54],includingrobotic into both the reasoning and self-evaluation processes. As\nmanipulation[21],visualquestionreasoning[80],andscien- illustratedinFigure1,Agent0-VLunifiesreasoning,verifica-\ntificdiscovery[28]. Currently,mostvision-languageagents tion,andself-repairwithinasingleLVLM,whichalternates\n5202\nvoN\n62\n]VC.sc[\n2v00991.1152:viXra\nBase Policy Self-Evolution Agent0-VL\nSolver Self-Repair Solver\n‚Ä¶\nEvaluation Verifier Verifier\n‚Ä¶\nIteration 1 Iteration N\nÔºàa) Ôºàb)\n)1@ssaP(ecnamrofreP\nQwen2.5-VL-7B",
    "char_length": 1486
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 3,
    "text": "Qwen2.5-VL-7B-TIR\n80 Agent0-VL-7B (Ours)\n+11.5% +12.2%\n75.6\n70 72.9\n67.2\n67.868.1\n+4.3% 65.0\n60 61.1\n+14.7% 58.659.6\n50 53.1\n46.347.2\n40\nMathVerse MMMU HallBench MathVista\nFigure1.TheevolveloopofAgent0-VLanditsperformancecomparison.TheleftpartillustratestheiterativeevolutionbetweentheSolver\nandVerifier,wheretheSolverprogressivelyrefinesreasoningstrategiesunderVerifierfeedback.Therightpartpresentsresultsshowingthat\nAgent0-VLoutperformstool-integratedreasoningmethodsacrossmultiplerepresentativebenchmarks.TIR:Tool-IntegratedReasoning.\nbetween two synergistic roles: (1) the Solver, which per- apartiallyobservableMarkovdecisionprocess(POMDP):\nformsmulti-turnreasoningandselectivelyinvokesexternal M = (S,A,O,T,R,Œ≥), where Œ≥ is the discount factor,\ntoolsforgroundedcomputationandvisualperception;and Ristherewardfunction,whichwillbeintroducedinsec-\n(2)theVerifier,whichvalidatesintermediatereasoningsteps tion3. Thewholemultimodalreasoningdynamicsandtool\nthroughgenerativecritiqueandtool-basedfeedback,gener- feedbackareexplicitlymodeledthroughthefollowingcom-\natingfine-grainedrewardsignalsandrepairinstructions. ponents:\nBy alternating between these roles, the model forms\nStateSpace.s ‚ààSdenotesthelatentmultimodalreasoning\nt\na closed feedback loop that supports continual self-\nstate,encodingthetextualreasoningcontext,visualfeatures,\nimprovement. Weformalizethisiterativelearningparadigm\nandpasttoolinputandoutputtraces.\nasaSelf-EvolvingReasoningCycle(SERC).Intheinner",
    "char_length": 1475
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 4,
    "text": "loop,themodelperformsmulti-turnreasoning,tool-based ActionSpace. Eachactiona t ‚ààAcanbeeither: (1)atex-\nverification,andselectiverepairtoprogressivelyrefineits\ntualreasoningstepa(text),or(2)astructuredtoolinvocation\nt\nreasoning trajectory. In the outer loop, we employ rein- a(tool),whichexecutesanexternalprogram,suchasPython\nt\nforcementlearning,specifically,GroupRelativePolicyOpti- code. Hence,A=A ‚à™A .\ntext tool\nmization(GRPO)[44],toupdatethesharedpolicy,jointly\nObservationSpace. Anobservationo ‚ààOcontainsfeed-\nenhancingreasoningandverificationcapabilitiesovertime. t\nbackfromexternaltoolsorenvironment,suchasreturned\nThisprocesstransformsthelearningobjectivefromstatic\nnumericalresultsortextualretrievals. Thetransitionfunc-\nreward maximization into a process of distributional self-\ntionT(s |s ,a ,o )captureshowstateevolvesgivengen-\nconsistency,wherereasoningandevaluationbehaviorsare t+1 t t t\neratedresponseanditscorrespondingtoolfeedback.\njointlyalignedandcontinuallyoptimized.\nOur main contribution is Agent0-VL, a unified self- Trajectory. Given an input x = (I,q), the\nevolvingLVLMthatintegratesreasoning,verification,and model generates a multimodal trajectory: œÑ =\nself-repair into a single model trained entirely from zero {(s 1 ,a 1 ,o 1 ),(s 2 ,a 2 ,o 2 ),...,(s T ,a T ,o T )}, where each\nexternalrewards. Experimentsongeometricreasoningand transitionencodesonereasoning-tool-feedbackinteraction.\nvisual scientific analysis demonstrate that Agent0-VL-7B",
    "char_length": 1473
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 5,
    "text": "achieves stable multi-iteration performance improvement, 3.Methodology\nshowinganaverage12.5%improvementovertheQwen-VL\nbasemodel. Furthermore,whenusedindependentlyasapro- In this section, we introduce Agent0-VL, a self-evolving\ncessrewardmodel,Agent0-VLalsoimprovesthemodel‚Äôs vision-languageagentthatintegratestool-useparadigmsinto\ntest-timescalingperformancebyanaverageof7.3%. boththereasoningandself-evaluationprocesses. Thecore\nideaistoenableaVLMtonotonlyreasonandsolveprob-\n2.Preliminaries lems,butalsotoverify,critique,andrepairitsownreasoning\ntrajectoriesthroughaunified,self-evolvingloop. Wefirstde-\nMulti-turnTool-IntegratedReasoning(TIR)agentstrained scribethedual-rolearchitecturecomposedofaSolveranda\nviaReinforcementLearning(RL)presentsubstantialchal- Verifier,thenpresenthowthemodelperformstool-grounded\nlenges,primarilyduetotheinherentlycompositionalnature reasoning,verification,andconfidence-gatedself-repair. Fi-\nofreasoningandthepartialobservabilityofmultimodalen- nally,wedetailtheSelf-EvolvingReasoningCycle(SERC),\nvironments. To address these complexities, we formulate wheretheserolesinteractthroughinnerandouteroptimiza-\nthereasoningprocessofouragentwithintheframeworkof tionloopstoachievecontinualself-improvement.\nVerifier\nAgent0-VL\nInput\nQuery + Image ReasoningOutput\nStep 1 Step 2 ‚Ä¶ Step n-1 Step n Answer\nSolver Evaluate\nTool calling Sandbox\nThink\nTool List\nStep 1 Step 2 ‚Ä¶ Step n-1 Step n Answer Score 1 Score2 ‚Ä¶Score n-1 Score n Answer Score\nTool calling Tool calling",
    "char_length": 1497
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 6,
    "text": "Critique Score Confidence\nùëú = ùëáùëúùëúùëô(ùëé)\n# #\nSelf-Repair\nCode Sandbox Fail\nVerification Decision\nùúè = { ùë† ,ùëé ,ùëú ,‚Ä¶,(ùë† ,ùëé ,ùëú )}\n! ! ! \" \" \"\nPass\nPolicy\nUpdate\ng(ùúè) = ùõº!¬∑ùëü\"#$ + ¬∑ùõ¥$(ùõº%.ùëü&'()*++ ùõº,¬∑ùëü$\"\"- ‚àí ùõº.¬∑ùê∂)\nReinforcement Optimization\nReward ‚Ä¢ Tool reward ùëü!\"\"# ‚Ä¢ Verify reward ùëü%&'()*\n‚Ä¢ Outcome reward ùëü\"$!ùúè ‚Ä¢ Cost penalty ùê∂'&+,('\nFigure2.TheFrameworkofAgent0-VL.TheunifiedpolicyœÄ alternatesbetweentwointernalroles:theSolverthatgeneratesreasoning\nŒ∏\ntrajectorieswithtoolcalls,andtheVerifierthatperformsgenerativeverificationusingtoolfeedbacktoproducecritiquesandstep-wise\nrewards.TheserolesarejointlyoptimizedthroughtheSelf-EvolvingReasoningCycle,whereself-generatedrewardsguidepolicyupdates\nviaRL.\n3.1.UnifiedSolver-VerifierArchitecture triplet(s ,a ,o ),theVerifieroutputsafeedbacktuple:\nt t t\nTo achieve autonomous evolution, as shown in Figure 2,\nV =(score ,conf ,critique ),\nAgent0-VLadoptsaunifieddual-roledesign,whereasingle t t t t\nLVLM alternates between two internal modes: a Solver where score ‚àà [‚àí1,1] measures factual correctness,\nt\nthatperformstool-integratedreasoning,andaVerifierthat conf ‚àà [0,1] estimates epistemic certainty, and critique\nt t\nintrospectivelyevaluates,critiques,andrepairstheSolver‚Äôs providesnatural-languagereflectiondescribingpotentialrea-\noutputs. Wedetailthearchitectureasfollows: soningflaws. TheVerifiercanalsore-invoketoolstocross-\nUnifiedPolicyFormulation. WedefineasharedpolicyœÄ checkcorrectness,providingagroundedandinterpretable\nŒ∏",
    "char_length": 1472
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 7,
    "text": "thatgovernsbothrolesthrougharoleindicatorm‚àà{S,V}: signalforself-evaluation.\n(cid:40) 3.2.Tool-GroundedVerificationandSelf-Repair\nœÄS(a |s ), m=Solver,\nœÄ (a |s ,m)= Œ∏ t t (1)\nŒ∏ t t œÄV(a |s ,a ,o ), m=Verifier, Withinthedual-roleframework,theSolverperformsmulti-\nŒ∏ t t t t\nturnreasoningbyinvokingexternaltools,whiletheVerifier\nwhere s denotes the multimodal state, a the generated provides process-level feedback and evaluation based on\nt t\nreasoningaction,ando thetoolorenvironmentfeedback. the Solver‚Äôs reasoning trajectory and outputs. If the Veri-\nt\nfieridentifiesdeficienciesorinconsistenciesintheSolver‚Äôs\nSolver. Whenm = S,themodelperformsmulti-turnrea-\nreasoning, it initiates a self-repair process that leverages\nsoningandselectivelyinvokesexternaltools. Theresulting\nself-evaluation signals to revise and refine the reasoning\nobservationso areincorporatedintothecontext,allowing\nt\ntrajectory,therebyimprovingreasoningaccuracy.\nthe agent to iteratively refine its reasoning with grounded\nSpecifically, Agent0-VL introduces a structured tool-\nevidence. Formally,theSolverfollows\ngrounded generative verification mechanism designed to\na ‚àºœÄS(a |s ), b =f(b ,o ), producedenseandinterpretablefeedbacksignalsforrein-\nt Œ∏ t t t+1 t t\nforcement learning. At each step t, the Verifier produces\nwhereb representsthelatentbeliefstateencodingaccumu-\nt V . Duringverification,theVerifiercanqueryexternaltools\nt\nlatedreasoningcontextandmultimodalinformation.",
    "char_length": 1451
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 8,
    "text": "toobtainfactualevidence. Bycombininglanguage-based\nVerifier. Whenm=V,themodelswitchesintoagenerative reflectionwithexecutabletool-derivedevidence,themodel\nverificationmodetoevaluateeachreasoningstep. Giventhe transformsverificationfromastaticcorrectnesscheckintoa\nAlgorithm1Agent0-VLTrainingProcess Basedontheaboveverificationandself-repairprocesses,\nRequire: Unified policy œÄ , reference policy œÄ , confi- theresultingstep-wiserewardisthendefinedas:\nŒ∏ Œ∏old\ndencethresholdœÑ c . r =r(t) ‚àíg ¬∑C(t) , (4)\n1: InitializeœÄ Œ∏ withsuperviseddatatolearntoolusageand t proc t repair\nverificationformats. whereC(t) penalizesunnecessaryrepairs.\nrepair\n2: foreachiterationk =1,...,N iter do 3.3.Self-EvolvingReasoningCycle\n3: //InnerLoop: GenerationandSelf-Evaluation\n4: foreachtaskx i =(I i ,q i )do Having defined the agent‚Äôs architecture and verification\n5: InitializetrajectoryœÑ i ‚Üê‚àÖ,states 1 . mechanisms,wenowintroducethemodeltrainingprocess\n6: fort=1,...,T do (seeAlgorithm1forthewholeprocess). Specifically, the\n7: Sample action a t ‚àº œÄ Œ∏ (¬∑|s t ,m = S) and exe- trainingprocessisdividedintotwostages.First,asupervised\ncutetogeto . fine-tuning (SFT) stage for cold-start initialization, ensur-\nt\n8: Generate verification V t = ingthemodellearnspropertoolusageandself-evaluation\n(score ,conf ,critique )‚àºœÄ (¬∑|s ,a ,o ,EE). formats. Second,aRL-basedself-evolvingreasoningcycle\nt t t Œ∏ t t t\n9: Computeprocessrewardr p (t ro ) c (Eq.2). (SERC)thatoperationalizesthisarchitecture,enablingthe",
    "char_length": 1482
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 9,
    "text": "10: ifconf t <œÑ c then agenttolearnfromitsowntool-groundedfeedback. SERC\n11: Generate repair ‚àÜ t = f Œ∏ (s t ,a t ,V t ); re- consistsoftwonestedloops: aninnerloopfordatagenera-\nsamplea‚Ä≤ ‚àºœÄ (¬∑|s ,‚àÜ ,RA). tionandanouterloopforpolicyevolution.\nt Œ∏ t t\n12: endif Theinnerloopisresponsibleforgeneratingexperience\n13: Computeeffectivestepreward(Eq.4). bycouplingreasoningandverification. Foragivenmulti-\n14: endfor modaltaskx,theSolverfirstgeneratesacompletereasoning\n15: Computetotaltrajectoryreturng(œÑ i )(Eq.5). trajectoryœÑ = {(s t ,a t ,o t )}T t=1 ,invokingexternaltoolsas\n16: endfor needed. Immediatelyfollowingthis,theVerifierre-evaluates\n17: //OuterLoop: PolicyUpdate eachsteptofthetrajectorytoproducetheverificationtuple\n18: OptimizeœÄ Œ∏ viaGRPOusingallcollectedtrajectories V t andthecorrespondingprocessrewardr p (t ro ) c (usingEq.2).\n(Eq.6). IftheVerifier‚Äôsconfidenceforastepisbelowthethreshold\n19: endfor œÑ c ,theselectiverepairmechanism(Eq.4)istriggered,and\nacostC isapplied. Finally,thetotalreturng(œÑ)forthe\nrepair\ntrajectoryisaggregated,integratingboththefinaloutcome\ndynamicevaluationprocedure,enablingiterativerefinement rewardr andtheaccumulatedstep-wiseprocessrewards:\nout\nofitsreasoningtrajectories.\nT\nBasedontheVerifier‚Äôsstep-wiseassessments,wefurther (cid:88)\ng(œÑ)=Œ± r + Œ≥t‚àí1r , (5)\ndesignacorrespondingprocess-levelrewardthatintegrates out out t\nt=1\nsemantic reliability, tool-based validation, and cross-role\nwhereŒ± balancesthetworewardtypesandŒ≥ isthedis-\nregularization. out",
    "char_length": 1495
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 10,
    "text": "countfactor.\nr(t) =Œª ¬∑r(tool )+score ¬∑conf Theouterloopthenusesthesegeneratedtrajectoriesto\nproc tool t t t\n(cid:124) (cid:123)(cid:122) (cid:125) evolve the unified policy œÄ . We employ Group Relative\nsemanticreliability (2) Œ∏\nPolicyOptimization(GRPO)[44],avariantofPPOdesigned\n‚àíŒ≤ div ¬∑D KL (cid:0) œÄ Œ∏ V‚à•œÄ Œ∏ E(cid:1) , forgenerativetasks. ForagroupofGtrajectories{œÑ }G\ni i=1\nsampledunderthecurrentpolicy,wecomputeanormalized\nwhereŒª scalestool-basedcorrectness,andŒ≤ stabilizes\ntool div\nadvantageforeachtrajectory:\nthedual-roledistributionalalignment.\nAftercompletingself-verification,theVerifierdetermines g(œÑ )‚àímean(g ,...,g )\nAÀÜ = i 1 G ,\nwhether the model should perform self-repair to correct i std(g ,...,g )+Œµ\n1 G\nits reasoning process based on the confidence score conf\nt whereg =g(œÑ )andŒµisanormalizationconstant. Thisrel-\nobtained during verification. Let œÑ denote a confidence i i\nc ativeadvantageAÀÜ reframesthelearningobjectiveas‚Äùbeing\nthreshold. Therepairgateatsteptisdefinedas: i\nbetterthanthegroupaverage.‚ÄùThepolicyisthenoptimized\ng =œÉ (cid:0) Œ∫(œÑ ‚àíconf ) (cid:1) , (3) toincreasethelikelihoodoftrajectorieswithpositiveadvan-\nt c t\ntageswhilemaintainingstabilitythroughaKLregularization\nwhereœÉ(¬∑)isthesigmoidfunction,andŒ∫controlsthegating term:\ntemperature. When g is activated, the Verifier issues a (cid:104) (cid:16) (cid:17)(cid:105)\nt L =‚àíE min œÅ AÀÜ , clip(œÅ ,1‚àíœµ,1+œµ)AÀÜ\nlocalrepairinstruction‚àÜ t =f Œ∏ (s t ,a t ,V t ),andtheSolver EDLP i i i i i (6)",
    "char_length": 1471
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 11,
    "text": "regeneratesacorrectedsegmenta‚Ä≤ t ‚àºœÄ Œ∏ (¬∑|s t ,‚àÜ t ,m=S). +Œ≤ KL D KL (cid:0) œÄ Œ∏ ‚à•œÄ Œ∏old (cid:1) ,\nwhereœÅ = œÄŒ∏(œÑi) istheimportancesamplingratio.\ni œÄŒ∏old (œÑi)\n4.Experiments\nInthissection,weevaluateAgent0-VLacrossmultiplevi-\nsual reasoning benchmarks to answer the following ques-\ntions:(1)Canitimprovereasoningabilityoverexistingopen-\nsourcebaselines? (2)Doesrepeatedself-evolutionthrough\nmultipleiterationsyieldconsistentperformancegains? (3)\nHoweffectivearetheproposedcomponents? and(4)Can\nAgent0-VLserveasaprocessrewardmodeltoenhanceother\nLVLMs?\n4.1.ExperimentalSetup\nBenchmarks. Toevaluatetheeffectivenessofourmethod,\nwe conducted experiments on a set of seven benchmarks.\nThe science and mathematical benchmarks include Math-\nVerse[74],MathVision[53],MathVista[31],WeMath[40],\nandMMMU[70],whileotherbenchmarksconsistofHallu-\nsionBench[16],andChartQA[32].\nBaselines. Additionally,wecomparedAgent0-VLagainst\nthreetypesofbaselines: (1)Closed-SourceLVLMs,includ-\ning GPT-4o [1], o1 [35], Gemini-2.0-pro [7], and Claude-\n3.7-Sonnet[2]; (2) Open-Source General MLLMs, includ-\ning Qwen2.5-VL-3B [3], Qwen2.5-VL-7B [3], Qwen2.5-\nVL-32B[3],InternVL-2.5-8B[6]andInternVL3-8B[84];\n(3) Open-Source Reasoning MLLMs, including R1-\nVL-7B [71],Vision-R1-7B [20], R1-Onevision-7B [66],\nOpenVLThinker-7B [8], VLAA-Thinker-7B [4], MM-\nEureka-7B [33], Thyme [76], Qwen3-VL-8B [49], and\nThinkLite-VL-7B[58]; and(4)self-evolvingmethods, in-\ncludingEvoLMM[51],VisPlay[18],andVision-Zero[55].",
    "char_length": 1472
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 12,
    "text": "TrainingDatasets. Weconstructalarge-scalemultimodal\nreasoningcorpustailoredforbothSFTandRLstages. The\nSFTdataset(200ksamples)isbuiltfromopen-sourcebench-\nmarksincludingGeometry3K[30],GeoQA[5],Mulberry\ndataset[68],MM-Eureka[33],andRetool[11],wherewe\nautomaticallygeneratetool-augmentedreasoningtrajecto-\nriesusingGPT-5andQwen2.5-VL-72Basteachermodels.\nFor the RL stage, we construct an additional 40k dataset.\nDetaileddataconstructionproceduresareprovidedinAp-\npendixD.\n4.2.MainResults\nOverall Performance. Table 1 summarizes the perfor-\nmanceofAgent0-VLacrossallvisualreasoningbenchmarks.\nOurmodelconsistentlyachievessubstantialimprovements\noverallopen-sourcebaselines. Specifically,Agent0-VL-7B\noutperformsexistingopen-source7Bmodels,achievinga\n4.29%averagegainoverThinkLite-VL-7B.Agent0-VLalso\ndemonstratesclear advantages overexisting self-evolving\nvision‚Äìlanguage models, including Vision-Zero [55] and\nEvoLMM[51]. ComparedtothebaseQwen2.5-VL-7B,it\nshows a 12.5% improvement, and a 10.3% gain over its\necnamrofreP\nllarevO\nPass @1\nBoN with Qwen2.5-VL-7B\nBoN with Agent-Zero-VL-7B 69.1 70\n66.9\n64.4\n62.8\n60 58.359.5 57.2\n55.3\n53.6 53.0\n51.2\n50 50.0\nQwenVL2.5-3B QwenVL2.5-7B InternVL-2.5-8B Qwen2.5-VL-32B\nFigure 3. The overall Best-of-8 evaluation results across seven\nmultimodal reasoning benchmarks with different critic models.\nOur model greatly enhances the overall performance compared\nwithQwen2.5-VL-7Bmodel.\ntool-integratedvariantQwen2.5-VL-7B-TIR.Evenwitha",
    "char_length": 1468
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 13,
    "text": "stronger base model (Qwen3-VL-8B), Agent0-VL main-\ntainsstrongcompatibilityandfurtherimprovesperformance,\nsurpassingthebaseby6.1%anditstool-augmentedvariant\n(Qwen3-VL-8B-TIR)by4.6%.Notably,Agent0-VL-8Balso\noutperformsclosed-sourcesystemssuchasGPT-4oonkey\nbenchmarksincludingMathVista,HallBench,andChartQA,\nunderscoringthegeneralizationandreasoningstrengthof\nourapproach.\nPerformanceAcrossTaskDomains. Indomain-specific\nevaluations,Agent0-VLdemonstratesthemostsignificant\nimprovementsonmathematicalreasoningbenchmarkssuch\nasMathVistaandWeMath,wheretool-groundedexecution\nandverificationplayacrucialroleinaccuratesymbolicrea-\nsoning. Our 7B and 8B models achieve 18.1% and 7.4%\nimprovements,respectively,overtheirbasemodelsonmath-\nrelatedbenchmarks. Forperception-heavybenchmarkssuch\nasHallusionBenchandChartQA,integratingtheVerifier‚Äôs\nfactualgroundingsubstantiallyreducesvisualhallucinations,\nleadingto12.2%and3.1%improvementscomparedtothe\nbasemodel. TheseresultsindicatethatAgent0-VLenhances\nbothsymbolicreasoningandvisualunderstanding,confirm-\ningtherobustnessandgeneralityofourframeworkacross\ndiversetaskdomains.\nIterative Self-Evolution. We further investigate how the\nmodel‚Äôsreasoningcapabilitiesevolveacrossmultipleiter-\nationsofSERC.AsshowninTable2,Agent0-VLdemon-\nstratesasteadyandmonotonicimprovementovertime: in\nthefirstiteration,itsoverallperformanceincreasesby5.2%\ncomparedtothebasemodel,followedbyadditionalgains\nof4.0%and2.8%inthesecondandthirditerations,respec-",
    "char_length": 1476
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 14,
    "text": "tively. Theseresults validate the effectiveness of ourself-\nevolving framework, showing that the model can achieve\nstableandcontinuousperformancegainsthroughiterative\nself-improvement.\nTable1. Comparisonofmodelperformanceacrossrepresentativevisualreasoningbenchmarks. Closed-sourceLVLMsarelistedfor\nreference, while open-source models are grouped by general-purpose and reasoning-oriented categories. The best results among all\nopen-sourcedmodelsareboldedandthesecondbestresultsareunderlinedinthetable.Note:Qwen2.5-VL-7B-TIRandQwen3-VL-8B-TIR\ndenotetheresultsofintegratingtool-enhancedreasoningintothecorrespondingbasemodels,obtainedfromourlocalfine-tuningand\nevaluation.\nModel MathVerse MathVision MathVista WeMath HallBench ChartQA MMMU Avg.\nval\nClose-sourceMLLMs\nGPT-4o[1] 50.8 30.4 63.8 68.8 55.0 85.7 69.1 60.5\nOpenAI-o1[35] 57.0 60.3 73.9 - - 83.1 77.6 -\nClaude-3.7-Sonnet[2] 52.0 41.3 66.8 72.6 55.4 56.5 75.0 59.9\nOpen-SourceGeneralMLLMs\nInternVL-2.5-8B[6] 39.5 19.7 64.4 53.5 61.7 79.1 62.7 54.4\nInternVL-3-8B[84] 39.8 29.3 71.6 58.1 64.3 85.9 60.7 58.5\nQwen2.5-VL-7B[3] 46.3 25.1 67.8 62.1 65.0 83.5 58.6 58.3\nQwen2.5-VL-7B-TIR‚àó 47.2 26.3 68.1 63.7 67.2 84.1 59.6 59.5\nQwen3-VL-8B[49] 62.1 53.9 77.2 72.5 72.1 84.6 69.6 70.3\nQwen3-VL-8B-TIR‚àó 63.1 54.7 79.4 73.1 72.8 85.4 70.9 71.3\nOpen-SourceReasoningMLLMs\nR1-VL-7B[71] 40.4 24.7 63.5 60.1 54.7 76.1 - -\nVision-R1-7B[20] 51.9 30.7 73.5 73.9 68.8 79.8 50.5 61.3\nR1-OneVision-7B[66] 46.4 29.9 64.1 61.8 67.5 77.8 - -",
    "char_length": 1476
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 15,
    "text": "OpenVLThinker-7B[8] 45.7 26.3 71.2 66.7 70.2 78.4 - -\nVLAA-Thinker-7B[4] 52.7 29.2 69.7 70.2 68.2 80.1 - -\nMM-Eureka-Qwen-7B[33] 50.5 27.9 73.6 67.4 66.9 82.1 52.7 60.2\nThinkLite-VL-7B[58] 52.1 32.9 75.1 69.3 70.9 84.8 55.5 62.9\nThyme-VL-7B[76] 51.3 27.6 70.0 - 71.0 86.1 - -\nEvoLMM[51] 44.9 27.8 70.5 86.7 52 -\nVisPlay[18] - 31.2 - - 92.3 - - -\nVision-Zero[55] 52.2 28.1 72.6 - - - - -\nAgent0-VL-7B(Ours) 53.1 37.3 75.6 71.7 72.9 87.3 61.1 65.6\nAgent0-VL-8B(Ours) 65.5 56.2 83.7 79.6 74.3 89.7 73.4 74.6\nTable2.PerformancecomparisonofAgent0-VL-7Bacrossiterativetrainingstages.Eachiteration(Iter1-3)progressivelyrefinesreasoning\nandtool-groundedcapabilities,consistentlyoutperformingthebasemodelacrossallbenchmarks.\nModelName MathVerse MathVision MathVista WeMath HallBench ChartQA MME-Real MMMU Avg.\nBaseModel 46.3 25.1 67.8 62.1 65.0 83.5 58.3 50.6 57.3\nIter1 48.4 29.6 69.2 66.8 67.9 84.7 63.9 53.7 60.5\nIter2 51.1 35.3 72.8 70.1 70.3 86.1 64.7 58.3 63.6\nIter3 53.1 37.3 75.6 71.7 72.9 87.3 65.3 61.1 65.5\nTable3.AblationstudyofAgent0-VLondifferentbenchmarks.RemovingSelfRepair,Tool-GroundedReward,orSERCnotablydegrades\nperformance,highlightingthecomplementaryrolesofthesemodules.\nSetting MathAvg. HallBench ChartQA MME-Real MMMU\nAgent0-VL-7B 59.4 72.9 87.3 65.3 61.1\nw/oSelfRepair 57.5 71.6 86.1 64.1 57.9\nw/oToolUse 53.1 67.5 86.2 61.9 54.7\nw/oSERC(SFTonly) 51.8 65.8 85.4 60.5 52.5\nQwen2.5-VL-7B(BaseModel) 50.3 65.0 83.5 58.3 50.6",
    "char_length": 1438
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 16,
    "text": "4.3.AblationStudies learning(ii)ToolUse,and(iii)SelfRepair. Foreachabla-\ntion,weremovethecorrespondingmodulewhilekeeping\nTounderstandthecontributionofeachmajorcomponentin\nallothercomponentsintact,andretrainthemodelunderthe\nAgent0-VL,weconductcontrolledablationexperimentsfo-\nsamesettings. TheresultsaresummarizedinTable3.\ncusingonthreekeymodules:(i)theSERCforreinforcement\nWereporttheresultsinTable3. Accordingtotheresults, tionalLVLMsthatstopafterproducinganincorrectanswer,\nfirst, we observe that eliminating the outer reinforcement Agent0-VLcontinuestointrospectandrepairitsreasoning.\nlearning loop (SERC) and relying solely on SFT training Inthisexample,theSolverinitiallymisinterpretsthe‚Äúblind\nleadstothemostsignificantperformancedrop,withanaver- spot‚Äùsegment,leadingtoanincorrectresult. TheVerifier\nagedecreaseof8.7%acrossallbenchmarks. Thisresultcon- detects this logical error using tool-grounded verification\nfirmsthatthebidirectionalinteractionbetweenreasoningand andprovidesstructuredfeedbackpinpointingthefaultystep.\nevaluationisessentialforenablinglong-termself-evolution, TheSelf-Repairmodulethensynthesizesacorrectivepatch,\nwhichconstitutesoneofthecorecontributionsofthiswork. whichtheSolverreusestoregenerateavalidreasoningchain\nSecond,whentoolusageisremovedandthemodelperforms andreachthecorrectanswer. Thiscasedemonstrateshow\nonlytext-basedreasoningandself-evaluation,theaverage Agent0-VLconvertserrorsintosuccessfulreasoningthrough",
    "char_length": 1451
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 17,
    "text": "performancedeclinesby6.5%. Thishighlightsthecrucial itsintegratedreasoning‚Äìevaluation‚Äìrepairloop.\nroleoftoolintegrationinenhancingreasoningaccuracy,es-\npeciallyformathematicalandfact-verificationtasks. Finally, 5.RelatedWork\nremovingtheself-repairmechanism,allowingthemodelto\nself-evaluatewithoutperformingexplicitcorrection,results Self-EvolvingMethods.Toreducetherelianceoncostlyhu-\ninamoderateaverageperformancedropof2.5%. Inpartic- mansupervision,recentresearchinbothLLMsandVLMs\nular,formathematicalreasoningbenchmarks,re-executing has explored self-evolving techniques that enable models\nreasoningthroughselectiverepairprovidesadditionaloppor- to improve autonomously using unlabeled data and self-\ntunities for resampling and correction, thereby improving generatedfeedback[62]. InLLMs,acommonstrategyisto\noverallreliability. derivepseudo-rewardsfromthemodel‚Äôsownoutputs.Forex-\nample,self-consistencyleveragesagreementamongmultiple\n4.4.PerformanceasaProcessRewardModel\ngeneratedsolutionsasalearningsignal[56],whichhasen-\nTofurtherevaluatethegeneralizationandstandaloneutility hancedreasoningintaskslikemathandcodegeneration,as\nof the Verifier module, we deploy it as a Process Reward seeninTTRL[85],MM-UPT[59],andSRT[43]. MM-UPT\nModel(PRM)toassessreasoningtrajectoriesgeneratedby furtherimprovesspatialreasoningbygeneratingsynthetic\nother VLMs. This experiment serves two key purposes: geometryproblems. Otherapproachesuseastrongmodel",
    "char_length": 1441
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 18,
    "text": "(i)toexaminewhethertheVerifiercangeneralizebeyond asanautomatedevaluator[38],orrelyoninternalcuessuch\nAgent0-VLandreliablyevaluatethestep-levelcorrectnessof asconfidencescores[25,78]andoutputentropy[73]. Deep-\nexternalmodels‚Äôoutputs,and(ii)tovalidatetheeffectiveness Conf [12], for instance, enhances reasoning efficiency by\nofourtool-groundedrewardmodelingapproach,evenwhen selectingresponsesbasedontokenentropy.\ndecoupledfrompolicylearning. In the VLM domain, self-evolving agents are also pro-\nAsshowninFigure3,ourverifiersignificantlyimproves gressingrapidly. ViPER[72]proposesatwo-stagecoarse-\nBest-of-8(BoN)performancewhenusedasarewardscorer to-fine training framework that integrates instance- and\nacrossvariousLVLMs. ComparedtoQwen2.5-VL-7Bas image-levelreconstructionwithself-critiquingandpredic-\nthe critic model, Agent0-VL consistently yields stronger tion. In contrast, Vision-Zero [55] employs a domain-\ntrajectoryselectionacrossmodelsofdifferentscales,from agnostic, game-based self-play strategy. By alternating\nQwen2.5-VL-3B up to Qwen2.5-VL-32B, demonstrating between self-play and reinforcement learning with verifi-\nmoreeffectiverewardassignmentandsharperstep-leveldis- able rewards, it achieves scalable and sustained improve-\ncrimination. Table4furtherhighlightstheeffectivenessof ment. ThesemethodsdemonstratethatVLMs,likeLLMs,\nourmodel,showingsubstantialimprovementsinbothstep- canevolvethroughstructuredfeedbackloopsinvolvingun-",
    "char_length": 1453
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 19,
    "text": "levelrewardcorrelationandoverallaccuracyacrossseven certaintymodeling,taskgeneration,andcurriculum-based\nmultimodal reasoning benchmarks. When integrated as a learning. Incomparison,ourworkbuildsontheseadvances\nPRMintodifferentopen-sourceLVLMs,Agent0-VLcon- byintroducingatool-integratedvisualreasoningandself-\nsistentlyenhancesreasoningstabilityandfactualgrounding, evaluationframework,enablingcontinualandstableperfor-\nyieldinganaveragegainof7.3%.Notably,evensmallermod- manceimprovements.\nelssuchasQwen2.5-VL-3Bbenefitsignificantlyfromthe\nTool-Integrated Reasoning. Tool-Integrated Reasoning\nstructuredfeedback,indicatingstronggeneralizationacross\n(TIR) aims to enable models to invoke external tools\narchitecturesandmodelscales.\n(e.g., search engines [14, 37, 50], calculators [15, 52])\nto overcome knowledge limitations and execute complex\n4.5.CaseStudy\ntasks[13,17,42,69,82,83]. Subsequentworksignificantly\nTo illustrate how Agent0-VL performs self-evolving rea- enhancedLLMcapabilitiesinmulti-toolcoordination,plan-\nsoning, we present a representative visual reasoning case ning,andexecutionthroughinstruction-tuningandagentic\ninFigure4(fullversioninAppendix4.5). Unlikeconven- frameworks[22,26,39,41]. Recentresearchbegantoex-\nTable4. PerformanceoftheAgent0-VLasaPRM.‚Äú+Ours‚ÄùdenotesmodelsintegratedwiththeAgent0-VLforrewardscoring. Across\ndiversemultimodalreasoningbenchmarks,ourmethodconsistentlyenhancesaccuracy,validatingitseffectivenessasageneralizable,\ntool-groundedrewardmodel.",
    "char_length": 1491
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 20,
    "text": "Model MathVerse MathVision MathVista WeMath HallBench ChartQA Overall\nQwen2.5-VL-3B[3] 34.8 21.9 58.4 51.7 59.8 73.1 50.0\n+Ours 38.9 26.1 65.8 54.2 61.2 75.2 53.6\nQwen2.5-VL-7B[3] 46.3 25.1 67.8 62.1 65.0 83.5 58.3\n+Ours 51.2 33.6 72.3 66.9 68.1 84.6 62.8\nInternVL2.5-8B[6] 39.5 19.7 64.4 53.5 61.7 79.1 53.0\n+Ours 43.2 26.2 67.3 59.8 63.6 83.0 57.2\nInternVL2.5-8B[6] 39.8 29.3 71.6 58.1 64.3 85.9 58.2\n+Ours 45.4 33.0 74.6 62.5 67.2 88.3 61.8\nQwen2.5-VL-32B[3] 48.5 38.4 74.7 69.1 71.8 84.0 64.4\n+Ours 53.0 44.3 78.6 75.2 74.7 88.5 69.1\nPhase 1: Initial Reasoning Phase 2: Evaluation\n< d $ .. 2 t u O h 7 e n i 0 n e ^ k E p \\ > c a o T i s s r h c t s e $ ib , g ( l p e o e o a n in s l d t i i i t e s n iv r g t p e o r a e c t t x a a d - l t a c u io x u e n i l s a N ) i t s e o t ‚Ä¶ r c h t o a T h u h t ( n e ‚Ä¶ p t d o e e s r- i s c t c i l v o r e i c p k t y i w - o a n is x i e \" s f ) r \" o b m is y c i { { s \" \" r s i s Q t t t i e c 4 e p a p ( _ $ l _ i l i x n o n d > g d e i e 0 c x x , \" e \" y : : r 2 1 < r , o , * 0 r \" * . $ s } T c ) . h o \", e r e \" d t \" o e :- o f 1 i l n . _ 0 i c t , i h o \" e c n c o o k n f \" f : ' i f b d a l e i l n s n e d c } e s \" p : o 1. t 0 ' i , s \" w cr r i o ti n q g u ‚Ä¶ e\" T : E \" h R r e r e r o a e r s fo o F r n o e i , n u t g n h c d e o t n ru ta e i n b s li n a d spot\n3.I need to (a) Check if segment A-B enters Q2. * (b)\nCheck if segment B-C enters Q2. * (c) Sum the results.",
    "char_length": 1455
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 21,
    "text": "(a) Segment A-B: ‚Ä¶. (b) Segment B-C: From $B(7, -1)$ Phase 3: Self-Repair\n[Q4] to $C(-4, -3)$ [Q3]. Both points have $y < 0$. EE Key Feedback:‚Ä¶<think> 1. Diagnosis:*EE has assigned a score ... 2. Root Cause: RA's\nTherefore, the entire segment is in the $y < 0$ region definition of the \"blind spot\" was incorrect. 3. EE Correction: ‚Ä¶4. Repair Strategy: I must issue\nand **cannot enter Q2**. Length in blind spot is $0$. a `PATCH` to correct the core logical premise‚Ä¶</think>\n(c) Total length = $0 + 0 = 0$. </think> { \"action\": \"PATCH\", \"target_step\": 1, \"patch_type\": \"text\",justification\": \"Original logic\n<answer>0.00 </answer> Failed Attempt incorrectly‚Ä¶}\nQuestion:As shown in the figure, a coast guard\nwatchtower is located at the coordinate origin Phase 4: Corrected Reasoning\nO(0, 0).Three key reef locations, A, B, and C, <think> Goal: Restart reasoning based on the `PATCH`. 2. Corrected Premise: Blind Spot = Quadrant 4 (Q4, $x > 0, y < 0$). 3. New Plan‚Ä¶</think>\nare marked on the nautical grid...Calculate {\"tool_name\":\"calc_intersection_and_distance\", \"tool_input\":..} √† [Tool Output]\nhow much of the ship's total journey (segments <think> **Integrate (a):** Tool returns length of A-B in Q4 as **$1.01015$**. 6. **Execute (b) -‚Ä¶* I will call the tool.\nA-B and B-C) is sailed within the watchtower's </think>{\"tool_name\":\"calc_intersection_and_distance\", \"tool_input\":..} √† [Tool Output]",
    "char_length": 1400
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 22,
    "text": "blind spot. < < t a h n i s n w k> e r * > * 8 In .1 te 3 g < r / a a te n s (b w ) e :* r * > Tool returns length of B-C in Q4 as **$7.11601‚Ä¶</think> Correct\nFigure4.SimplifiedillustrationofAgent0-VL‚Äôsself-evolvingreasoningprocessonageometricreasoningtaskduringtrainingphase.The\nmodelfirstproducesanincorrectanswer(Phase1),afterwhichtheVerifieridentifiesthelogicalerror(Phase2),triggersSelf-Repairto\ngenerateacorrection(Phase3),andfinallyre-executesreasoningviatheSolvertoreachthecorrectsolution(Phase4). Thecomplete\nmulti-phasecaseisprovidedinAppendix4.5(Figure8).\ntendTIRtoVLMs,enablingmodelstonotonlyprocesstext thatguidemoreeffectiveself-improvement.\nbutalsodynamicallyinteractwithvisualinformation. One\nlineofworkpositionstheVLMasanorchestratorthatcalls 6.Conclusion\nuponexternalvisionexperttools[19,67]orskillreposito-\nWepresentedAgent0-VL,aself-evolvingvision‚Äìlanguage\nries [29]. Another line of work explores how VLMs can\nagent that unifies reasoning, verification, and self-repair\nperformdynamicreasoningatthe‚Äúvisionlevel,‚Äùtreatingim-\nwithinasinglemodel. Throughitsdualroles,theSolverand\nageexplorationitselfasatool,likezooming[45]andvisual\nthe Verifier, Agent0-VL operates under the Self-Evolving\nquerying[61]. Recently, manyeffortshaveadoptedRein-\nReasoningCycle,wherethemodelcontinuallyrefinesitsrea-\nforcementLearning(RL)fortraining[46‚Äì48]. GRIT[10]\nsoningthroughtool-groundedverification,confidence-gated\nandDeepEyes[79]leverageRLtoincentivizethemodelto",
    "char_length": 1469
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 23,
    "text": "repair, and reinforcement learning. Empirically, Agent0-\nactivelyciteimageregions(e.g.,boundingboxes)withinits\nVLoutperformsexistingopen-sourcemodelsacrossawide\nreasoningchains. Alsosomeworks(e.g.,WebWatcher[13])\nrangeofvisualreasoningbenchmarks,achievingstronger\nutilizeRLtoenhancethegeneralizationoftooluseformul-\nfactualconsistencyandmulti-turnstability.\ntimodal agents in deep research tasks [34, 60]. Building\non this line of work, our method further integrates tool-\nAcknowledgement\naugmentedreasoningintothemodel‚Äôsself-evaluationpro-\ncess,enablingthegenerationofprocess-levelrewardsignals ThisworkispartiallysupportedbytheAIforMathFund\nfromRenaissancePhilanthropy,AmazonResearchAward,\nand Cisco Faculty Research Award. The Authors also ac- [14] Google. Trydeepresearchandournewexperimentalmodel\nknowledgetheNationalArtificialIntelligenceResearchRe- ingemini,youraiassistant,2024.\nsource(NAIRR)Pilot,PurdueAnvilAIforcontributingto [15] ZhibinGou,ZhihongShao,YeyunGong,YelongShen,Yujiu\nthisresearchresult. Yang, MinlieHuang, NanDuan, andWeizhuChen. Tora:\nAtool-integratedreasoningagentformathematicalproblem\nReferences solving. arXivpreprintarXiv:2309.17452,2023.\n[16] TianruiGuan,FuxiaoLiu,XiyangWu,RuiqiXian,Zongxia\n[1] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad, Li,XiaoyuLiu,XijunWang,LichangChen,FurongHuang,\nIlgeAkkaya,FlorenciaLeoniAleman,DiogoAlmeida,Janko YaserYacoob,etal. Hallusionbench:anadvanceddiagnostic",
    "char_length": 1430
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 24,
    "text": "Altenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4 suiteforentangledlanguagehallucinationandvisualillusion\ntechnicalreport,2024. inlargevision-languagemodels. pages14375‚Äì14385,2024.\n[2] Anthropic. Claude3.7Sonnet,anthropic.com. https://\n[17] ZhichengGuo,SijieCheng,HaoWang,ShihaoLiang,Yujia\nwww.anthropic.com/claude/sonnet,2025. 2025-\nQin, Peng Li, Zhiyuan Liu, Maosong Sun, and Yang Liu.\n02-24.\nStabletoolbench: Towardsstablelarge-scalebenchmarking\n[3] ShuaiBai,KeqinChen,XuejingLiu,JialinWang,Wenbin ontoollearningoflargelanguagemodels. arXivpreprint\nGe,SiboSong,KaiDang,PengWang,ShijieWang,JunTang, arXiv:2403.07714,2024.\netal. Qwen2.5-vltechnicalreport,2025.\n[18] YichengHe,ChengsongHuang,ZongxiaLi,JiaxinHuang,\n[4] HardyChen,HaoqinTu,FaliWang,HuiLiu,XianfengTang,\nandYonghuiYang. Visplay:Self-evolvingvision-language\nXinya Du, Yuyin Zhou, and Cihang Xie. Sft or rl? an\nmodelsfromimages. arXivpreprintarXiv:2511.15661,2025.\nearlyinvestigationintotrainingr1-likereasoninglargevision-\n[19] YushiHu,WeijiaShi,XingyuFu,DanRoth,MariOstendorf,\nlanguagemodels,2025.\nLukeZettlemoyer,NoahASmith,andRanjayKrishna.Visual\n[5] Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang,\nsketchpad: Sketchingasavisualchainofthoughtformul-\nLingbo Liu, Eric P. Xing, and Liang Lin. Geoqa: A geo-\ntimodallanguagemodels. AdvancesinNeuralInformation\nmetricquestionansweringbenchmarktowardsmultimodal\nProcessingSystems,37:139348‚Äì139379,2024.\nnumericalreasoning,2022.",
    "char_length": 1445
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 25,
    "text": "[20] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao,\n[6] ZheChen,WeiyunWang,YueCao,YangzhouLiu,Zhang-\nZheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin.\nweiGao, ErfeiCui, JinguoZhu, ShenglongYe, HaoTian,\nVision-r1:Incentivizingreasoningcapabilityinmultimodal\nZhaoyang Liu, et al. Expanding performance boundaries\nlargelanguagemodels,2025.\nof open-source multimodal models with model, data, and\n[21] Physical Intelligence, Kevin Black, Noah Brown, James\ntest-timescaling,2025.\nDarpinian, Karan Dhabalia, Danny Driess, Adnan Esmail,\n[7] Google Deepmind. Gemini 2.5: Our most intelligent\nMichaelEqui,ChelseaFinn,NiccoloFusai,etal. œÄ0.5: a\nAI model ‚Äî blog.google. https://blog.google/\nvision-language-actionmodelwithopen-worldgeneralization,\ntechnology/google-deepmind/gemini-model-\n2025. URLhttps://arxiv.org/abs/2504.16054,1(2):3.\nthinking-updates-march-2025/#gemini-2-5-\nthinking,2025. Accessed:2025-03-25. [22] BowenJin,HansiZeng,ZhenruiYue,JinsungYoon,Sercan\nArik,DongWang,HamedZamani,andJiaweiHan. Search-\n[8] Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei\nr1:Trainingllmstoreasonandleveragesearchengineswith\nWang, and Kai-Wei Chang. Openvlthinker: An early ex-\nreinforcement learning. arXiv preprint arXiv:2503.09516,\nplorationtocomplexvision-languagereasoningviaiterative\n2025.\nself-improvement,2025.\n[23] BoLi,YuanhanZhang,DongGuo,RenruiZhang,FengLi,\n[9] YiDingandRuqiZhang.Sherlock:Self-correctingreasoning",
    "char_length": 1425
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 26,
    "text": "invision-languagemodels. arXivpreprintarXiv:2505.22651, HaoZhang,KaichenZhang,PeiyuanZhang,YanweiLi,Zi-\nwei Liu, et al. Llava-onevision: Easy visual task transfer.\n2025.\narXivpreprintarXiv:2408.03326,2024.\n[10] YueFan,XuehaiHe,DijiYang,KaizhiZheng,Ching-Chen\nKuo, Yuting Zheng, Sravana Jyothi Narayanaraju, Xinze [24] LeiLi,YuqiWang,RunxinXu,PeiyiWang,XiachongFeng,\nGuan, andXinEricWang. Grit: Teachingmllmstothink LingpengKong,andQiLiu. Multimodalarxiv:Adatasetfor\nwithimages. arXivpreprintarXiv:2505.15879,2025. improvingscientificcomprehensionoflargevision-language\n[11] JiazhanFeng,ShijueHuang,XingweiQu,GeZhang,Yujia models. arXivpreprintarXiv:2403.00231,2024.\nQin,BaoquanZhong,ChengquanJiang,JinxinChi,andWan- [25] Pengyi Li, Matvey Skripkin, Alexander Zubrey, Andrey\njunZhong. Retool:Reinforcementlearningforstrategictool Kuznetsov,andIvanOseledets. Confidenceisallyouneed:\nuseinllms. arXivpreprintarXiv:2504.11536,2025. Few-shotrlfine-tuningoflanguagemodels. arXivpreprint\n[12] Yichao Fu, Xuewei Wang, Yuandong Tian, and Jiawei arXiv:2506.06395,2025.\nZhao. Deep think with confidence. arXiv preprint [26] XuefengLi,HaoyangZou,andPengfeiLiu. Torl: Scaling\narXiv:2508.15260,2025. tool-integratedrl. arXivpreprintarXiv:2503.23383,2025.\n[13] Xinyu Geng, Peng Xia, Zhen Zhang, Xinyu Wang, Qi- [27] ZongxiaLi,WenhaoYu,ChengsongHuang,RuiLiu,Zhen-\nuchenWang,RuixueDing,ChenxiWang,JialongWu,Yida wen Liang, Fuxiao Liu, Jingxi Che, Dian Yu, Jordan",
    "char_length": 1446
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 27,
    "text": "Zhao, Kuan Li, et al. Webwatcher: Breaking new fron- Boyd-Graber, Haitao Mi, et al. Self-rewarding vision-\ntierofvision-languagedeepresearchagent. arXivpreprint languagemodelviareasoningdecomposition. arXivpreprint\narXiv:2508.05748,2025. arXiv:2508.19652,2025.\n[28] Jiaqi Liu, Songning Lai, Pengze Li, Di Yu, Wenjie Zhou, [42] Timo Schick, Jane Dwivedi-Yu, Roberto Dess`ƒ±, Roberta\nYiyang Zhou, Peng Xia, Zijun Wang, Xi Chen, Shixiang Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer,\nTang,etal. Mimickingthephysicist‚Äôseye: Avlm-centric NicolaCancedda,andThomasScialom. Toolformer: Lan-\napproach for physics formula discovery. arXiv preprint guagemodelscanteachthemselvestousetools. Advances\narXiv:2508.17380,2025. inNeuralInformationProcessingSystems,36:68539‚Äì68551,\n[29] Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng 2023.\nLi,TianheRen,XueyanZou,JianweiYang,HangSu,Jun [43] SheikhShafayat,FahimTajwar,RuslanSalakhutdinov,Jeff\nZhu, et al. Llava-plus: Learning to use tools for creating Schneider,andAndreaZanette. Canlargereasoningmodels\nmultimodal agents. In European conference on computer self-train? arXivpreprintarXiv:2505.21444,2025.\nvision,pages126‚Äì142.Springer,2024. [44] ZhihongShao,PeiyiWang,QihaoZhu,RunxinXu,Junxiao\n[30] PanLu,RanGong,ShibiaoJiang,LiangQiu,SiyuanHuang, Song,XiaoBi,HaoweiZhang,MingchuanZhang,YKLi,Y\nXiaodanLiang,andSong-ChunZhu. Inter-gps:Interpretable Wu,etal. Deepseekmath:Pushingthelimitsofmathematical",
    "char_length": 1447
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 28,
    "text": "geometryproblemsolvingwithformallanguageandsymbolic reasoninginopenlanguagemodels,2024.\nreasoning,2021. [45] Haozhan Shen, Kangjia Zhao, Tiancheng Zhao, Ruochen\n[31] PanLu,HritikBansal,TonyXia,JiachengLiu,ChunyuanLi, Xu,ZilunZhang,MingweiZhu,andJianweiYin. Zoomeye:\nHannanehHajishirzi,HaoCheng,Kai-WeiChang,Michel Enhancingmultimodalllmswithhuman-likezoomingcapa-\nGalley,andJianfengGao. Mathvista:Evaluatingmathemat- bilitiesthroughtree-basedimageexploration. InProceedings\nicalreasoningoffoundationmodelsinvisualcontexts. In of the 2025 Conference on Empirical Methods in Natural\nThe 3rd Workshop on Mathematical Reasoning and AI at LanguageProcessing,pages6613‚Äì6629,2025.\nNeurIPS‚Äô23,2023. [46] AlexSu,HaozheWang,WeimingRen,FangzhenLin,and\nWenhuChen. Pixelreasoner:Incentivizingpixel-spacerea-\n[32] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty,\nsoningwithcuriosity-drivenreinforcementlearning. arXiv\nand Enamul Hoque. Chartqa: A benchmark for question\npreprintarXiv:2505.15966,2025.\nanswering about charts with visual and logical reasoning.\npages2263‚Äì2279,2022. [47] Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao,\nZhengyuanYang,JunZhang,GuanjieChen,JiaweiGu,Jun-\n[33] FanqingMeng,LingxiaoDu,ZongkaiLiu,ZhixiangZhou,\ntaoLi,XiaoyeQu,etal. Openthinkimg: Learningtothink\nQuanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi,\nwithimagesviavisualtoolreinforcementlearning. arXiv\nWenhaiWang,JunjunHe,etal. Mm-eureka:Exploringthe\npreprintarXiv:2505.08617,2025.",
    "char_length": 1466
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 29,
    "text": "frontiersofmultimodalreasoningwithrule-basedreinforce-\n[48] ZhaochenSu,PengXia,HangyuGuo,ZhenhuaLiu,YanMa,\nmentlearning,2025.\nXiaoyeQu,JiaqiLiu,YanshuLi,KaideZeng,Zhengyuan\n[34] KartikNarayan,YangXu,TianCao,KavyaNerella,VishalM\nYang,etal. Thinkingwithimagesformultimodalreasoning:\nPatel,NavidShiee,PeterGrasch,ChaoJia,YinfeiYang,and\nFoundations,methods,andfuturefrontiers. arXivpreprint\nZheGan. Deepmmsearch-r1:Empoweringmultimodalllms\narXiv:2506.23918,2025.\ninmultimodalwebsearch. arXivpreprintarXiv:2510.12801,\n[49] Qwen-VL Team. Qwen3-vl: Sharper vision, deeper\n2025.\nthought,broaderaction. https://qwen.ai/blog?id=\n[35] OpenAI. OpenAIo1SystemCard‚Äîopenai.com. https:\n99f0335c4ad9ff6153e517418d48535ab6d8afef&\n//cdn.openai.com/o1-system-card.pdf, 2024.\nfrom=research.latest-advancements-list,\nAccessed:2024-09-12.\n2025. 2025-09-22.\n[36] OpenAI. Introducing gpt-5. https://openai.com/\n[50] TongyiDeepResearchTeam,BaixuanLi,BoZhang,Dingchu\nindex/introducing-gpt-5/,2025.\nZhang,FeiHuang,GuangyuLi,GuoxinChen,HuifengYin,\n[37] OpenAI. Openaideepresearchsystemcard,2025.\nJialongWu,JingrenZhou,etal. Tongyideepresearchtechni-\n[38] Jing-ChengPang,PengyuanWang,KaiyuanLi,Xiong-Hui calreport. arXivpreprintarXiv:2510.24701,2025.\nChen,JiachengXu,ZongzhangZhang,andYangYu. Lan- [51] Omkar Thawakar, Shravan Venkatraman, Ritesh Thawkar,\nguage model self-improvement by reinforcement learning AbdelrahmanMShaker,HishamCholakkal1RaoMuham-",
    "char_length": 1426
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 30,
    "text": "contemplation. InInternationalConferenceonLearningRep- madAnwer,SalmanKhan,andFahadKhan. Evolmm:Self-\nresentations,pages1‚Äì11,2024. evolvinglargemultimodalmodelswithcontinuousrewards.\n[39] Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E arXivpreprintarXiv:2511.16672,2025.\nGonzalez. Gorilla: Largelanguagemodelconnectedwith [52] KeWang,HouxingRen,AojunZhou,ZimuLu,SichunLuo,\nmassiveapis. arXivpreprintarXiv:2305.15334,2023. WeikangShi,RenruiZhang,LinqiSong,MingjieZhan,and\n[40] RunqiQiao,QiunaTan,GuantingDong,MinhuiWu,Chong Hongsheng Li. Mathcoder: Seamless code integration in\nSun,XiaoshuaiSong,ZhuomaGongQue,ShanglinLei,Zhe llmsforenhancedmathematicalreasoning. arXivpreprint\nWei,MiaoxuanZhang,etal. We-math:Doesyourlargemul- arXiv:2310.03731,2023.\ntimodalmodelachievehuman-likemathematicalreasoning?, [53] KeWang,JuntingPan,WeikangShi,ZimuLu,HouxingRen,\n2024. AojunZhou,MingjieZhan,andHongshengLi. Measuring\n[41] YujiaQin,ShihaoLiang,YiningYe,KunlunZhu,LanYan, multimodalmathematicalreasoningwithmath-visiondataset.\nYaxiLu,YankaiLin,XinCong,XiangruTang,BillQian,etal. pages95095‚Äì95169,2024.\nToolllm:Facilitatinglargelanguagemodelstomaster16000+ [54] KangruiWang,PingyueZhang,ZihanWang,YaningGao,\nreal-worldapis. arXivpreprintarXiv:2307.16789,2023. Linjie Li, Qineng Wang, Hanyang Chen, Chi Wan, Yip-\ningLu,ZhengyuanYang,etal. Vagen: Reinforcingworld Feng,LiShen,etal. Mulberry:Empoweringmllmwitho1-",
    "char_length": 1409
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 31,
    "text": "modelreasoningformulti-turnvlmagents. arXivpreprint likereasoningandreflectionviacollectivemontecarlotree\narXiv:2510.16907,2025. search,2024.\n[55] Qinsi Wang, Bo Liu, Tianyi Zhou, Jing Shi, Yueqian Lin, [69] ShunyuYao,JeffreyZhao,DianYu,NanDu,IzhakShafran,\nYiran Chen, Hai Helen Li, Kun Wan, and Wentian Zhao. Karthik Narasimhan, and Yuan Cao. React: Synergizing\nVision-zero: Scalable vlm self-improvement via strategic reasoningandactinginlanguagemodels. InInternational\ngamifiedself-play. arXivpreprintarXiv:2509.25541,2025. ConferenceonLearningRepresentations(ICLR),2023.\n[56] XuezhiWang,JasonWei,DaleSchuurmans,QuocLe,Ed [70] XiangYue,YuanshengNi,KaiZhang,TianyuZheng,Ruoqi\nChi,SharanNarang,AakankshaChowdhery,andDennyZhou. Liu,GeZhang,SamuelStevens,DongfuJiang,WeimingRen,\nSelf-consistencyimproveschainofthoughtreasoninginlan- YuxuanSun,etal. Mmmu:Amassivemulti-disciplinemulti-\nguagemodels. arXivpreprintarXiv:2203.11171,2022. modalunderstandingandreasoningbenchmarkforexpertagi.\nInProceedingsoftheIEEE/CVFConferenceonComputer\n[57] XiyaoWang, ChunyuanLi, JianweiYang, KaiZhang, Bo\nVisionandPatternRecognition,pages9556‚Äì9567,2024.\nLiu,TianyiXiong,andFurongHuang. Llava-critic-r1:Your\ncriticmodelissecretlyastrongpolicymodel. arXivpreprint [71] Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu,\narXiv:2509.00676,2025. XikunZhang,ShijianLu,andDachengTao. R1-vl: Learn-\ning to reason with multimodal large language models via\n[58] XiyaoWang,ZhengyuanYang,ChaoFeng,HongjinLu,Lin-",
    "char_length": 1485
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 32,
    "text": "step-wisegrouprelativepolicyoptimization,2025.\njie Li, Chung-Ching Lin, Kevin Lin, Furong Huang, and\nLijuanWang. Sotawithless:Mcts-guidedsampleselection [72] JuntianZhang,SongJin,ChuanqiCheng,YuhanLiu,Yankai\nfordata-efficientvisualreasoningself-improvement,2025. Lin,XunZhang,YufeiZhang,FeiJiang,GuojunYin,Wei\nLin,etal. Viper: Empoweringtheself-evolutionofvisual\n[59] LaiWei,YutingLi,ChenWang,YueWang,LingheKong,\nperceptionabilitiesinvision-languagemodel. arXivpreprint\nWeiranHuang,andLichaoSun. Unsupervisedpost-training\narXiv:2510.24285,2025.\nformulti-modalLLMreasoningviaGRPO. arXivpreprint\n[73] QingyangZhang,HaitaoWu,ChangqingZhang,PeilinZhao,\narXiv:2505.22453,2025.\nandYataoBian. Rightquestionisalreadyhalftheanswer:\n[60] JinmingWu,ZihaoDeng,WeiLi,YidingLiu,BoYou,BoLi,\nFully unsupervised llm reasoning incentivization. arXiv\nZejunMa,andZiweiLiu. Mmsearch-r1:Incentivizinglmms\npreprintarXiv:2504.05812,2025.\ntosearch. arXivpreprintarXiv:2506.20670,2025.\n[74] RenruiZhang,DongzhiJiang,YichiZhang,HaokunLin,Ziyu\n[61] Qiong Wu, Xiangcong Yang, Yiyi Zhou, Chenxin Fang,\nGuo,PengshuoQiu,AojunZhou,PanLu,Kai-WeiChang,\nBaiyangSong,XiaoshuaiSun,andRongrongJi. Grounded\nYuQiao,etal. Mathverse:Doesyourmulti-modalllmtruly\nchain-of-thought for multimodal large language models,\nseethediagramsinvisualmathproblems? pages169‚Äì186.\n2025.\nSpringer,2024.\n[62] PengXia,KaideZeng,JiaqiLiu,CanQin,FangWu,Yiyang\n[75] Yi-Fan Zhang, Qingsong Wen, Chaoyou Fu, Xue Wang,\nZhou,CaimingXiong,andHuaxiuYao. Agent0: Unleash-",
    "char_length": 1501
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 33,
    "text": "Zhang Zhang, Liang Wang, and Rong Jin. Beyond llava-\ningself-evolvingagentsfromzerodataviatool-integrated\nhd: Diving into high-resolution large multimodal models.\nreasoning. arXivpreprintarXiv:2511.16043,2025.\narXivpreprintarXiv:2406.08487,2024.\n[63] WeiXiong,HanningZhang,ChenluYe,LichangChen,Nan\n[76] Yi-FanZhang,XingyuLu,ShukangYin,ChaoyouFu,Wei\nJiang,andTongZhang. Self-rewardingcorrectionformathe-\nChen,XiaoHu,BinWen,KaiyuJiang,ChangyiLiu,Tianke\nmaticalreasoning. arXivpreprintarXiv:2502.19613,2025.\nZhang,etal. Thyme:Thinkbeyondimages. arXivpreprint\n[64] Ran Xu, Jingjing Chen, Jiayu Ye, Yu Wu, Jun Yan, Carl arXiv:2508.11630,2025.\nYang,andHongkunYu. Incentivizingagenticreasoningin [77] Yi-FanZhang,TaoYu,HaochenTian,ChaoyouFu,PeiyanLi,\nllmjudgesviatool-integratedreinforcementlearning. arXiv JianshuZeng,WulinXie,YangShi,HuanyuZhang,Junkang\npreprintarXiv:2510.23038,2025.\nWu,etal. Mm-rlhf:Thenextstepforwardinmultimodalllm\n[65] ZhenghaiXue,LongtaoZheng,QianLiu,YingruLi,Xiaosen alignment. arXivpreprintarXiv:2502.10391,2025.\nZheng,ZejunMa,andBoAn. Simpletir: End-to-endrein- [78] XuandongZhao,ZheweiKang,AosongFeng,SergeyLevine,\nforcementlearningformulti-turntool-integratedreasoning. andDawnSong.Learningtoreasonwithoutexternalrewards.\narXivpreprintarXiv:2509.02479,2025. arXivpreprintarXiv:2505.19590,2025.\n[66] YiYang,XiaoxuanHe,HongkunPan,XiyanJiang,YanDeng, [79] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao,",
    "char_length": 1429
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 34,
    "text": "XingtaoYang,HaoyuLu,DachengYin,FengyunRao,Min- GuohaiXu,LeYang,ChaoShen,andXingYu. Deepeyes:\nfengZhu,etal. R1-onevision:Advancinggeneralizedmulti- Incentivizing‚Äùthinkingwithimages‚Äùviareinforcementlearn-\nmodalreasoningthroughcross-modalformalization,2025. ing. arXivpreprintarXiv:2505.14362,2025.\n[67] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, [80] YiyangZhou,ChenhangCui,RafaelRafailov,ChelseaFinn,\nEhsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, and Huaxiu Yao. Aligning modalities in vision large lan-\nMichael Zeng, and Lijuan Wang. Mm-react: Prompting guage models via preference fine-tuning. arXiv preprint\nchatgptformultimodalreasoningandaction. arXivpreprint arXiv:2402.11411,2024.\narXiv:2303.11381,2023. [81] Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang,\n[68] HuanjinYao,JiaxingHuang,WenhaoWu,JingyiZhang,Yibo ZhaorunChen,ChenhangCui,XiyaoWang,YunLi,Linjun\nWang,ShunyuLiu,YingjieWang,YuxinSong,Haocheng Zhang,andHuaxiuYao. Calibratedself-rewardingvisionlan-\nguagemodels.InAdvancesinNeuralInformationProcessing\nSystems(NeurIPS),2024.\n[82] YiyangZhou,YangfanHe,YaofengSu,SiweiHan,JoelJang,\nGedasBertasius,MohitBansal,andHuaxiuYao. Reagent-v:\nAreward-drivenmulti-agentframeworkforvideounderstand-\ning. arXivpreprintarXiv:2506.01300,2025.\n[83] YiyangZhou,ZhaoyangWang,TianleWang,ShangyuXing,\nPeng Xia, Bo Li, Kaiyuan Zheng, Zijian Zhang, Zhaorun\nChen,WenhaoZheng,etal. Anyprefer:Anautomaticframe-\nworkforpreferencedatasynthesis. InInternationalConfer-",
    "char_length": 1477
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 35,
    "text": "enceonLearningRepresentations(ICLR),2025.\n[84] JinguoZhu,WeiyunWang,ZheChen,ZhaoyangLiu,Shen-\nglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su,\nJieShao,etal. Internvl3: Exploringadvancedtrainingand\ntest-timerecipesforopen-sourcemultimodalmodels,2025.\n[85] YuxinZuo,KaiyanZhang,LiSheng,ShangQu,GanquCui,\nXuekaiZhu,HaozhanLi,YuchenZhang,XinweiLong,Ermo\nHua,BiqingQi,YoubangSun,ZhiyuanMa,LifanYuan,Ning\nDing, and Bowen Zhou. TTRL: Test-time reinforcement\nlearning. arXivpreprintarXiv:2504.16084,2025.\nAgent0-VL: Exploring Self-Evolving Agent for\nTool-Integrated Vision-Language Reasoning\nSupplementary Material\nA.Notation SystemPrompt(Solver)\nB.ImplementationDetails\nYou are the Reasoner in a unified\nBase Model. Agent0-VL is implemented upon Qwen2.5- tool-integrated VLM agent. You must\nsolve tasks through multi-turn\nVL-7B-Instruct and Qwen3-VL-8B. Both the Solver and\nreasoning and selective tool use.\nVerifiershareparametersŒ∏.\nTraining Details. Our training pipeline consists of two Rules:\nsequentialstages: supervisedfine-tuning(SFT)andRL.In\ntheSFTstage,Agent0-VLisfirsttrainedontool-usageand\n- Wrap internal reasoning in\nimagemanipulationdata,followedbygradualannealingon\n<think>...</think>.\nmathematicalcodereasoningdata. Thelearningrateisset - Call tools only when necessary using:\nto1√ó10‚àí5,whileotherhyperparametersremainconsistent. {\"tool_name\":\"<Tool>\",\"tool_input\":{...}}\n- Wait for tool_output before continuing.\nWeadoptabatchsizeof128,trainfor3epochs,andapplya",
    "char_length": 1476
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 36,
    "text": "- Be concise and deterministic; no\nlinearwarmupratioof0.05. Toensuretrainingstabilityand hallucinated values.\npreventearly-stagecollapseorentropydegenerationinthe\nRLprocess,weperformashortexternal-rewardpretraining Process:\nphasebeforeself-evolutionbegins. Specifically,themodelis\ntrainedfor2epochsusingconventionalRLwithexternally\n<think>\ndefinedcorrectnesssignalstoinitializestableexplorationand 1. Restate goal & plan next step.\ntool-usagebehaviors. Thispretrainingservesasawarm-up 2. Decide if a tool is needed; specify\ninput.\nthatactivatesstructuredexploration,afterwhichthemodel\n3. Integrate tool_output; update\ntransitionstotheself-evolvingRLphasedrivenpurelyby reasoning.\ninternalprocessrewards. Intheself-evolvingRLstage,the </think>\nlearningrateissetto5√ó10‚àí7,andtrainingisperformed Emit one tool call or continue reasoning.\nWhen finished:\nfor 1 epoch with a batch size of 256. The reinforcement\nlearningfollowstheGRPO[44]paradigmwithagroupsize\nN =8forrelativenormalization. WesettheKLdivergence (1) Output CONFIDENCE: <0-1>\ncoefficientŒ≤ = 0.001toregularizepolicydrift, collect (2)Output FINAL_ANSWER: <answer>.\nKL\n4 rollouts per task, and apply a repetition penalty of 1.05\ntodiscourageredundantreasoning. Theentropycoefficient\nisŒ≤ = 0.01,theselectiverepairthresholdœÑ = 0.7,and\nent c Figure5.SystempromptfortheSolver.\ntherepairpenaltyŒ∑ =0.05. Allexperimentsareconducted\nwithmixed-precisiontraining(bfloat16)on8NVIDIAH200\nGPUs.\nD.1.Overview",
    "char_length": 1450
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 37,
    "text": "C.PromptingandTemplates To ensure diversity and robustness, the training data are\nconstructed through a progressive curriculum, covering a\nWeprovidesystempromptsforourframework,asshownin\nwide range of multimodal reasoning tasks. The resulting\nFigure5-7,respectively.\ncorpusisdividedintothreefunctionaltiers:\n‚Ä¢ Directreasoningdata: single-turntextualorvisualques-\nD.TrainingDataConstructionPipeline\ntionssolvablewithouttoolcalls,groundinglinguisticrea-\nThis section describes the multi-stage data construction soningandvisualperception.\npipelineusedtotrainAgent0-VL.Theobjectiveistobuild ‚Ä¢ Tool-augmenteddata: problemsthatrequirecodeexecu-\nalarge-scalecorpusofhigh-quality,tool-integratedreason- tion,OCR,orvisualanalysis,providingfactualgrounding\ning trajectories that equip the model with both reasoning andverifiableevidence.\nandverificationcapabilitiesbeforeenteringtheself-evolving ‚Ä¢ Multi-turn reasoning data: complex vi-\nreinforcementstage. sual‚Äìmathematical tasks involving iterative tool\nSymbol Meaning\nx Taskinstance(image+text)\ns ,a ,o State/context,action(reasoning/toolcall),observation(tooloutput)atstept\nt t t\nœÑ Trajectory{(s ,a ,o )}T\nt t t t=1\nœÄ UnifiedpolicywithroletokensforRA/EE\nŒ∏\nr(t) Process-levelrewardatstept\nproc\ng(œÑ) Compositetrajectoryreward(Eq.5)\nœÑ Confidencethresholdfortriggeringlocalrepair\nc\nTable5.Keynotationsusedthroughoutthepaper.\nSystemPrompt Formathematicalandchart-basedreasoning,wefurtherin-\ncludeMathVerse[74], MathVista[31], WeMath[40], and\nChartQA[32].",
    "char_length": 1494
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 38,
    "text": "You are the Verifier in a unified\ntool-integrated VLM agent. Your role\nis to verify a given reasoning D.2.Multi-StageSFTDataConstruction\ntrajectory step-by-step, optionally\ncalling tools to check facts. The SFT stage aims to teach Agent0-VL to perform end-\nto-end,tool-integratedreasoningandself-verificationunder\nminimal human supervision. We employ a three-step au-\ntomatic pipeline inspired by recent multimodal reasoning\nInputs: trajectory prefix $\\tau_{1:t} =\n{(s_k, a_k, o_k)}_{k=1..t}$. works[33,77].\nRules: (1) Task Sampling and Prompt Construction. We ran-\n(1)Wrap internal thought in domlysamplemultimodalproblems(Q,I)fromthedatasets\n<think>...</think>. above. For each problem, a structured prompt is applied\n(2)Use the same tool schema for factual\nthat requests: (i) a detailed reasoning trace (wrapped in\nchecks.\n(3) Output exactly one JSON line per step: <think>...</think>)and(ii)explicittool-useplans\ninJSONformat. Teachermodels(GPT-5[36]andQwen2.5-\nVL-72B[3])areusedtobootstrapthegenerationofcomplete\n{\"step_index\":t,\ntrajectories.\n\"score\":<-1-1>,\n\"confidence\":<0-1>,\n(2) Tool Execution and Verification. All tool calls are\n\"critique\":\"<<=2 sentences>\",\nexecuted in a sandbox environment to obtain real results.\n\"tool_check\":true|false\n} Foreachreasoningstepa t ,thecorrespondingobservation\no islogged,formingcompletemultimodaltrajectoriesœÑ =\nt\nPrinciples: {(a ,o )}T . Anauxiliaryverifiermodelthenchecksthe\nt t t=1\nconsistencybetweentextualreasoning,tooloutputs,andthe",
    "char_length": 1487
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 39,
    "text": "(1) Ground verification on objective tool finalanswer. Inconsistentorunexecutabletrajectoriesare\nevidence. automaticallyfiltered.\n(2) Penalize unsupported or inconsistent\nreasoning. (3)Tool-GroundedSelf-VerificationandRepairExam-\n(3) High confidence requires agreement ples. To initialize the model‚Äôs self-evaluation ability, we\nbetween tool and text.\nfurtherincludeexampleswheretheverifiergeneratesstruc-\ntured feedback V = (score ,conf ,critique ) and cor-\nt t t t\nrection signals for low-confidence steps. These ‚Äúreason-\nFigure6.SystempromptfortheVerifier. ing‚Äìverification‚Äìrepair‚Äùtriplesserveasbridgesamplesfor\ntransitioningfromsupervisedlearningtoself-evolvingrein-\nforcementlearning.\nuse,correction,andreflection. After filtering and deduplication, we obtain approxi-\nAllpromptsarederivedoradaptedfromexistingmulti- mately200khigh-qualitymultimodaltrajectories. Thisuni-\nmodalbenchmarks,includingGeometry3K[30],GeoQA[5], fiedSFTdatasetinitializesthedual-rolereasoningandveri-\nMulberry [68], LLaVA-OV-Image [23], MM-RLHF [77], ficationbehaviorsofAgent0-VL,layingthefoundationfor\nSMR[75],MM-Eureka[33],Retool[11],andarXivQA[24]. self-evolvingoptimization.\nSystemPrompt D.3.DatasetForReinforcementLearning\nWe start from math and perception problems from Math-\nYou are the Self-Repair module of a Verse [74], MathVista [31], WeMath [40], arXivQA [24],\nunified tool-integrated VLM. andChartQA[32],andThinkLiteVL[58]. Themodelrolls\nYou receive: (i) the original trajectory",
    "char_length": 1473
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 40,
    "text": "outreasoningtrajectoriesunderitscurrentpolicy,whilethe\nprefix \\tau_{1:t}, (ii) the EE\nverification triple for step t internalVerifiergeneratesstep-levelrewardsandcritiquesto\n(score_t, confidence_t, critique_t), guideoptimization.\nand (iii) the minimal repair target\nD.4.QualityControl\n(segment uÀÜ(t)). To ensure factual reliability and semantic alignment, we\napply:\nGOAL\n‚Ä¢ Executionvalidation: alltoolinvocationsarere-runina\n- If confidence_t < \\tau_c, propose a\nsandboxtoremoveinvalidtraces.\nminimal, local patch to uÀÜ(t) that\nfixes the specific error WITHOUT ‚Ä¢ Semantic consistency check: textual reasoning must\nrewriting validated context. alignwithtoolresultsandvisualobservations.\n- Use tools to recompute only what is\n‚Ä¢ Redundancy filtering: near-duplicate reasoning traces\nnecessary to validate the patch.\nareprunedviaembeddingsimilarity.\n‚Ä¢ Manual spot-check: about 10k samples are manually\nreviewedformultimodalandreasoningcorrectness.\nREASONING FORMAT\nThissystematicconstructionensuresthatAgent0-VLlearns\n- Put your full planning and diagnostics\nnot only to reason and act across modalities, but also to\ninside <think>...</think>.\n- After <think>, either (A) emit a PATCH verify,repair,andself-evolvethroughgrounded,tool-based\nJSON (and optionally a tool call), or evidence.\n(B) emit a NO_CHANGE JSON if repair\nIn total, Agent0-VL is trained on approximately 240k\nis not warranted.\ncuratedmultimodalreasoningtrajectories: ‚àº200kSFTtra-\njectoriesforcold-startreasoning,and‚àº40kdataforRL.",
    "char_length": 1494
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 41,
    "text": "PATCH JSON (single line): E.CaseStudies\n{\n\"action\": \"PATCH\",\nTo provide an intuitive understanding of how Agent0-VL\n\"target_step\": t,\nperformsreasoning,verification,andtoolinteractioninprac-\n\"patch_type\":\n\"<text|code|tool_call|parameter>\", tice,wevisualizeseveralrepresentativecasescoveringboth\n\"new_content\": \"<the minimal single-stepandmulti-stepreasoningscenarios. Theseexam-\nreplacement content>\",\nplesillustratehowthemodeldynamicallyinvokesexternal\ntools,integratestooloutputsintoitsinternalreasoningpro-\n\"justification\": \"<<= 2 sentences cess,andleveragestheVerifierforprocess-levelverification\nreferencing critique/evidence>\" andcorrection. Together,thesecaseshighlightthemodel‚Äôs\n}\nabilitytogroundreasoninginvisualevidence,maintainfac-\ntualconsistency, andperformself-reflectiveimprovement\nNO_CHANGE JSON (single line):\n{ acrossdiversevisualreasoningtasks.\n\"action\": \"NO_CHANGE\",\n\"target_step\": t,\n\"reason\": \"<why repair is not warranted\nor evidence is insufficient>\"\n}\nFigure7.SystempromptforSelf-RepairMode.\nAs shown in the figure, a coast guard Phase 1: Initial Reasoning System Prompt for Solver\nwatchtower is located at the coordinate <think>\norigin O(0, 0).Three key reef locations, A, B, 1. Thegoal is to calculate **\nand C, are marked on the nautical grid.A 2. *The description \"from due East (positive x-axis) counter-clockwise by $270^\\circ$, ending",
    "char_length": 1367
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 42,
    "text": "ship departs from point A, planning to sail in at due North (positive y-axis)\" is **self-contradictory**. * (a) A $270^\\circ$ counter-\nsequence to point B, and then to point C. Its clockwise rotation from East ($0^\\circ$) should end at South ($270^\\circ$). * (b) Reaching\nroute consists of two straight-line North ($90^\\circ$) from East ($0^\\circ$) is only a $90^\\circ$ rotation.\nsegments: A-B and B-C**. The ‚Ä¢ One possible interpretation is that the beam covers the $270^\\circ$ **major arc** from East\nwatchtower‚Äòs searchlight beam is limited. It to North, i.e., $0^\\circ \\rightarrow -90^\\circ(\\text{or } 270^\\circ) \\rightarrow 180^\\circ\ncan only rotate counter-clockwise from due \\rightarrow 90^\\circ$.\nEast (positive x-axis) by **$270^\\circ$**, ‚Ä¢ * Under this interpretation, the covered areas are Q1, Q4, and Q3.\nending at due North (positive y-axis). ‚Ä¢ * Therefore, the **Blind Spot = Quadrant 2 ($x < 0, y > 0$)**.\nCalculate how much of the ship's total 3.I need to (a) Check if segment A-B enters Q2. * (b) Check if segment B-C enters Q2. * (c) Sum\njourney (segments A-B and B-C) is sailed the results.\nwithin the watchtower's blind spot. 4.**Execute Plan:**\n‚Ä¢ * (a) Segment A-B: From $A(8, 6)$ [Q1] to $B(7, -1)$ [Q4]. Both points have $x > 0$. Therefore,\nthe entire segment is in the $x > 0$ region and **cannot enter Q2**. Length in blind spot is\n$0$.\n‚Ä¢ * (b) Segment B-C: From $B(7, -1)$ [Q4] to $C(-4, -3)$ [Q3]. Both points have $y < 0$.",
    "char_length": 1451
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 43,
    "text": "Phase 3: Self-Repair Self-Repair System Prompt Therefore, the entire segment is in the $y < 0$ region and **cannot enter Q2**. Length in\nblind spot is $0$.\n**EE Key Feedback:** $\\text{score}_1 = -1.0$, $\\text{conf}_1 = 1.0$, ‚Ä¢ * (c) Total length = $0 + 0 = 0$.\n$\\text{critique}_1$ = \"The true blind spot is Q4 ($x > 0, y < 0$)\". </think>\n**Trigger Condition:** $\\text{conf}_t \\ge \\tau_c$ and $\\text{score}_t < 0$ (high- <answer>0.00 </answer> Failed Attempt\nconfidence error), activating repair gate $g_t$.\n<think> 1. **Diagnosis:** EE has assigned a score of $-1.0$ with $1.0$ confidence at\n` 3. s * te * p E _ E i n C d o e r x re :1 c ` ti . o 2 n . : * * * * R T o h o e t ` C c a ri u ti s q e u :* e * ` R e A xp 's l i d c e it f l i y n s it t i a o t n e o s f t t h h e e c \" o b r l r i e n c d t s b p l o in t d \" w sp a o s t i n is c orrect. Phase 2: Evaluation System Prompt for Verifier\n**Quadrant 4 (Q4)**. 4. **Repair Strategy:** I must issue a `PATCH` to correct the {\"step_index\":1, \"score\":-1.0, \"confidence\":1.0, \"critique\":\"Reasoning contains a critical logic\ncore logical premise in `step_index:1` of the reasoning. This patch will change the error. The definition of 'blind spot' is wrong. The instruction 'from due East (0¬∞) counter-\ncomputational basis for all subsequent steps. clockwise by 270¬∞' is explicit and covers the angular range [0¬∞, 270¬∞], which is Q1, Q2, and Q3.\n</think> The 'ending at due North' text contradicts the 270¬∞ rotation and should be treated as",
    "char_length": 1499
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 44,
    "text": "{ \"action\": \"PATCH\", \"target_step\": 1, \"patch_type\": \"text\", \"new_content\": \"Corrected interference or a typo. Therefore, the true blind spot is Q4 ($x > 0, y < 0$).\", \"tool_check\":false}\nPremise: Based on EE verification, the blind spot is identified as Quadrant 4 (Q4, $x > {\"step_index\":2, \"score\":-1.0, \"confidence\":1.0, \"critique\":\"The subsequent calculations (steps\n0, y < 0$).\", \"justification\": \"Original logic incorrectly identified Q2 as the blind spot 4a, 4b), while correct based on the faulty premise (Q2), are invalid because the premise is\ndue to contradictory input text. EE has clarified the correct definition is Q4.\" } wrong. The actual route segments A-B and B-C do transit the true blind spot, Q4.\",\n\"tool_check\":false}\nError Found\nPhase 4: Corrected Reasoning Solver System Prompt, with `PATCH` injected from SR\n<think>\n1. **Goal:** Restart reasoning based on the `PATCH`. 2. **Corrected Premise:** Blind Spot = Quadrant 4 (Q4, $x > 0, y < 0$). 3. **New Plan:** * (a) Calculate the length of segment A-B within\nQ4. * (b) Calculate the length of segment B-C within Q4. * (c) Sum the lengths and round. 4. **Execute (a) - A-B:** * Segment starts at $A(8, 6)$ [Q1] and ends at $B(7, -1)$ [Q4]. * It crosses\nthe x-axis ($y=0$) into the blind spot Q4. * I need to find the intersection $P$ of the line A-B with $y=0$, and then calculate the distance between $P$ and $B$. * I will call a tool for this\ngeometric calculation. </think>",
    "char_length": 1450
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 45,
    "text": "{\"tool_name\":\"calc_intersection_and_distance\", \"tool_input\":{\"p1\":[8, 6], \"p2\":[7, -1], \"axis_to_cross\":\"y=0\", \"target_quadrant\":4}}\n**[Tool Output]**\n`{\"intersection_point\":[7.142857, 0.0], \"distance_in_quadrant\":1.01015}`\n<think>\n5. **Integrate (a):** Tool returns length of A-B in Q4 as **$1.01015$**. 6. **Execute (b) - B-C:** * Segment starts at $B(7, -1)$ [Q4] and ends at $C(-4, -3)$ [Q3]. * It starts *in* the blind spot Q4\nand leaves by crossing the y-axis ($x=0$). * I need to find the intersection $Q$ of the line B-C with $x=0$, and then calculate the distance between $B$ and $Q$. * I will call the tool.\n</think>\n{\"tool_name\":\"calc_intersection_and_distance\", \"tool_input\":{\"p1\":[7, -1], \"p2\":[-4, -3], \"axis_to_cross\":\"x=0\", \"target_quadrant\":4}}\n**[Tool Output]** `{\"intersection_point\":[0.0, -2.272727], \"distance_in_quadrant\":7.11601}`\n<think>\n7. **Integrate (b):** Tool returns length of B-C in Q4 as **$7.11601$**. 8. **Execute (c) - Summation:** * Total Distance = (Length of A-B in Q4) + (Length of B-C in Q4) * Total = $1.01015 +\n7.11601 = 8.12616$. * Rounding to two decimal places, the result is $8.13$.\n</think>\n<answer> 8.13 </answer> Correct\nFigure8.Afullreasoning‚Äìevaluation‚ÄìrepaircycleofAgent0-VLonageometricreasoningtask.TheSolverfirstgeneratesanincorrect\nsolution(Phase1),whichtheVerifieridentifiesandcritiquesthroughtool-groundedverification(Phase2). Basedonthisfeedback,the",
    "char_length": 1408
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 46,
    "text": "modelperformsSelf-Repairtopatchthefaultypremise(Phase3)andre-executesreasoningwiththecorrectedlogic(Phase4),producing\ntheverifiedfinalanswer.\nQuestion: What street is indicated by the sign on the Tool Calling\npole near the center bottom of the image?\nOptions: A. MACHIGAN B. MACHLGUN C. MICHIGUN\nD. MICHIGAN import os\nPlease select the correct answer from the options import random\nabove. from PIL import Image\n# Define paths\nimage_path = \"hr_bench_4k/179.jpg\"\noutput_dir = \"/mnt/data/temp_processed_images\"\n# Ensure the output directory exists\nModel Response os.makedirs(output_dir, exist_ok=True)\n# Load the image\n<Think>\nThe task is to identify the street name displayed on the sign located image = Image.open(image_path)\n# Define cropping region (x1, y1, x2, y2)\nnear the bottom center of the image. The sign is relatively small and\ncrop_coords = (1550, 2650, 1850, 2750)\ndistant, making the text difficult to read without magnification. To\nimprove readability, the image should be cropped and zoomed in # Crop and zoom\ncropped = image.crop(crop_coords)\naround this specific region, enhancing the visibility of the sign‚Äôs text\nfor accurate recognition. The sign lies slightly to the right of the zoom_factor = 2\nzoomed = cropped.resize( (cropped.width * zoom_factor, cropped.height * zoom_factor))\nimage‚Äôs central lower area, so cropping this portion will allow for\n# Generate output filename\nfocused inspection. The cropping coordinates can be estimated",
    "char_length": 1459
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 47,
    "text": "based on the sign‚Äôs relative position within the image. suffix = random.randint(1000, 9999)\noutput_path = os.path.join(output_dir, f\"179_{suffix}.jpg\")\nTool Calling # Save and print path\nzoomed.save(output_path)\nTool execution print(output_path)\nAfter examining the cropped and enlarged section, the sign clearly\nreads ‚ÄúN MICHIGAN AV 100 E‚Äù, confirming that the indicated street\nis Michigan Avenue.\n</think> Tool execution\n<answer>\nD. MICHIGAN <sandbox\\_output> <subimage\\_sign> </sandbox\\_output>\n</answer>\nFigure9. Single-steptool-integratedreasoning. TheexampledemonstrateshowAgent0-VLidentifiesastreetnamefromanimage\nbyreasoningaboutthetask,decidingtocropandzoominonaspecificregionoftheimage,andinvokingtheimage-croppingtoolto\nenhancevisibility.Theresultingoutputconfirmsthestreetname,showinghowthemodeleffectivelygroundsitsreasoningprocessinvisual\nmanipulation.\nModel Response Tool Calling\n<Think>\nWe need the cone‚Äôs **vertical height (h)**, but the question <tool>\nprovides the **slant height (l)**. import math\nFrom the Pythagorean theorem for the right triangle formed by r = 2\nheight, radius, and slant height:\\[h = \\sqrt{l^2 - r^2}\\] l = 8\nOnce we find \\(h\\), we can compute the cone‚Äôs volume using:\\[V h = math.sqrt(l**2 - r**2)\n= \\frac{1}{3}\\pi r^2 h\\] V = (1/3) * math.pi * (r**2) * h\nround(V, 2)\n</tool>\nTool Calling\nTool execution\nQuestion: Find the\nvolume of the cone Tool execution <sandbox\\_output> 32.45\nshown.\n</sandbox\\_output>\nRound your answer to\n**two decimal places**.",
    "char_length": 1494
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 48,
    "text": "We get the result from python code. The result is 32.45.\n<answer>\nTherefore, when rounded to two decimal\n32.54 cm¬≥\nplaces:\\[\\boxed{V = 32.54 \\, \\text{cm}^3}\\]\n</answer>\n</think>\nFigure10. Mathematicalreasoningwithcodeexecution. Thiscaseillustrateshowthemodeldecomposesageometryprobleminto\nstructuredreasoningsteps,formulatesthenecessaryequationsusingthePythagoreantheorem,andcallsthePythoncomputationtoolto\nverifyandcomputethecone‚Äôsvolume.Thetool-groundedreasoningensuresboththenumericalcorrectnessandinterpretabilityofthefinal\nsolution.\nReasoning Model Solution Reward Model Evaluation\nQuestion S T $ ro h f t ( o e e x t p ) s p = 1 r o : o x f I ^ b d e 2 l a e e + c n m h t x i f a f - y u s 6 t n k h $ c s e a t u i G n o s d n o t a $ a o l g n d ( d x e ) t t $ h e , e r w m n h c in i o c e m h t i h p s e a r r e n e p u t r m h e e s b m e e n r . t e o d f c b o y m a m gr o a n p h ro . o T t o s d b o e t t h w is e , e w n e t w m o u f s u t n fi c n t d io t n h s e , Step 1 Correct { { { a ‚úì ‚úì ‚úì n } } } d C C A g l o p e r m a p a p r r p o h p l p e i r c r o t i a e a b l t l s a e e c n m s o a t p l r s y a e t s t a : e i t s g R e y m e : c F e o i n n g t n d : i i C z n e o g d r r r o e n o c e t t e s ly d s i e d t p e o a n h r t a i a f t i n e e d d ly l e t t h h e b e o n g t o c h a o l a m lg p e a b r r in a g ic < im to p o o l r > t T s o y o m l p C y a a l s li s n p g",
    "char_length": 1424
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 49,
    "text": "Step 2: Find the Roots of $f(x)$ {‚úì}Explicit methodology: Stated the approach clearly # Define the function\nT w f a T T a h h h n e c e i e d s t s o r s - g e o 2 r e i i t o v . t n $ e t t o g s $ h s . f ( e o W u x r o f s e + e o a q t 3 t w l f u s o ) u ( a o o f n x o t k d c i - r o t i f 2 $ s i n o o ) t f r ( i n = t n x t o w ) a c 0 $ r t z $ o e e i r $ s o n r t o $ h o u : \\ e t m $ { s - $ v b 3 f a x o , e ^ l r 2 u r 2 s $ \\ e } f t s + $ ( h x . o a x )$ f t - : $ m $ x 6 $ $ u x = l f _ t o i 1 0 p r $ = l w y $ - h t 3 W o i c \\ - e q h 6 u t c a h a a n e d n d f \\ u s t a e o n d x l c d v t t { e i a t o o n t n h d + 's i } 1 s \\ v . q q a T u u l h u a a e e d d s i e r x s a _ n z t 2 i u e c = m r o e 2 b . q $ F e u $ o r a s r t $ a io f r n ( e x ) + b $ 3 y , Step 2 Correct { { 6 { { { ‚úì ‚úì ‚úì ‚úì ‚úì $ } } } } } a C V A C C n a c o o le d l c r r i a r r d u s e e r r u c c f a p a m t t t r c e e e r o t t s q f o o o a e u r t $ c n i a z e + t t t a o a x i 1 t o r t t $ i i i r n o z o a n a n s c t e : t m i i o W t o u n e n e p t : : h l : $ l $ o $ - ( x o x d x r = ^ + : g 2 I - a 3 d 3 n + ) e , ( i x x n z 2 e t - $ - i d 2 f 6 i ) e a = = d l g 0 0 f e $ a $ b c r t a o ic rs s t t h e a p t s multiply to $- x f # r p < o = t r = S o o i n x o s o t * t s l p l ( v * > \" . = e 2 R s s y o f + o m p o x r . t s b t - s o h o 6 o l e l v s f e r ( f o ' ( ( x x f o ' , ) ) t : x s \" ) , roots)",
    "char_length": 1457
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 50,
    "text": "Step 3: Find the Roots of $g(x)$ from the Graph Step 3 Incorrect {‚úì} Correct method: Identifying x-intercepts for roots\nThe roots of a function shown on a graph are its x-intercepts, which are the points where {‚úì} Appropriate visual approach: Using graph inspection\nt g h (x e ) $ c , u w rv e e c c a r n o i s d s e e n s t i t f h y e th h e o s r e iz p o o n i t n a t l s . x-axis. By visually inspecting the provided graph of $y = { { ‚ùå ‚ùå } } C N R o I v T e IC rif A ic L a E t R io R n O : F R a : i I l n e c d o t r o re c c h t e r c o k o t r e id a e d n in ti g f i a c g a a ti i o n n st graph properties\nThe parabola intersects the x-axis at:$$x_1 = -1 \\quad \\text{and} \\quad x_2 = 2$$ {‚ùå}Symmetry violation: Claimed roots violate parabola symmetry\nThe set of roots for $g(x)$ is $\\{-1, 2\\}$.\n{‚úì} Clear presentation: Well-organized comparison\nStep 4: Compare the Roots of $f(x)$ and $g(x)$ Step 4 Correct {‚úì} Correct logical structure: Proper set intersection approach\nNow, we compare the sets of roots we found for both functions:Roots of $f(x)$: $\\{-3, 2\\}$; {‚úì} Accurate computation: Given the inputs, correctly identified\nRoots of $g(x)$: $\\{-1, 2\\}$. The common root is the value that appears in both common root\nsets.$$\\text{Common Root(s): } \\{2\\}$$ {‚ùå}Propagated error: Used incorrect $g(x)$ roots from Step 3\nEach function also has a root that is not shared: $x = -3$ for $f(x)$ and $x = -1$ for $g(x)$. {!}Fortuitous result: Error did not affect final answer",
    "char_length": 1487
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 51,
    "text": "S B t a e s p e d 5 : o F n in t a h l e C c o o n m c p lu a s r i i o so n n, the functions $f(x)$ and $g(x)$ share exactly one root ($x=2$), Step 5 Correct { { ‚úì ‚úì } } C Ac o c rr u e r c a t t e c o m n a c p lu p s in io g n : : C O o n rr e e c c o tl m y l m in o k n e d ro t o o t c i h d o e i n c t e if i B ed\nand each function has another root that is unique to it. This corresponds to choice B. {‚úì} Complete reasoning: Acknowledged unique roots for each function\n{!} Lucky correctness: Final answer correct despite intermediate error\nFinal Answer: B Answer Correct\nFigure11.Evaluator-basedprocessverification.ThisexampleshowcasestheVerifierroleofAgent0-VL,wherethemodelcritically\ninspects each reasoning step produced by the Solver in a function-root comparison task. The model performs step-level judgments\n(Correct/Incorrect),identifiespropagationerrors,andrecognizeswhenthefinalconclusionremainsvaliddespiteintermediatemistakes.This\ndemonstratesthemodel‚Äôscapacityforfine-grainedself-evaluationandprocess-levelreasoninganalysis.\nModel Response\n<Think>\nQuestion:Below is the graph of Looking at this problem, I need to find the midline equation of a trigonometric function. I'm\na trigonometric function. It has given two critical points: a minimum at (-œÄ/2, 2.4) and a maximum at (3œÄ/2, 13). Let me think\na minimum point at $\\left(- through this systematically and use code to verify my reasoning. Tool Calling\n\\frac{1}{2} \\pi, 2.4\\right)$ and",
    "char_length": 1459
  },
  {
    "paper_id": "Agent0VL",
    "chunk_id": 52,
    "text": "a maximum point at The midline of any trigonometric function is the horizontal line exactly halfway between its <tool>\n$\\left(\\frac{3}{2} \\pi, maximum and minimum y-values. This is because trigonometric functions oscillate min_y = 2.4\n1 e 3 q \\ ua ri t g io h n t )$ of . W th h e a t f i u s n c t t h i e o n m ? id G li i n ve e symmetrically around their midline max_y = 13\nan exact expression. Step 1 I have: Minimum point: (-œÄ/2, 2.4); Maximum point: (3œÄ/2, 13) # m C id a li l n c e u l = a t ( e m m in i _ d y l i + n e max_y) / 2\nGoal: Find exact midline equation print(f\"Midline = ({min_y} +\nThe x-coordinates don't matter for the midline calculation - only the y-values do. {max_y}) / 2 =\n{midline}\")</tool>\nStep 2 For the midline: Midline = (Maximum y-value + Minimum y-value) / 2\nLet me calculate this:\nTool Calling\nThis gives me: Midline = (2.4 + 13) / 2 = 7.7\nTool execution Tool execution\nThrough systematic reasoning and computational verification, I've determined that the <sandbox\\_output> 32.45\nmidline equation is: y = 7.7 </sandbox\\_output>\n</think>\n<answer>\n7.7\n</answer>\nFigure12.Analyticalreasoningwithvisualgrounding.Inthistrigonometricmidlineproblem,Agent0-VLinterpretsgraphicalinput,\nreasonsstep-by-stepthroughsymboliccomputation,andvalidatesitsreasoningusingcodeexecution.Bycombiningperceptualunderstanding\nandanalyticalcomputation,themodelachievesconsistentreasoninggroundedinbothmathematicalandvisualevidence.",
    "char_length": 1442
  }
]