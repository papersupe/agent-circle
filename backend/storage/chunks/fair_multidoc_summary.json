[
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 0,
    "text": "Improving Fairness of Large Language Models in Multi-document\nSummarization\nHaoyuanLi1,RuiZhang2,SnigdhaChaturvedi1\n1UniversityofNorthCarolinaatChapelHill,2PennsylvaniaStateUniversity\n{haoyuanl,snigdha}@cs.unc.edu,{rmz5227}@psu.edu\nAbstract level. Summary-levelfairnessmeasureshowfairly\na summary represents documents with different\nFairness in multi-document summarization\nsocialattributevalues. Corpus-levelfairnessmea-\n(MDS)iscrucialforprovidingcomprehensive\nsureshowfairlyacorpusofsummariesasawhole\nviews across documents with diverse social\nrepresentsdifferentsocialattributevalues.\nattributevalues,whichcansignificantlyimpact\ndecision-making. Forexample,asummariza- Recent studies (Zhang et al., 2023; Li et al.,\ntionsystemthattendstooverrepresentnegative 2024) find that modern summarization methods\nreviews of products can mislead customers likeLLMsstrugglewithbothsummary-leveland\ninto disregarding good products. Previous corpus-level fairness. To improve the summary-\nworksmeasurefairnessinMDSattwolevels:\nlevelfairness,Zhangetal.(2023)promptLLMsto\nsummary-level and corpus-level. While\ngeneratesummariesbasedonthedistributionofso-\nsummary-levelfairnessfocusesonindividual\ncialattributesamongdocuments. However,itrelies\nsummaries, corpus-level fairness focuses\nonusers’priorknowledgeoffairnessissuesandso-\non a corpus of summaries. Recent methods\nprimarily focus on summary-level fairness. cialattributes,limitingitseffectivenessinpractice.",
    "char_length": 1452
  },
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 1,
    "text": "We propose FairPO, a preference tuning Huang et al. (2024) improve the summary-level\nmethod that focuses on both summary-level fairnessofT5(Raffeletal.,2020)bypolicygradi-\nandcorpus-levelfairnessinMDS.Toimprove ent,buttheirmethodmaynotgeneralizetomodern\nsummary-levelfairness,weproposetogenerate\nmodels like LLMs. Furthermore, both methods\npreferencepairsbyperturbingdocumentsets.\nfocusexclusivelyonsummary-levelfairness,over-\nToimprovecorpus-levelfairness,wepropose\nlookingthecorpus-levelfairness.\nfairness-aware preference tuning by dynami-\ncallyadjustingtheweightsofpreferencepairs. We propose FairPO (Fair Preference Optim-\nOurexperimentsshowthatFairPOoutperforms ization),apreferencetuning(Ziegleretal.,2019)\nstrongbaselineswhilemaintainingthecritical method that focuses on both summary-level and\nqualitiesofsummaries.Thecodeisavailableat corpus-levelfairnessofLLMsinMDS.Whilepre-\nhttps://github.com/leehaoyuan/coverage_fairness.\nviousworks(Stiennonetal.,2020;Roitetal.,2023)\nusespreferencetuningtoimproveotherqualities\n1 Introduction\nofsummaries,FairPOisthefirsttousepreference\nMulti-document summarization (MDS) aims to tuningforthefairnessinMDS.FairPOisbasedon\nsummarize the salient information from multiple Direct Preference Optimization (DPO) (Rafailov\ndocuments about an entity, such as reviews of a etal.,2024). Tooptimizesummary-levelfairness,\nproduct. Eachofthesedocumentsisgenerallyas- FairPOgeneratespreferencepairsgivenperturbed",
    "char_length": 1448
  },
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 2,
    "text": "sociatedwithasocialattributessuchassentiments input document sets by removing a small subset\ninreviews. Thesedocumentswithdifferentsocial of documents with certain social attribute values.\nattributevaluese.g. positivesentimentornegative To further improve corpus-level fairness, FairPO\nsentimenttendtohavediverseinformationorcon- performsfairness-awarepreferencetuningbydy-\nflicting opinions. It is crucial that the summary namicallyadjustingtheweightsofpreferencepairs.\nfairly represents conflicting information since it WeconductanempiricalevaluationofFairPO\ncansignificantlyimpactdecision-making. using three LLMs: Llama3.1 (AI@Meta, 2024),\nPreviousworks(Shandilyaetal.,2018;Olabisi Mistral (Jiang et al., 2023), and Gemma2 (Team\netal.,2022;Huangetal.,2024)measurefairness et al., 2024), on the Amazon (Ni et al., 2019),\nin MDS at two levels: summary-level or corpus- MITweet(Liuetal.,2023),andSemEvaldatasets\n1143\nProceedingsofthe63rdAnnualMeetingoftheAssociationforComputationalLinguistics(Volume2:ShortPapers),pages1143–1154\nJuly27-August1,2025©2025AssociationforComputationalLinguistics\n(Mohammadetal.,2016). Ourexperimentsshow The average coverage probability, p(d,s), is\nthat FairPO outperforms strong baselines while then calculated by averaging coverage probabil-\nmaintainingothercriticalqualitiesofsummaries, ity, p(d ,s), across all documents in the docu-\ni\nsuchasrelevanceandfactuality. ment set D. Using these values, Equal Cover-",
    "char_length": 1446
  },
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 3,
    "text": "Ourcontributionsareasfollows: agecalculatesthecoverageprobabilitydifference\n• We propose FairPO to improve the fairness of c(d i ,S) = p(d i ,s) p(d,s). Equal Coverage\n−\nLLMsinMDS; valueEC(D,S)isthencalculatedastheaverage\n• We propose to improve summary-level and of the absolute average coverage probability dif-\ncorpus-levelfairnessbyperturbation-basedpref- ference c(d i ,S) for documents with each social\nerencepairgenerationandfairness-awareprefer- attributevalue:\nencetuning;\nK\n1\n• Weperformcomprehensiveexperimentstoshow\nEC(D,S) = E( c(d\ni\n,S) a\ni\n= k ) (3)\ntheeffectivenessofFairPO. K | { | } |\nk=1\nX\nAlowerEC(D,S)indicatesafairersummaryS.\n2 Background\nTo evaluate the fairness of a system, we use the\nInthissection,weprovidebackgroundknowledge averageEqualCoveragevalueacrossthecorpusG.\nonfairnessinMDS.LetGdenotealldocumentsets\nin a corpus for MDS. Each document set D G Coverage Parity examines whether certain so-\n∈\ncontains multiple documents d ,...,d , where cial attribute values are systematically overrepre-\n1 n\n{ }\neachdocumentd islabeledwithasocialattribute sented or underrepresented across the corpus G.\ni\na 1,...,K . ForeachdocumentsetD,aMDS CoverageParitycollectsthesecoverageprobabil-\ni\n∈ { }\nsystemissupposedtogenerateasummaryS. ity differences c(d i ,S) from all input documents\nToevaluatefairnessinMDS,weuseEqualCov- of the dataset G whose social attribute value is k\nerage EC(D,S), a summary-level measure, and intoasetC k . ThecoverageParityvalueCP(G)is",
    "char_length": 1483
  },
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 4,
    "text": "CoverageParityCP(G),acorpus-levelmeasure, then calculated as the average of the absolute av-\nproposedbyLietal.(2024). Below,wesummarize eragecoverageprobabilitydifferencec(d i ,S)for\ntheseconceptsasintroducedintheoriginalpaper. documentswitheachsocialattributevalue:\nEqualCoverage examineswhethereachsocial 1 K\nattributevaluehasequalprobabilitiesofbeingcov-\nCP(G) =\nK |\nE(C\nk\n)\n|\n, (4)\nered by the summary S for a document set D. X k=1\nAlowerCP(G)indicatesafairersystem. Formore\nSpecifically,itfirstdefinescoverageprobability\ndetails,pleaserefertoLietal.(2024).\ndifference c(d i ,S) as the difference between the\ncoverageprobabilityforthedocumentd ,p(d ,s).\ni i\n3 FairPO\nIt also defines the average coverage probability\nacrossalldocuments,p(d,s). Toestimatethecov- In this section, we describe our proposed prefer-\nerage probability for the document d i , p(d i ,s), encetuningmethod,FairPO.\nFairPO estimates the probability p(d ,s ) that a\ni j\ndocumentd iscoveredbyasummarysentences . 3.1 Perturbation-basedPreferencePair\ni j\nSpecifically,theprobabilityp(d ,s )isestimated Generation\ni j\nasthemaximumentailmentprobabilityp(d i,l ,s j ) Inthissection,wedescribehowtogenerateprefer-\nbetweenanydocumentchunkd i,l ofthedocument encepairsbasedonperturbation. Apreferencepair\nd i and the summary sentence s j using an entail- forFairPOcontainsachosensummaryS c andare-\nmentmodel: jectedsummaryS forthedocumentsetD. Ideally,\nr\nthe chosen and rejected summaries should differ",
    "char_length": 1469
  },
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 5,
    "text": "p(d ,s ) = max p(d ,s ) d d , (1)\ni j i,l j i,l i\n{ | ∈ } significantly in representing documents with dif-\nThe coverage probability for the document d , ferentsocialattributevalues. Tothisend,FairPO\ni\np(d ,s), is then estimated as the average of the generatessummariesforperturbedinputdocument\ni\nprobabilityp(d ,s ): sets,wheresmallsubsets(α%)ofdocumentswith\ni j\nspecificsocialattributevaluesareremoved.\n1\np(d ,s) = p(d ,s ), (2) Specifically,FairPOfirstgeneratesasummaryS\ni i j\nS\nfortheinputdocumentsetDandidentifiesitsmost\n| | s Xj∈ S\n1144\noverrepresented,k+,andunderrepresented,k ,so- Domain Soci.Attr. Soci.Attr.Val. Doc.SetSize Doc.Len\n−\ncialattributevalue. Forthecompletenessofinfor- negative,neutral,\nAmazon Review Sentiment 8 40\npositive\nmation,FairPOonlyconsiderssocialattributeval- left,center,\nMiTweet Tweet Ideology 20 34\nright\nuesthatappearinmorethanα%ofthedocuments\nSemEval Tweet Stance support,against 30 17\n(detailsinApp. A.4). Thesearedeterminedbased\nTable 1: Dataset statistics. Doc. Set Size means size\non the highest or lowest average coverage prob-\nofdocumentsets. Doc. Len. meansaveragelengthof\nability differences, E( c(d\ni\n,S) a\ni\n= k ). Then,\n{ | } documents.\nFairPOgeneratessummaryS+ andS fortheper-\n−\nturbedinputdocumentsetwhereα%ofrandomly\nsampleddocumentswithsocialattributevaluea thesocialattributevaluek ifthesumofcoverage\ni\nof k+ and k are removed. Among summaries probability differences, C (D,S ), is greater or\n− k",
    "char_length": 1451
  },
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 6,
    "text": "S, S+, S , FairPOselectsthesummarywiththe less than zero respectively. In ea ∗ ch training step,\n−\nlowest Equal Coverage value, indicating the best FairPO estimates the overrepresentation O(k) of\nsummary-level fairness, as the chosen summary socialattributevaluek:\nv S a c l . u T e h i e ss s e u l m ec m te a d ry as w t i h th e t r h e e je h c i t g e h d e s s u t m Eq m u a a r l y C S ov . erage O(k) = (D,S) ∈ T k + | C k (D,S) |· π θ (S | D)/ | S |\nr π (S D)/ S\nP (D,S) ∈ T k + θ | | |\n3.2 Fairness-awarePreferenceTuning (7)\nP\nwhereT+ isthesetofdocumentsetsD andcorre-\nIn this section, we describe fairness-aware pref- k\nspondingchosenorrejectedsummariesthatover-\nerence tuning that optimizes summary-level and\nrepresentsocialattributevaluek (C (D,S ) > 0)\nk\ncorpus-levelfairness. Toachievethis,FairPOdy- ∗\ninrecenttrainingsteps. Similarly,FairPOestimates\nnamicallyassignsseparateweightsforthechosen\nsummary S and the rejected summary S based\ntheunderrepresentationU(k)usingthesetT k− of\nc r documentsetsandsummariesthatunderrepresent\nonestimatedcorpus-levelfairnessduringtraining.\nsocialattributevaluek (C (D,S ) < 0)asEq. 7.\nk\nFairPO modifies the DPO objective (more ex- ∗\nUsingtheoverrepresentationO(k)andunderrep-\nplanations in App. A.3) and introduces separate\nresentation U(k), FairPO assigns weight w and\nc\nweights, w and w , for the chosen summary S\nc r c w . Chosensummariesthathelpbalanceoverrep-\nr\nandrejectedsummaryS respectively:",
    "char_length": 1455
  },
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 7,
    "text": "r resentationO(k)andunderrepresentationU(k)re-\nπ (S D) π (S D) ceive higher weights and vice versa for rejected\nθ r θ c\nσ( m)β(w log | w log | )\n− r π ref (S r D)− c π ref (S c D) summaries. Forexample,theweightw c shouldbe\n| |\n(5) higherifasystematicallyunderrepresentedsocial\nwhere σ is the sigmoid function, π is the policy attributevaluek(U(k) > O(k))isoverrepresented\nθ\nmodel, π ref is the reference model, and m is the bythechosensummaryS (C (D,S ) > 0). For\nc k c\nrewardmarginasinDPO: socialattributevaluek,FairPOcomputesaninter-\nπ (S D) π (S D) mediate weight w for the chosen summary S :\nθ c θ r c,k c\nm = βlog | βlog | (6)\nπ (S D) − π (S D)\nref c ref r 2\n| |\nw = (8)\nc,k\nThetermσ( m)inEq. 5servesasascalingfactor\n1+(O(k)/(U(k))Ck(D,Sc)/τ\n−\nandFairPOdoesnotconsideritsgradient. whereτ isthetemperature. Theweightw forcho-\nc\nFairPOassignsweightsw andw tosummaries sensummariesistheaverageintermediateweight\nc r\nbasedontheirimpactoncorpus-levelfairness. Itas- w c,k acrossallsocialattributevalues. Theweight\nsignshighweightsw c tochosensummariesthatim- w r fortherejectedsummaryS r iscomputedsimi-\nprovecorpus-levelfairnessbybalancingtheover- larlywiththeintermediateweightw r,k :\nrepresentationandunderrepresentationofsocialat-\n2\ntributevalues. Conversely,itassignshighweights w r,k = (9)\n1+(U(k)/(O(k))Ck(D,Sr)/τ\nw to rejected summaries that hurt corpus-level\nr\nfairness. Toestimatecorpus-levelfairness,FairPO The design ensures that summaries improving\ncorpus-levelfairnessareprioritized.",
    "char_length": 1499
  },
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 8,
    "text": "computes the sum of coverage probability differ-\nences for documents with social attribute values\n4 Experiments\nofk,C (D,S ) = c(d,S )foreach\nchosen k orrejec ∗ tedsum d m∈{a d r i y| a , i S =k .}Asum ∗ maryS is Inthissection,wedescribeexperimentsoffinetun-\nP ∗ ∗\nconsideredoverrepresentingorunderrepresenting ingmodelswithFairPO.\n1145\nAmazon MITweet SemEval Overall ueslikeFairPOforafaircomparison;(ii)OPTune\nEC CP EC CP EC CP EC CP\n↓ ↓ ↓ ↓ ↓ ↓ ↓ (Chenetal.,2024),whichselectsthechosenandre-\nLlama3.1 7.95 1.89 4.50 0.59 2.98 1.41 5.14 1.30\n+DPO 7.23 1.27 4.25 0.47 2.66 1.09 4.72 0.94 jectedsummariesasDPOandweightspreference\n+OPTune 6.70 0.62 4.33 0.51 2.60 0.95 4.54 0.69 pairs based on EC value differences; (iii) Policy\n+Prompt 7.42 1.64 4.36 0.45 2.62 0.29 4.80 0.79\ngradients (Lei et al., 2024) and (iv) a prompting\n+PolicyG. 7.73 1.88 4.51 0.55 2.97 1.38 5.07 1.27\n+FairPO 6.87 0.42 4.24 0.42 2.49 0.66 4.53 0.50 method (Zhang et al., 2023). Implementation de-\nMistral 8.36 2.83 4.16 0.61 2.83 1.27 5.12 1.57\ntailsofthesebaselinesareinAppA.5. Forevalua-\n+DPO 7.20 1.82 3.55 0.34 2.41 0.93 4.39 1.03\n+OPTune 6.85 0.88 3.58 0.51 2.07 0.57 4.17 0.65 tion,weconsidersummary-levelandcorpus-level\n+Prompt 7.74 1.92 3.97 0.37 2.35 0.36 4.68 0.88 fairnessusingEqualCoverage(EC)andCoverage\n+FairPO 6.32 0.46 3.70 0.40 2.10 0.43 4.04 0.43\nParity(CP)(Lietal.,2024). Alowervalueisbetter\nGemma2 8.32 2.48 4.20 0.60 2.81 0.96 5.11 1.35",
    "char_length": 1432
  },
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 9,
    "text": "+DPO 6.90 0.91 4.04 0.40 2.44 0.56 4.46 0.62 forthesemeasures. Wereporttheaverageresults\n+OPTune 6.84 0.88 3.89 0.57 2.32 0.49 4.35 0.65 onthreesplittingsoftraining,validationandtest-\n+Prompt 7.28 1.16 4.33 0.32 2.73 0.48 4.78 0.65\n+FairPO 6.18 0.44 3.76 0.48 2.50 0.45 4.15 0.46 inginTab. 2.Weadditionallyreporttheresultsfor\neachsplittinginApp. A.6. WeobservethatFairPO\nTable 2: Summary-level fairness (EC) and corpus-\noutperformsothermethodsformostLLMsonall\nlevelfairness(CP)ofsummariesgeneratedbydifferent\ndatasetsandyieldsthebestoverallperformancefor\nmethods. Thebestperformingmethodisinbold. The\nallLLMs. TheresultsshowthatFairPOimproves\nsecond-bestperformingmethodisunderlined. FairPO\nhasthebestoverallperformance. bothsummary-levelandcorpus-levelfairness.\n4.4 AblationStudy\n4.1 Datasets\nTovalidatetheeffectofperturbation-basedprefer-\nWeexperimentonthreedatasets: Amazon(Nietal., encepairgenerationandfairness-awarepreference\n2019),MITweet(Liuetal.,2023),SemEval(Mo- tuning, we compare FairPO with its ablated ver-\nhammad et al., 2016) datasets. Each dataset in- sions. Weconsiderthefollowingablatedversions:\ncludes1000samplesfortraining,300samplesfor (i)(w/opert.),wherethechosenandrejectedsum-\nvalidation, and 300 samples for testing. The di- mariesareselectedamongthreerandomlysampled\nvision of training, validation, and testing sets is summaries based on Equal coverage values; (ii)\nbasedonstratifiedsamplingofsocialattributeval- (w/o fair.) that performs preference tuning using",
    "char_length": 1490
  },
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 10,
    "text": "uesandtopics. Tab. 1showsthestatisticsofthese the DPO objective instead of the fairness-aware\ndatasets. Thesummarylengthis50words. Details preferencetuning;(iii)(w/orew.) thatdirectlyas-\nofpreprocessingareinApp. A.1. signsweightsw andw intheDPOobjective(Eq.\nc r\n13),whichunderminestheeffectivenessofreward\n4.2 ImplementationDetails\nmargin (more explanations in App. A.3). Tab.3\nWe perform experiments with three LLMs: reports the results for each dataset, and Overall\nLlama3.1-8b-Instruct(AI@Meta,2024),Mistral- scores,whichistheaverageacrossalldatasets. A\n7B-Instruct-v0.3(Jiangetal.,2023),Gemma-2-9b- lowervalueindicatesbetterfairness.\nit(Teametal.,2024). EachLLMistrainedfor2 From the table, we observe that FairPO yields\nepochsusingLoRA(Huetal.,2021)withalearn- the best overall performance compared to its ab-\ningrateof5e 5andbatchsizeof16. Togenerate latedversions. Theresultsshowtheeffectiveness\n−\npreferencepairs,FairPOremovesα = 10%ofdoc- of perturbation-based preference pair generation\numents. The temperature τ is 1 on the MITweet andfairness-awarepreferencetuning. Italsopro-\ndataset, 2 for Mistral and 1 for other LLMs on videsempiricalevidencesforthedesignchoiceof\ntheAmazondataset,3forMistraland2forother objectiveofFairPO.\nLLMsontheSemEvaldataset. Allhyperparame-\n4.5 HumanEvaluationofFairPO\nters are tuned on the validation set. More details\nareinApp. A.4. Weperformahumanevaluationtocomparethefair-\nnessofsummariesgeneratedbyLLMstunedwith\n4.3 AutomaticEvaluationofFairPO",
    "char_length": 1490
  },
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 11,
    "text": "DPOandFairPO.ForeachLLM,werandomlyse-\nWecompareFairPOwiththefollowingbaselines: lect10pairsofsummariesgeneratedbytheLLM\n(i)DPO(Rafailovetal.,2024),wherethechosen tunedwithDPOorFairPO,yieldingatotalof30\nandrejectedsummariesareselectedamongthree pairs. Each pair is annotated by three annotators\nrandomly sampled summaries based on EC val- recruitedfromAmazonMechanicalTurk. Annota-\n1146\nAmazon MITweet SemEval Overall forapairofsummaries,weinstructPrometheus2\nEC CP EC CP EC CP EC CP\n↓ ↓ ↓ ↓ ↓ ↓ ↓ (7B)(Kimetal.,2024)toselectthebettersummary\nLlama3.1\nFariPO 6.57 0.37 4.20 0.26 2.39 0.56 4.39 0.39 inthreedimensions: fluency,relevance,andfactu-\nw/opert. 7.01 0.48 4.07 0.34 2.54 0.81 4.54 0.54 ality. Tomitigatepositionbias(Huangetal.,2023),\nw/ofair. 6.70 0.95 4.26 0.31 2.29 0.65 4.42 0.64\nwe perform the pairwise comparison twice with\nw/orew 6.48 0.79 4.19 0.27 2.60 0.86 4.42 0.64\nMistral different orders of summaries and only consider\nFariPO 6.98 0.89 3.56 0.21 1.97 0.36 4.17 0.49 consistent results. Tab. 4 reports the differences\nw/opert. 7.29 1.64 3.81 0.21 2.30 0.26 4.47 0.71\nbetweenthewinningandlosingratesofdifferent\nw/ofair. 7.31 1.36 3.57 0.25 2.21 0.66 4.37 0.76\nw/orew 7.05 1.26 3.65 0.14 2.06 0.55 4.25 0.65 methods. Apositivevalueindicatessummaryqual-\nGemma2 ityisbettercomparedtooriginalLLMs.\nFariPO 6.09 0.33 3.84 0.47 2.53 0.59 4.15 0.46\nw/opert. 6.18 0.19 4.17 0.21 2.43 0.53 4.26 0.31 From the table, we observe that the quaility of",
    "char_length": 1456
  },
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 12,
    "text": "w/ofair. 6.77 1.11 3.84 0.51 2.39 0.59 4.34 0.74 summariesgeneratedbyLLMstunedwithFairPO\nw/orew 6.89 0.90 3.94 0.40 2.49 0.44 4.44 0.58\niscomparablewithsummariesgeneratedbyorigi-\nTable3: Summary-levelfairness(EC)andcorpus-level nalLLMs. Contrarily,promptingsignificantlyhurt\nfairness(CP)ofsummariesgeneratedbyablatedver- the quality of summaries. The results show that\nsionsofFairPO.Thebestperformingmethodisinbold. FairPOimprovesthefairnessofsummarieswhile\nFairPOhasthebestoverallperformance. maintainingtheirquality.\nLlama3.1 Mistral Gemma2\nflu. rel. fac. flu. rel. fac. flu. rel. fac. 5 Conclusion\n↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑\nDPO 7.56 8.33 2.78 5.11 11.5611.56 5.11 1.11 8.67\nOPTune 1.00 0.44 -6.89 -0.78 6.78 8.89 7.00 11.67 11.67\nPrompt -15.33-19.22-24.44-0.44 -6.00 -5.56 -42.67-50.78-51.44 We propose FairPO, a preference tuning method\nFairPO 5.78 3.11 2.89 2.11 5.33 9.11 11.44 16.11 9.44\nthatoptimizessummary-levelfairnessandcorpus-\nTable4: Pairwisecomparisonofqualitybetweensum- level fairness in MDS. Specifically, FairPO gen-\nmariesgeneratebyLLMsbeforeandaftertuning. Sta- eratespreferencepairsusingperturbeddocument\ntisticalsignificantdifferences(p<0.05)accordingto sets to improve summary-level fairness and per-\npairedbootstrapresampling(Koehn,2004)areunder- formsfairness-awarepreferencetuningtoimprove\nlined. FairPOdoesnotaffectsummaryquality.\ncorpus-levelfairness. Ourexperimentsshowthat\nFairPOoutperformsstrongbaselineswhilemain-\ntainingcriticalqualitiesofsummaries.",
    "char_length": 1476
  },
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 13,
    "text": "torsareaskedtoreadallcorrespondingdocuments\nandselectthefairersummary. WechoosetheAma-\nzondatasetsinceeachdocumentsetonlycontains 6 Limitation\neightreviews(Tab. 1)andjudgingthesentimentof\nanopinionisrelativelyeasyforcommonusers. The Our experiments demonstrate FairPO’s effective-\nRandolph’sKappa(Randolph,2005)betweenan- nessinimprovingbothsummary-levelandcorpus-\nnotationsofthreeannotatorsis0.40,whichshows level fairness of summaries within individual do-\namoderatecorrelation. Thecorrelationisexpected mains. Whilethisworkfocusesonoptimizingfair-\nconsideringthesubjectivityofthetask. Morede- nesswithinasingledomain,extendingFairPOto\ntailsareinApp. A.2. improve fairness simultaneously across multiple\nOutof30pairs,summariesgeneratedbyFairPO- domains with diverse social attributes presents a\ntunedLLMsarefairerin18pairsandsummaries promising future direction. Besides, FairPO cur-\ngenerated by DPO-tuned LLMs are fairer in 9 rently selects the two summaries with the largest\npairs.Thedifferenceisstatisticallysignificant(p < fairnessdifferencesamongthethreegeneratedsum-\n0.05)usingbootstrap(Koehn,2004). Theresults mariesforpreferencetuning,followingcommonly\nshowthatFairPOperformsbetterthanDPOinim- used practices of DPO. Exploring approaches to\nproving fairness. We additionally show example utilize all three summaries generated by FairPO\nsummariesgeneratedbyFairPOinApp. A.7. canbeanotherinterestingfuturedirection.\n4.6 EvaluationofSummaryQuality\n7 Acknowledgment",
    "char_length": 1473
  },
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 14,
    "text": "ToevaluateFairPO’simpactonsummaryquality,\nwecomparesummariesgeneratedbyLLMsbefore This work was supported by NSF grant DRL-\nandaftertuningtoimprovefairness. Specifically, 2112635and2338418.\n1147\n8 EthicalConsideration YuanyuanLei,KaiqiangSong,SangwooCho,Xiaoyang\nWang, Ruihong Huang, and Dong Yu. 2024. Po-\nThedatasetsweuseareallpubliclyavailable. We laritycalibrationforopinionsummarization. arXiv\ndonotannotateanydataonourown. Allthemod- preprintarXiv:2404.01706.\nelsusedinthispaperarepubliclyaccessible. The\nHaoyuan Li, Yusen Zhang, Rui Zhang, and Snigdha\ninferenceandfinetuningofmodelsareperformed Chaturvedi. 2024. Coverage-based fairness\nononeNvidiaA6000orNvidiaA100GPU. in multi-document summarization. Preprint,\narXiv:2412.08795.\nWe perform human evaluation experiments on\nAmazon Mechanical Turk. The annotators were Songtao Liu, Ziling Luo, Minghua Xu, LiXiao Wei,\ncompensatedatarateof$20perhour. Duringthe Ziyao Wei, Han Yu, Wei Xiang, and Bang Wang.\n2023. Ideologytakesmultiplelooks: Ahigh-quality\nevaluation,humanannotatorswerenotexposedto\ndatasetformultifacetedideologydetection. InThe\nanysensitiveorexplicitcontent.\n2023ConferenceonEmpiricalMethodsinNatural\nLanguageProcessing.\nSaif Mohammad, Svetlana Kiritchenko, Parinaz Sob-\nReferences\nhani, Xiaodan Zhu, and Colin Cherry. 2016.\nAI@Meta.2024. Llama3modelcard. SemEval-2016 task 6: Detecting stance in tweets.\nInProceedingsofthe10thInternationalWorkshop\nLichangChen,JiuhaiChen,ChenxiLiu,JohnKirchen-",
    "char_length": 1466
  },
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 15,
    "text": "onSemanticEvaluation(SemEval-2016),pages31–\nbauer, Davit Soselia, Chen Zhu, Tom Goldstein, 41,SanDiego,California.AssociationforComputa-\nTianyiZhou, andHengHuang.2024. Optune: Ef- tionalLinguistics.\nficient online preference tuning. arXiv preprint\nJianmo Ni, Jiacheng Li, and Julian McAuley. 2019.\narXiv:2406.07657.\nJustifyingrecommendationsusingdistantly-labeled\nreviews and fine-grained aspects. In Proceedings\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,\nof the 2019 Conference on Empirical Methods in\nYuanzhiLi, SheanWang, LuWang, WeizhuChen,\nNatural Language Processing and the 9th Interna-\netal.2021. Lora: Low-rankadaptationoflargelan-\ntional Joint Conference on Natural Language Pro-\nguagemodels. InInternationalConferenceonLearn-\ncessing (EMNLP-IJCNLP), pages 188–197, Hong\ningRepresentations.\nKong, China. Association for Computational Lin-\nguistics.\nKung-HsiangHuang,PhilippeLaban,AlexanderRFab-\nbri,PrafullaKumarChoubey,ShafiqJoty,Caiming OlubusayoOlabisi,AaronHudson,AntonieJetter,and\nXiong,andChien-ShengWu.2023. Embracediver- AmeetaAgrawal.2022. Analyzingthedialectdiver-\ngenceforricherinsights: Amulti-documentsumma- sityinmulti-documentsummaries. InProceedingsof\nrizationbenchmarkandacasestudyonsummariz- the29thInternationalConferenceonComputational\ning diverse information from news articles. arXiv Linguistics,pages6208–6221,Gyeongju,Republic\npreprintarXiv:2309.09369. ofKorea.InternationalCommitteeonComputational\nLinguistics.\nNannanHuang,HaythamFayek,andXiuzhenZhang.",
    "char_length": 1492
  },
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 16,
    "text": "2024. Bias in opinion summarisation from pre- LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,\ntrainingtoadaptation: Acasestudyinpoliticalbias. CarrollWainwright,PamelaMishkin,ChongZhang,\narXivpreprintarXiv:2402.00322. SandhiniAgarwal,KatarinaSlama,AlexRay,etal.\n2022. Training languagemodelsto followinstruc-\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Men- tions with human feedback. Advances in Neural\nsch,ChrisBamford,DevendraSinghChaplot,Diego InformationProcessingSystems,35:27730–27744.\ndelasCasas,FlorianBressand,GiannaLengyel,Guil-\nRafaelRafailov,ArchitSharma,EricMitchell,Christo-\nlaumeLample,LucileSaulnier,etal.2023. Mistral\npherDManning,StefanoErmon,andChelseaFinn.\n7b. arXivpreprintarXiv:2310.06825.\n2024. Directpreferenceoptimization:Yourlanguage\nmodelissecretlyarewardmodel. AdvancesinNeu-\nSeungone Kim, Juyoung Suk, Shayne Longpre,\nralInformationProcessingSystems,36.\nBillYuchenLin,JaminShin,SeanWelleck,Graham\nNeubig,MoontaeLee,KyungjaeLee,andMinjoon\nColinRaffel,NoamShazeer,AdamRoberts,Katherine\nSeo.2024. Prometheus2: Anopensourcelanguage\nLee,SharanNarang,MichaelMatena,YanqiZhou,\nmodelspecializedinevaluatingotherlanguagemod-\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nels. Preprint,arXiv:2405.01535.\nits of transfer learning with a unified text-to-text\ntransformer. Journalofmachinelearningresearch,\nPhilippKoehn.2004. Statisticalsignificancetestsfor 21(140):1–67.\nmachinetranslationevaluation. InProceedingsofthe",
    "char_length": 1440
  },
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 17,
    "text": "2004ConferenceonEmpiricalMethodsinNatural Justus J Randolph. 2005. Free-marginal multirater\nLanguage Processing, pages 388–395, Barcelona, kappa(multiraterk[free]): Analternativetofleiss’\nSpain.AssociationforComputationalLinguistics. fixed-marginalmultiraterkappa. Onlinesubmission.\n1148\nPaulRoit,JohanFerret,LiorShani,RoeeAharoni,Ge- sethasequalproportionsofdocumentsetsD dom-\noffreyCideron,RobertDadashi,MatthieuGeist,Ser- inatedbyeachsocialattributevalues. Wesample\ntanGirgin,LeonardHussenot,OrgadKeller,Nikola\n1000productsandtheircorrespondingreviewsfor\nMomchev, Sabela Ramos Garea, Piotr Stanczyk,\ntraining,300productsforvalidation,and300prod-\nNinoVieillard,OlivierBachem,GalElidan,Avinatan\nHassidim,OlivierPietquin,andIdanSzpektor.2023. uctsfortesting.\nFactually consistent summarization via reinforce-\nmentlearningwithtextualentailmentfeedback. In MITweet (Liu et al., 2023) consists of tweets\nProceedings of the 61st Annual Meeting of the As- withlabelsofpoliticalideologiesondifferentfacets\nsociationforComputationalLinguistics(Volume1: about different topics. The social attribute of a\nLong Papers), pages 6252–6272, Toronto, Canada.\ntweetwillbeleftifitisleftonmostfacets,rightif\nAssociationforComputationalLinguistics.\nitisrightonmostfacets,otherwiseneutral. First,\nAnuragShandilya,KripabandhuGhosh,andSaptarshi weevenlydividealltweetsofeachtopicintotwo\nGhosh.2018. Fairnessofextractivetextsummariza-\npartssothatthedistributionoftopicsisthesame",
    "char_length": 1456
  },
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 18,
    "text": "tion. In Companion Proceedings of the The Web\nbetweentwoparts. Foreachpart,weclustertweets\nConference2018,pages97–98.\naboutthesametopicbasedontheirTFIDFsimilar-\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel ityintoclusters. Wethendividetheseclustersinto\nZiegler, Ryan Lowe, Chelsea Voss, Alec Radford,\ninput document sets of 20 tweets about the same\nDarioAmodei,andPaulFChristiano.2020. Learn-\ntopic. We generate 1000 input document sets for\ningtosummarizewithhumanfeedback. Advances\ninNeuralInformationProcessingSystems,33:3008– trainingfromthefirstpartofthetweets. Similarly,\n3021. wegenerate300inputdocumentsetsforvalidation\nand 300 input document sets for testing from the\nGemma Team, Thomas Mesnard, Cassidy Hardin,\nsecondpartofthetweets. Whengeneratinginput\nRobertDadashi,SuryaBhupatiraju,ShreyaPathak,\nLaurentSifre,MorganeRivière,MihirSanjayKale, document sets of training, validation, and testing\nJuliette Love, et al. 2024. Gemma: Open models sets,wealsoperformstratifiedsamplingbasedon\nbased on gemini research and technology. arXiv\nthe distribution of social attribute values so that\npreprintarXiv:2403.08295.\neachsethasequalproportionsofdocumentsetsD\nYusenZhang, NanZhang, YixinLiu, AlexanderFab- dominatedbyeachsocialattributevalue.\nbri, Junru Liu, Ryo Kamoi, Xiaoxin Lu, Caiming\nXiong,JieyuZhao,DragomirRadev,etal.2023. Fair TweetStance (Mohammadetal.,2016)consists\nabstractive summarization of diverse perspectives. of tweets with labels of stance toward a target",
    "char_length": 1481
  },
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 19,
    "text": "arXivpreprintarXiv:2311.07884. phrasesuchasClimateChangeorHillaryClinton.\nFirst,weevenlydividealltweetsofeachtopicinto\nDanielMZiegler,NisanStiennon,JeffreyWu,TomB\nBrown, Alec Radford, Dario Amodei, Paul Chris- two parts so that the distribution of target phrase\ntiano, and Geoffrey Irving. 2019. Fine-tuning lan- isthesamebetweentwoparts. Weclustertweets\nguage models from human preferences. arXiv aboutthesametargetphrasebasedontheirTFIDF\npreprintarXiv:1909.08593.\nsimilarityintoclusters. Wethendividetheseclus-\nters into input document sets of 30 tweets about\nA Appendix\nthe same target phrase. We generate 1000 input\nA.1 Datasets document sets for training from the first part of\nthetweets. Similarly,wegenerate300inputdoc-\nInthissection,wedescribehowwepreprocessthe\numentsetsforvalidationand300inputdocument\ndatasets.\nsetsfortestingfromthesecondpartofthetweets.\nAmazon (Nietal.,2019)consistsofreviewswith Whengeneratinginputdocumentsetsoftraining,\nlabelsoftheirratingsofdifferentproducts. Wefil- validation,andtestingsets,wealsoperformstrat-\nteroutreviewsthatarenon-Englishorwithoutrat- ified sampling based on the distribution of social\nings. Weobtainthesocialattributeofeachreview attributevaluessothateachsethasequalpropor-\nbasedonitsratingprovidedinthedataset. Theso- tionsofdocumentsetsDdominatedbyeachsocial\ncialattributeofareviewwillbepositiveifitsrating attributevalue.\nis 4 or 5, neutral if its rating is 3, and negative if",
    "char_length": 1437
  },
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 20,
    "text": "itsratingis1or2. Toconstructtraining,validation A.2 HumanEvaluation\nand testing sets, we perform stratified sampling Weperformahumanevaluationtocomparethefair-\nbasedonthedistributionofsocialattributevalues nessofsummariesgeneratedbyLLMstunedwith\namongdocumentsetsforeachset. Therefore,each DPOandFairPO.ForeachLLM,werandomlyse-\n1149\nlect10pairsofsummariesgeneratedbytheLLM Comparing with the derivative of DPO objective\ntunedwithDPOorFairPO,yieldingatotalof30 (Eq. 10), the term σ( m) remains consistent in\n−\npairs. Tofurthersimplifytheevaluation, wecon- thederivativeofFairPOobjective.\nsiderdocumentsetswithonlynegativeandpositive Suppose we directly add seperate weights w\nc\nreviews. Each pair is annotated by three annota- andw forchosenandrejectedsummariestoDPO\nr\ntorsrecruitedfromAmazonMechanicalTurk. The objective. The corresponding objective is as fol-\nannotatorsshouldbefromEnglish-speakingcoun- lows:\ntries and have HIT Approval Rates greater than\nπ (S D)\n98%. For each pair, annotators are first asked to logσ(βw log θ c |\nc\n− π (S D)−\nread corresponding reviews and unique opinions ref c\n| (13)\nautomaticallyextractedbyGPT-4o-mini(Ouyang π θ (S r D)\nβw log | )\nr\net al., 2022). They then evaluate whether each π ref (S r D)\n|\nsummary reflects these opinions and classify the\nThecorrespondingderivativeisasfollows:\nsummaryasleaningnegative,fair,orleaningposi-\ntive. Eventually,theyareaskedtoselectthefairer\n∂π (S D)",
    "char_length": 1427
  },
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 21,
    "text": "summary in each pair. The interface of human σ( m ′ )β(w r π θ (S r D) − 1 θ r |\n− | ∂θ\nevaluationisshowninFig. 1. (14)\n∂π (S D)\nw c π θ (S c D) − 1 θ c | )\nA.3 RelationbetweenFairPOandDPO − | ∂θ\nTheFairPOobjective(Eq. 5)ismotivatedbythe wherem isaweightedrewardmargin:\n′\nderivateoftheDPOobjectivewithrespecttothe\nmodelparametersθ: π θ (S c D) π θ (S r D)\nβw log | βw log | (15)\nc r\nπ (S D) − π (S D)\n∂π (S D) ref c ref r\nσ( m)β(π θ (S r D) − 1 θ r | | |\n− | ∂θ\n(10) Comparingwithm, m islesseffectiveasamea-\n∂π (S D) ′\nπ θ (S c D) − 1 θ c | ) sureofthemodel’sabilitytodistinguishbetween\n− | ∂θ\nthechosensummaryS andtherejectedsummary\nc\nw m h o e d r e e l, σ π i r s ef th i e s t s h ig e m re o f i e d re f n u c n e ct m io o n d , e π l θ , a is nd th m e p i o s l t i h cy e S r sincethetermlog π π re θ f (S (S c | c D | D ) ) andlog π π re θ f (S (S r r | D | D ) )\nhave different weights. We additionally provide\nrewardmargininDPO:\nempiricalevidencesinApp.4.4.\nπ (S D) π (S D)\nθ c θ r\nβlog | βlog | (11)\nπ (S D) − π (S D) A.4 ImplementationDetails\nref c ref r\n| |\nTherewardmarginmcanbeviewedasameasure To reduce training cost, we perform LoRA (Hu\nof the model’s ability to distinguish between the etal.,2021)tuning. Specifically,therankforLoRA\nchosensummaryS andtherejectedsummaryS . tuning is16 and thescaling factor isalso 16. All\nc r\nAlargervalueofmindicatesthatthemodelisal- modelsarequantizedin8-bittoadditionallyreduc-\nreadyproficientatdifferentiatingS fromS . Con- ingtrainingcost.",
    "char_length": 1499
  },
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 22,
    "text": "c r\nsequently,DPOassignslowerweights,σ( m),to When performing perturbation on each docu-\n−\nchosenandrejectedsummarieswherethemodelis mentsettogeneratepreferencepairs,weobserve\nconfident in their differences and higher weights that certain social attribute values are extremely\ntochosenandrejectedsummarieswherethediffer- rare in some document sets. If FairPO removes\nences is more challenging. The term σ( m) can α percent of documents with these rare social at-\n−\nhelpthemodelfocusesmoreondifficultcases. tributevalues,thosesocialattributevalueswilldis-\nTheobjectiveofFairPOisdesignedsothatcho- appearentirelyfromthedocumentset. Therefore,\nsenandrejectedsummarieshaveseparateweight when performing perturbation, we only consider\nwhilepreservingtheeffectofthetermσ( m)in socialattributevaluesthatappearinmorethanα\n−\nEq.10. The derivative of FariPO objective with percent of the documents. In the most extreme\nrespecttothemodelparametersθ isasfollows: case, if only one social attribute value meets this\nrequirement,FairPOwillsampledifferentsubsets\n∂π (S D)\nσ( m)β(w r π θ (S r D) − 1 θ r | ofαpercentofdocumentswiththatsocialattribute\n− | ∂θ\n(12)\nvalue. Bydoingthis,weassurethecompleteness\n∂π (S D)\nw c π θ (S c D) − 1 θ c | ) ofsocialattributevaluesafterperturbation.\n− | ∂θ\n1150\nFigure1: InterfaceforHumanEvaluation\n1151\nBelow is a list of product reviews:\n1.This is a card reader that does everything I needed it to . My adapters for the micro SD cards were defective",
    "char_length": 1471
  },
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 23,
    "text": "so I have no complaints only praise . It reads any Compact Flash , Memory Stick , SD , and XD cards . Well that\nis all I wanted to say except this is a great product overall , and thank you .\n2.The pins in the CF slot are very flimsy and get bent out of alignment easily , making it impossible to insert\nthe card ( until you perform delicate surgery on the pins with small tweezers ) . Do not buy this product if you\nwill ever use the CompactFlash slot . It will just lead to frustration .\n3.So far I only use this for SM and SD cards , but it installed ( USB ) quickly , easily and reads the cards I need\nread .\n4.Initially it worked great but after the 5th time it stopped working . It also helped fry my SD-card will all my\npictures and video clips . Not happy at all with this product .\n5.Reads 64 cards is quite deceiving . It only reads four types of cards made by 64 different manufacturers .\nAlso , the connector port is difficult to plug in .\n6.good product , reads quite fast. only issue is that the card reader does not have a satisfying ' click ' when\nthe card is inserted. you kinda have to stick the card in the slot and hope it is lodged properly .\n7.I can get it to read SD cards , but I bought it to read my CF 's and it won 't read a single one . My experience\nis in line with others . Go check out similar reviews on newegg.com.\n8.The card reader comes in retail packaging and totally lacks instructions on how best to put 68 types of cards",
    "char_length": 1460
  },
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 24,
    "text": "into 4 slots . It did read an SD card successfully . The micro usb plug on the usb cord broke after 1 use .\nPlease write a single summary around 50 words for all the above reviews.\nFigure2: SummarizationpromptfortheAmazonDataset.\nWeprompttheseLLMstogeneratesummaries Amazon MITweet SemEval Overall\nEC CP EC CP EC CP EC CP\nfor the input document sets of different datasets. ↓ ↓ ↓ ↓ ↓ ↓ ↓\nLlama3.1 7.90 1.92 4.43 0.26 2.94 1.33 5.09 1.17\nThepromptaretunedsothattheaveragelengthof +DPO 6.87 1.04 4.03 0.31 2.55 0.91 4.49 0.75\ngeneratedsummariesare50words. Weshowthe +OPTune 6.58 0.75 4.22 0.23 2.50 0.81 4.43 0.60\n+Prompt 7.71 1.84 4.33 0.38 2.53 0.26 4.86 0.83\nsummarizationpromptsfortheAmazondatasetin\n+PolicyG. 7.71 2.10 4.46 0.31 2.95 1.32 5.04 1.24\nFig. 2. The temperature for generation is 0.6 for +FairPO 6.57 0.37 4.20 0.26 2.39 0.56 4.39 0.39\nMistral 8.18 2.98 3.98 0.42 2.67 1.07 4.94 1.49\nallLLMs.\n+DPO 7.17 1.55 3.60 0.28 2.21 0.64 4.33 0.82\nThe set T+ in Eq.7 is updated so that recent +OPTune 7.48 1.56 3.60 0.25 2.00 0.67 4.36 0.83\nk\n+Prompt 7.67 1.93 4.02 0.23 2.38 0.38 4.69 0.85\ntrainingstepshavehigherimpacts. Specifically,at\n+FairPO 6.98 0.89 3.56 0.21 1.97 0.36 4.17 0.49\ntheendofeachtrainingstep,theimpactsofallthe Gemma2 8.44 2.75 4.17 0.34 2.74 0.91 5.12 1.33\nsamplesalreadyinthesetT+ arereducedwitha +DPO 6.87 1.04 4.04 0.29 2.42 0.70 4.44 0.68\nk\n+OPTune 6.90 1.15 3.86 0.45 2.40 0.65 4.39 0.75\ndiscountfactorγ. Then,allthesamplesthatover-",
    "char_length": 1462
  },
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 25,
    "text": "+Prompt 7.21 1.13 4.28 0.24 2.62 0.30 4.70 0.56\nrepresentssocialattributevaluek (C k (D,S )>0) +FairPO 6.09 0.33 3.84 0.47 2.53 0.59 4.15 0.46\nin current training steps are added to the se ∗ t T+.\nk Table 5: Summary-level fairness (EC) and corpus-\nThediscountfactorγ is0.75forLlama3.1and0.5\nlevelfairness(CP)ofsummariesgeneratedbydifferent\nforotherLLMs.\nmethodsonthefirstsplittingoftraining,validationand\nThe goal of the exponent, C k (D,S ), of testing.. Thebestperformingmethodisinbold. The\n∗\nO(k)/(U(k)orU(k)/(O(k)inEq. 8andEq. 9is second-bestperformingmethodisunderlined. FairPO\ntoadjusttheweightsw andw suchthatitmore hasthebestoverallperformanceonthefirstsplitting.\nc r\ndeviatesfrom1asC (D,S )moredeviatesfrom\nk\n∗\n0. Therefore,FairPOdoesnotdirectlyusetheraw\nconsider the policy gradients. Besides, for a fair\nvalue of the sum of coverage probability differ-\ncomparisonwithothermethods,weimplementthe\nencesC (D,S )astheexponent. Instead,FairPO\nk policy gradient method in an offline setting. The\n∗\nseparatelynormalizesC (D,S )amongalltrain-\nk learningrateforthepolicygradeintis1e 6fol-\n∗\ningsampleswhereC (D,S )isgreaterthanzero −\nk lowingtheoriginalpaper. Weonlyimplementthe\n∗\norlessthanzero.\npolicygradientmethodforLlama3.1sincethetrain-\ningisveryunstableevenifwelowerthelearning\nA.5 ImplementationofBaseline\nrate to 1e 9 for Mistral and Gemma2. For OP-\n−\nWeimplementthepolicygradientmethodproposed TuneandDPO,theyusethesamehyperparameters",
    "char_length": 1447
  },
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 26,
    "text": "by Lei et al. (2024) as a baseline. In the original asFairPO.\nimplementation,thereisalossthatmaximizethe\nA.6 ResultsusingDifferentDatasetSplitting\nprobability for reference summary in addition to\nthe policy gradients. Since datasets used in this TovalidatethestabilityofFairPOonthreedifferent\npaperdonotcontainreferencesummary,weonly splittingsofdatasets,wegeneratethetraining,vali-\n1152\nAmazon MITweet SemEval Overall in Fig. 3. From the figure, we observe that sum-\nEC CP EC CP EC CP EC CP\n↓ ↓ ↓ ↓ ↓ ↓ ↓ maries generated by LLMs tuned FairPO tend to\nLlama3.1 7.90 2.05 4.50 0.63 2.90 1.41 5.10 1.36\n+DPO 7.27 1.37 4.30 0.37 2.70 1.12 4.76 0.95 morebalancelypresentnegativeandpositiveinfor-\n+OPTune 6.92 0.40 4.30 0.52 2.82 1.00 4.68 0.64 mation.\n+Prompt 7.28 1.67 4.41 0.44 2.74 0.51 4.81 0.87\n+PolicyG. 7.75 1.85 4.47 0.48 2.80 1.30 5.02 1.21\n+FairPO 6.96 0.44 4.26 0.29 2.69 0.59 4.64 0.44\nMistral 8.60 2.74 4.18 0.73 2.91 1.28 5.23 1.58\n+DPO 7.24 1.79 3.39 0.26 2.70 1.15 4.44 1.07\n+OPTune 6.59 0.52 3.57 0.53 2.04 0.58 4.07 0.54\n+Prompt 7.90 1.76 3.74 0.51 2.43 0.52 4.69 0.93\n+FairPO 6.06 0.11 3.83 0.39 2.13 0.33 4.01 0.28\nGemma2 8.31 2.33 4.30 0.80 2.97 1.03 5.19 1.38\n+DPO 7.04 0.98 4.07 0.44 2.43 0.48 4.51 0.63\n+OPTune 6.91 0.56 3.94 0.86 2.35 0.56 4.40 0.66\n+Prompt 7.33 1.26 4.49 0.44 2.91 0.85 4.91 0.85\n+FairPO 6.09 0.44 3.82 0.65 2.70 0.32 4.20 0.47\nTable 6: Summary-level fairness (EC) and corpus-\nlevelfairness(CP)ofsummariesgeneratedbydifferent",
    "char_length": 1465
  },
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 27,
    "text": "methodsonthesecondsplittingoftraining,validation\nandtesting.Thebestperformingmethodisinbold.The\nsecond-bestperformingmethodisunderlined. FairPO\nhasthebestoverallperformanceonthesecondsplitting.\nAmazon MITweet SemEval Overall\nEC CP EC CP EC CP EC CP\n↓ ↓ ↓ ↓ ↓ ↓ ↓\nLlama3.1 8.06 1.70 4.57 0.87 3.11 1.51 5.25 1.36\n+DPO 7.55 1.39 4.43 0.74 2.73 1.24 4.90 1.12\n+OPTune 6.61 0.72 4.47 0.79 2.48 1.03 4.52 0.85\n+Prompt 7.26 1.42 4.35 0.53 2.59 0.11 4.74 0.69\n+PolicyG. 7.72 1.69 4.60 0.85 3.16 1.53 5.16 1.36\n+FairPO 7.07 0.44 4.25 0.71 2.38 0.82 4.57 0.66\nMistral 8.29 2.76 4.32 0.67 2.90 1.46 5.17 1.63\n+DPO 7.20 2.11 3.65 0.48 2.32 0.99 4.39 1.19\n+OPTune 6.47 0.55 3.58 0.76 2.18 0.46 4.08 0.59\n+Prompt 7.66 2.07 4.14 0.36 2.23 0.19 4.68 0.87\n+FairPO 5.92 0.38 3.71 0.61 2.21 0.59 3.95 0.53\nGemma2 8.21 2.36 4.14 0.67 2.72 0.96 5.02 1.33\n+DPO 6.80 0.70 4.01 0.49 2.46 0.49 4.42 0.56\n+OPTune 6.72 0.94 3.86 0.41 2.21 0.25 4.27 0.53\n+Prompt 7.29 1.09 4.21 0.28 2.66 0.28 4.72 0.55\n+FairPO 6.35 0.56 3.64 0.32 2.29 0.43 4.09 0.44\nTable 7: Summary-level fairness (EC) and corpus-\nlevelfairness(CP)ofsummariesgeneratedbydifferent\nmethodsonthethirdsplittingoftraining,validationand\ntesting. The best performing method is in bold. The\nsecond-bestperformingmethodisunderlined. FairPO\nhasthebestoverallperformanceonthethirdsplitting.\ndationandtestingsetusingdifferentrandomseeds\nandruntheautomaticevaluationonthesesplittings.\nTheresultsforeachsplittingareshowninTab. 5,",
    "char_length": 1458
  },
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 28,
    "text": "6,7respectively. Fromthetable,weobservethat\nFairPO still shows the best overall performance,\nwhich shows the stability of FairPO on different\nsplittingsofdatasets.\nA.7 QualiativeExample\nWe show sample summaries generated by LLMs\ntunedwithDPOandFairPOontheAmazondataset\n1153\nLlama3.1 Mistral Gemma2\nDPO: The product reviews are mixed, DPO: This tablet receives generally DPO: This plush bed topper is\nwith some customers expressing positive reviews, praised for its generally well-received for its comfort\nsatisfaction with the quality and price, portability, picture quality, speed, and and ability to stay in place thanks to\nwhile others find it flimsy, useless, and features like USB and HDMI ports. its straps. Many users praise its cloud-\npainful to use. Issues include bending, Some users found the battery life like softness and how it elevates their\ncrookedness, and difficulty with hair lacking, while one experienced a sleep quality. However, some users\nremoval, leading to disappointment defective product. Overall, it's found it loses its fluffiness over time\nand a recommendation to avoid the considered a good value for the price and requires manual plumping.\nproduct. and user-friendly, with a few noting\nits similarity to Android phones.\nFairPO: The product reviews are FairPO: This Toshiba tablet receives FairPO: This plush bed topper\nmixed, with some users praising its mixed reviews. Users praise its receives rave reviews for its comfort,",
    "char_length": 1459
  },
  {
    "paper_id": "fair_multidoc_summary",
    "chunk_id": 29,
    "text": "availability and quality, while others lightweight design, fast speed, great fluffiness, and stay-in-place straps.\ncriticize its flimsiness, uselessness, and picture quality, and affordability. Some users find it a dream come true,\nhigh price. Some users find the tips Some highlight its functionality similar while others experience flattening\ntoo long, weak, or hard to use, while to laptops and its ease of use. over time. It's praised for its luxurious\nothers appreciate its effectiveness, but However, concerns arise regarding feel, comparable to a feather bed\nnote that it may not be suitable for all battery life and a negative experience without the prickliness. While some\npurposes. with a damaged, non-functional find it ideal for air mattresses and\nproduct. adding height, others note back pain\nissues.\nFigure3: SamplesummariesgeneratedbyDPOandFairPO.\n1154",
    "char_length": 868
  }
]