[
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 0,
    "text": "CreativityPrism: A Holistic Benchmark for\nLarge Language Model Creativity\nZhaoyiJoeyHou BoweiAlvinZhang YiningLu\nUniversityofPittsburgh JohnsHopkinsUniversity UniversityofNotreDame\nBhimanKumarBaghel AnnelieseBrei XimingLu\nUniversityofPittsburgh UniversityofNorthCarolina UniversityofWashington\natChapelHill\nMengJiang FaezeBrahman SnigdhaChaturvedi\nUniversityofNotreDame AllenInstitutefor UniversityofNorthCarolina\nArtificialIntelligence atChapelHill\nHaw-ShiuanChang DanielKhashabi XiangLorraineLi\nUniversityofMassachusetts JohnsHopkinsUniversity UniversityofPittsburgh\nAmherst\nAbstract\nCreativityisoftenseenasahallmarkofhumanintelligence. Whilelargelanguage\nmodels(LLMs)areincreasinglyperceivedasproducingcreativetext,thereisstill\nnoholisticframeworktoevaluatetheircreativityacrossdiversescenarios. Existing\nevaluationmethodsremainfragmented,withdramaticvariationacrossdomainsand\ntasks,largelyduetodifferingdefinitionsandmeasurementsofcreativity. Inspired\nby the hypothesis that creativity is not one fixed idea, we propose, CREATIVI-\nTYPRISM,anevaluationanalysisframeworkthatdecomposescreativityintothree\ndimensions: quality,novelty,anddiversity. CREATIVITYPRISMincorporatesnine\ntasks,threedomains,i.e.,divergentthinking,creativewriting,andlogicalreason-\ning,andtwentyevaluationmetrics,whichmeasureeachdimensionintask-specific,\nuniqueways. Weevaluate17state-of-the-art(SoTA)proprietaryandopen-sourced\nLLMsonCREATIVITYPRISMandanalyzetheperformancecorrelationsamong",
    "char_length": 1465
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 1,
    "text": "different metrics and task domains. Our results reveal a notable gap between\nproprietary and open-source models. Overall, model performance tends to be\nhighlycorrelatedacrosstaskswithinthesamedomainandlesssoacrossdifferent\ndomains. Amongevaluationdimensions,diversityandqualitymetricsshowstrong\ncorrelations‚Äîmodelsthatperformwellononeoftenexcelontheother‚Äîwhereas\nnoveltyexhibitsmuchweakercorrelationwitheither. Thesefindingssupportour\nhypothesisthatstrongperformanceinonecreativitytaskordimensiondoesnot\nnecessarilygeneralizetoothers,underscoringtheneedforaholisticevaluationof\nLLMcreativity. 1.\n1 Introduction\nCreativity, the capacity to generate novel and valuable ideas or solutions [5, 15, 25], is a core\nhumancognitiveability. Itappearsacrossmanydomains: craftingstorieswithsurprisingplottwists\n1Projectwebsite:https://joeyhou.github.io/CreativityPrism/\nPreprint.Underreview.\n5202\ntcO\n32\n]LC.sc[\n1v19002.0152:viXra\n[3,30],producinggroundbreakingscientificdiscoveries[26,55],solvingproblemsunderconstraints\n[42,62],orevenexpressinghumorineverydaylife [23,67]. Itsmultifacetednaturehasprompted\nextensivestudyinpsychologyandcognitivescience,witheffortstocapturecreativitythroughboth\nqualitativeandquantitativeapproaches [1,21,46,57].\nRecently,withtherapidriseofgeneral-purposeLLMs,interesthasgrowninprobingtheircreativity\n[3,8,17,42,65]. Butaswithhumancreativity,creativityspanssuchdiverseandexpansivecontexts,",
    "char_length": 1413
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 2,
    "text": "makingitdifficulttodefine,formalize,and,aboveall,measure. Moreconcretely,theevaluationof\nLLMcreativityfacestwomainchallenges: thedifficultyofscalable,automaticevaluationduetothe\nconvolutednatureofcreativityandthedistinctdefinitionofcreativityacrossdifferentdomains. The\nformercallsforeffectiveautomaticevaluationmethods,asmanyexistingworks[8,61]heavilyrely\nonhumanevaluation,whichisexpensive,inaccessible,andalsotime-consuming. AsnewLLMs\nare coming out nearly every day, a scalable and accessible way of evaluating LLM creativity is\nnecessary. Thelatterrequiresacomprehensiveevaluationframeworkthatincorporatesevaluation\nofcreativityfromvariousdimensions,ascurrentworksarescatteredacrossdifferentdomainsand\nthus often target narrow or singular dimensions, failing to capture shortcomings in other equally\nimportant dimensions of creativity. For example, the Divergent Association Task (DAT) [4, 10]\nand the Creative Short Story Task [29] emphasize lexical diversity, yet LLMs can exploit them\nbygeneratingrandom,incoherentwords. TheCreativityIndex [40]comparesmodeloutputswith\npre-trainingcorpusatthen-gramlevel,butrisksoverestimatingormisjudgingcreativityassigned\ntoparaphrasedtextandmodelstrainedwithprivatedata. TheAlternativeUseTest(AUT) [17,48]\nsolelyfocusesonunconventionalideasofusingdailyitems,overlookingthepragmaticsofthose\nsolutions.Thesearejustexamplesfromthewiderangeofevaluationprotocolsforcreativity,asshown",
    "char_length": 1423
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 3,
    "text": "atthebottomofFigure.1. Suchtask-specific,adhocchoicesoftenyieldinconsistentconclusions\naboutcreativity,obscuringtheiractualcreativecapacity. Together,thesechallengesunderscorea\ncentralpoint: creativityismultidimensional,necessitatingaholisticevaluationframeworkthatcan\nscale‚Äîonethatintegratesmultipletasksandmetricstocapturequality,novelty,anddiversityina\nunifiedway.\nTo this end, we propose a holistic, scalable evaluation framework for LLM creativity evaluation,\nCREATIVITYPRISM,consistingof9tasksand20metrics. Thesetasksencompassdomainssuchas\nlogicalreasoning(includingmathematicalreasoningandcoding),creativewriting,anddivergent\nthinking. Due to the complexity of creativity, no single metric could represent the concept of\ncreativity,soourframeworkevaluatescreativityfromthreedistinctdimensions‚Äîquality,novelty,\nanddiversity‚Äîwhicharewidelyrecognizedinpriorinterdisciplinaryliteratureascoredimensions\nofcreativity[1,25,28,56]. Wesystematicallycategorizeexistingtask-specificmetricsalongthe\nthreedimensionstofacilitateacomprehensivemeasurementofmodelcreativity(Figure1). Quality\nevaluateswhetherLLMgenerationssatisfyfundamentaltaskrequirements,e.g,sentencecoherence\nandgrammaticalcorrectness. Noveltymeasurestheoriginalityofsolutionsorcontentbycomparing\ntheir difference from existing ones. Diversity examines the variation among generated content,\ncapturingthemodel‚Äôscapacitytoproducedistinctoutputs. Ourevaluationframeworktaxonomy",
    "char_length": 1436
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 4,
    "text": "utilizesthesethreecreativitydimensionstoprovideaholisticandstructuredviewforanalysis.\nWe evaluate 17 closed-sourced and open-sourced state-of-the-art (SoTA) LLMs on CREATIVI-\nTYPRISM and found a notable performance gap between proprietary and open-sourced models,\nespeciallyinlogicalreasoningtasks,followedbycreativewritingtasks. Inordertobetterunderstand\ntheconnectionsamongcreativitydimensionsandmetrics,wealsoconductadetailedanalysisof\nthecorrelationsamongmodels‚Äôperformanceinallcreativitymetrics. Resultshaveshownthatthe\nmodels perform similarly in metrics from the same task or the same domain. For metrics from\ndifferentdomains,modelsperformsimilarlyindiversityandqualitydimensions,whileperformances\ninnoveltydimensionsaremuchlesscorrelated. Webelievethisisduetoaninherentdifferencein\nhownoveltyisdefinedindifferenttasksanddomains. WebelievethatCREATIVITYPRISMlaysa\nsolidfoundationformeasuringmachinecreativityandguidingthefuturedevelopmentofcreative\nmodels.\n2 RelatedWork\nHumanCreativity Thedefinitionofcreativityhasvariedacrossdifferentdomains. Inpsychology,\nTorranceTestofCreativeThinking(TTCT)[1]considerscreativityasacombinationoforiginality,\nflexibility,fluency,andelaboration. Inmarketing,El-MuradandWest[13],Rosengrenetal.[53]\n2\nüí°\nüß† Divergent Thinking ‚úç Creative Writing üíª Logical Reasoning\nPrompt: Please write 10 nouns in English Prompt: Please write a few paragraphs for Prompt: Find the length of longest ‚Ä¶\nthat are as irrelevant‚Ä¶ a novel starting ‚Ä¶ Don‚Äôt use recursion",
    "char_length": 1488
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 5,
    "text": "Response: Certainly! Here are 10 single- Response: It‚Äôs been years: Bailey clearly Response:\nword, ‚Ä¶ 1. Volcano 2. Violin 3. means him no harm and has managed to be def solve():\nDemocracy 4. Carpet discreet enough that Nick‚Äôs queerness isn‚Äôt test_cases = int(input())\n5. Bacteria 6. Satellite 7. Patience 8. the talk of the Chronicle‚Ä¶‚Ä¶ for _ in range(test_cases):\nApron 9. Oxygen 10. Comet ‚Ä¶\nüíé üíé üíé\nCreativity Creative\nAUT DAT TTCT Short Story TTCW CS4 NeoCoder\nIndex Math\nFluency Coherence\nQuality\nElaboration Narrative Ending QUC Convergence@k Correctness\nElaboration\nNovelty AUT Originality Surprise RCS L-Uniqueness Divergence@k Novelty\nScore\nDiversity DAT Flexibility N-gram Emotional Dist-N\nScore Diversity Flexibility\nFigure1: OverviewofCREATIVITYPRISM. Weevaluated17LLMsacrossninedatasetsspanning\nthreedomains: divergentthinking,creativewriting,andlogicalreasoning. EachLLMwasprompted\ntocompletethetasks,andtheiroutputswereevaluatedusingtask-specificmetrics. However,these\nmetricsarediverseanddifficulttointerpretholisticallyintermsofmachinecreativity. Toaddress\nthis,weorganizethemetricsintothreekeydimensionsofcreativity: quality,novelty,anddiversity.\nCreativitycannotbecapturedbyasinglemeasure‚Äîitmustbeevaluatedthroughmultipledimensions.\nTaskdetailscanbefoundinTable1.\nconsidersadvertisementcreativityasthecombinationofusefulnessandoriginality;ontopofthat,\nSmith et al. [56] adds flexibility, fluency, elaboration, synthesis, and artistic values. In terms of",
    "char_length": 1469
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 6,
    "text": "creativityevaluation,Said-Metwalyetal.[54]summarizesmorethan100existingworksintofour\nperspectives to evaluate creativity: process, person, product, and press. Given this taxonomy of\ncreativityevaluationsubjects,ourstudyonevaluatingLLMfocuseson‚Äúproduct‚Äù,i.e.,LLM-generated\ntext,alongthreekeydimensions: novelty,diversity,andquality.\nMachineCreativity MeasurementofMachinecreativityhasbecomeincreasinglypopularwiththe\nrapiddevelopmentofLLMs. Manyrecentsurveysprovideacomprehensiveviewoftheprogress\ninmachinecreativity. Forexample,Ismayilzadaetal.[28]summarizesup-to-dateresearchinthe\nAIcommunityaboutcreativity(beforeDec. 2024),focusingonthevarietyoftasksthataredefined\naround creativity; Franceschelli and Musolesi [16] summarizes recent deep-learning methods to\ngenerateandevaluatecreativity,emphasizingthecomputationalmodelsinvolved. However,noneof\nthemfocusedonsystematicallyevaluatingmachinecreativity. Morerecently,Luetal.[39]provides\nacomprehensiveanalysisofvariousevaluationmethodsforcreativityacrossmultipledomains,but\ntheevaluationmetricsarelimitedtoonlyfour(CreativityIndex,Perplexity,SyntacticTemplates,\nandLLM-JudgeScores). Jainetal.[31]focusesonoutputhomogenizationandcoversdiversityand\nquality,insteadofallthreedimensionsinourcreativitytaxonomy. Heetal.[24]alsoproposesan\nevaluationframeworkformulti-modalcreativityevaluationamongfoundationalmodels,butthey\nfocusonproposingcreativitymetricstobetterevaluatetheoutputofexisting,non-creativity-specific",
    "char_length": 1463
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 7,
    "text": "tasks, while we focus on setting up a unified task suite, including both task design and metrics,\nthatrequirescreativitytosolve. Fangetal.[14]alsointroducesamulti-modalcreativityevaluation\nbenchmark, but they do not distinguish among different creativity dimensions and only conduct\nevaluationinoverallcreativity.\nThe community has explored a wide range of domain-specific problems where LLMs show dif-\nferent degrees of creativity. Examples include logical-based problem-solving [42, 62], physical\nandcommonsensereasoning[60],creativewriting[3,8,22,29,40,59],scientificdiscovery[55],\n3\nresponse diversity in question answering [43, 64], and human-ai collaborative creative problem\nsolving[6,9,45]. However,alloftheseworksstudyLLM-generatedcontentinonespecificdomain\nandwiththeirownevaluationphilosophiesandmetrics. Ourworkaimsatprovidingaholisticand\ncomprehensiveevaluationoftheLLM‚Äôsoutputfortasksinavarietyofdomains. Todothis,wefind\nthecommongroundofcreativitydefinitionsacrosstaskswhilemaintainingthetask-specificmetrics\nbycategorizingthemintodifferentdimensions. Oneofthecommongroundsacrossthosestudiesis\nthattheyallconsidercreativityaseitherdivergenceoracombinationofconvergenceanddivergence.\nInourevaluationframework,wealsoconsiderthisbalanceasoneofthekeythemesaswepropose\nourcreativitytaxonomy,beingdivergence(noveltyanddiversity)andconvergence(quality)in¬ß3.\nAutomaticTextEvaluation Evaluatingthecreativityofmachine-generatedtexthasbeenachal-",
    "char_length": 1450
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 8,
    "text": "lengingtask,andmuchoftheworkreliesonhumanevaluation. Butduetothecost,humanevaluation\nishardtoscaleandrequiresalongwaittime. Toachieveevaluationscalability,researchersadapt\nvarious automatic text evaluation techniques [7]. There are two broad groups of such evaluation\nmethods: feature-basedandgenerative-based. Theformerincludespsycholinguisticfeatures,such\nasarousal,valencescore[44],lexicalfeatures,suchaslexicaldiversity[49],andtextembedding\ndistances[51,63]. ThelatterismainlyLLM-as-a-judge[37,38,58]. Recentworkhasshownthe\npromising potential of this method in human-LLM evaluation alignment [66]. In our work, we\nkeeptheoriginalevaluationprocedureoftheoriginaltask,withextraverificationofhuman-LLM\nalignmentfortaskswithLLM-as-a-judge.\n3 CREATIVITYPRISM: AHolisticBenchmarkforMachineCreativity\nCREATIVITYPRISMstartsfromasimpleinsight:justasaprismrefractsasinglebeamintoaspectrum\nofcolors,creativitysplitsintodistincthueswhenitpassesthroughadifferentcontextordomain.\nAsshownin1,CREATIVITYPRISMevaluatesanLLMbypromptingitwithtasksin divergent\nthinking, creativewriting,and logicalreasoning. Thedivergentthinkingdomainconsistsof\nestablishedpsychologytasks,whichwereoriginallydesignedtoassesshumanabilityingenerating\ndiverse and alternative answers to given questions [4, 10, 17, 65]. The creative writing domain\nincludestasksthatrequiremodelstoproduceshortwrittenpieces‚Äîeitherthroughdirectinstructions\nto be creative or by imposing constraints that require unconventional thinking while adhering to",
    "char_length": 1501
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 9,
    "text": "specificrules[3,8,29,40]. Thelogicalreasoningdomainincludesonecodingandonemathtask\ntoevaluatemodels‚Äôabilitytogeneratecreativesolutionsunderstrict,explicitreasoningconstraints.\nOurtaskselection,whichresultsinninedatasets,isprimarilybasedontheavailabilityofautomatic\nevaluationmetricsthatarebothscalableandalignedwithhumanjudgments. Moretaskdetailsand\ninputexamplescanbefoundinTable1.\nLLMsarethentaskedtogenerateoutputsgiventhetask-specificinputandquestions. Thegenerated\nresultsareevaluatedwithvariousevaluationmetricsspanningquality,novelty,anddiversity. We\nbelievethatacreativeLLMshouldbeabletogenerate‚Äúnovel‚Äùand‚Äúdiverse‚Äùresponseswithhigh\n‚Äúquality.‚Äù Followingthis,thetask-specificevaluationmetricsfromtheninetasksabovearegrouped\nintothreedimensions:quality,novelty,anddiversity.EachtaskthatweincludeinCREATIVITYPRISM\ntouches at least one of those dimensions of creativity (Figure 1, more details in Appendix B).\nNote that not all tasks include all three evaluation dimensions, underscoring the importance of\nCREATIVITYPRISM-aholisticevaluationframeworkthatcapturesabroaderspectrumofcreativity\nthananysingletaskcanmeasure.\nQualityincludesmetricsthatevaluatehowwellthegeneratedcontentfulfillsthetask‚Äôsfunctionality.\nForexample,inNeoCoder,thequalityofgeneratedcodeismeasuredbythesuccessofexecutionand\ncodingtaskcompletion;inCS4,thequalityofgeneratedstoryismeasuredbystorycoherenceand\nconstraintsatisfaction.\nNoveltyincludesmetricsthatevaluatehowrarethegeneratedcontentiscomparedtoexistingor",
    "char_length": 1489
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 10,
    "text": "commonlyseencontent. Forexample,inbothNeoCoderandCreativeMath,noveltyinvolvescoming\nupwithsolutionsthataredifferentfromthereferencesolutions;inAUT,noveltyinvolvesdifferent\nuseofthetoolcomparedtoordinaryuses;inCreativityIndex,noveltyismeasuredbythenormalized\nn-gramoverlapsbetweenmodel-generatedtextandthetraceablepartofthetrainingcorpus.\nDiversityincludesmetricsthatevaluatehowmuchtheLLM-generatedcontentdiffers. Forexample,\nforcreativewritingtaskssuchasCS4andCreativeShortStory, diversityscoresmeasurelexical\n4\nTaskDescription Example\nAlternativeUsesTest(AUT)[17]:Givenacom- Createalistofcreativealternativeusesforabottle.\nmonlyseenobject(e.g,amug),LLMsgenerateun-\nconventionalusesofthatobject(e.g.,useamugasa\nplantpot).\nDivergent Association Task (DAT) [4, 10]: Pleasewrite10nounsinEnglishthatareasirrelevant\nLLMsgenerate10verydifferentnouns. fromeachotheraspossible,inallmeaningsanduses\nofthewords.\nTorranceTestsofCreativeThinking(TTCT) Whatmightbetheconsequencesifhumanssuddenly\n[65]: LLMs answer psychological questions in losttheabilitytosleep?\nwidely-usedhuman-facingcreativitytests.\nTorranceTestofCreativeWriting(TTCW)[8]: WriteaNewYorker-stylestorygiventheplotbelow.\nGivenasummaryofanarticlefromtheNewYorker, Makesureitisatleast2000words.Plot:Awoman\nLLMsgenerateanarticlewithasimilarstoryline. experiencesadisorientingnightinamaternityward\nwheresheencounters...;Story:\nCreative Short Story [29]: Given three key- Youneedtocomeupwithanovelanduniquestory",
    "char_length": 1462
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 11,
    "text": "words, LLMs generate a short story with at most thatusestherequiredwordsinunconventionalways\nfivesentences. orsettings.Makesureyouuseatmostfivesentences.\nThegiventhreewords:petrol,diesel,andpump.\nCreativity Index [40]: Given a prefix from a Pleasewriteafewparagraphsforanovelstarting\nparagraphinanovel, apoem, oraspeech, LLMs withthefollowingprompt:‚ÄúIt‚Äôsbeenyears:Bailey\ngeneratecompletions. clearlymeanshimnoharmandhasmanagedto...‚Äù\nCS4 [3]:GivenabasestorygeneratedbyGPT-4, BaseStory: ‚ÄúEvelyn was introverted by nature...‚Äù\nLLMsgeneratearevisiontothestorytofulfillan Nowmodifytheexistingstorytoaccommodatethe\nincreasingnumberofconstraintsonthestorycontent. followingconstraints:Theprotagonistsuffersphys-\nical discomfort when overwhelmed by emotions...\nComeupwithanewstoryin500words.\nNeoCoder [42]: Given a coding problem and You are given a sequence of integers a of length\nincreasingconstraintsonavailabletechniques,LLMs 2n.Youhavetosplitthese2nintegersintonpairs...\ngeneratesolutioncodethatbothsolvesthecoding Don‚Äôtusehashmap,whileloop.\nproblemandfulfillstheconstraints.\nCreativeMath[62]:Givenamathproblemand Question: Whatisthelargestpowerof2thatisa\nreference solutions, LLMs generate solutions that divisorof134-114? A.8B.16C.32D.64E.128;\ndifferfromtheprovidedones. ReferenceSolutions1:...;ReferenceSolutions2:...\nTable1: TasksinCREATIVITYPRISMwithexamples. : divergentthinking, : creativewriting,\n: logicalreasoning. SomeinputdetailsareomittedandcanbefoundinAppendixE.",
    "char_length": 1474
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 12,
    "text": "diversityofthegeneratedstories;fortheDATtask,diversityinvolvesthesemanticdifferenceamong\ntheLLM-generatednouns.\n4 Experiments\nToholisticallyevaluatemachinecreativity,weevaluated17state-of-the-artLLMsacrossninetasks,\nreportingbothtask-specificmetricsandanaggregatedcreativityscoreusingthethree-dimensional\nframework. Inthissection,wewillfirstintroducetheinferencesetups,whereLLMsareprompted\ntogeneratecreativeresponsesaccordingtocorrespondingtaskrequirements(¬ß4.1); thenwewill\ndescribetheevaluationprocess,includingscoreaggregation(¬ß4.2)andhowweuseLLM-as-a-Judge\nforscalableautomaticevaluation(¬ß4.2).\n4.1 Inference\nForallthetasksinCREATIVITYPRISM,wecollecttheoriginaldatasets. Unlessotherwisespecified,\nallthedataprocessingisdoneaccordingtotheoriginalpapers. MoredetailsareinAppendixE.\n5\nIn terms of models, we include 17 models in total, including open-source models from Mistral\n[32,33],Qwen[27,52],OLMo[20],Llama[19],andtheDeepseek[11,12]family,andproprietary\nmodelsfromGPT[47],Claude[2],andtheGemini[18,35]family. Foropen-sourcedmodels,we\nusevLLM(v0.7.2)[36]torunallexperiments. Forproprietarymodels,weuseAPIaccessfromthe\ncorrespondingcompany. Inferencetimeparametersvarydependingonthetaskandcanbefoundin\nthecorrespondingsectionsinAppendixE.\n4.2 Evaluation\nAggregatedCreativityScoring Wechoosetoaggregatescoresfromallmetricsineachdimension\nsothatwecanhaveaholisticinsightintohowamodelperformsinaspecificdimensionofcreativity.",
    "char_length": 1429
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 13,
    "text": "Althoughthesedimensionscoresarelaterfurtheraggregated(takingasimpleaverage)intoan‚Äúoverall‚Äù\ncreativityscore(showninTable2),this‚Äúoverall‚Äùscoreisonlytofacilitatemodelcomparison. We\nsuggestfutureresearchersinterestedinusingourbenchmarkchoosefromthosethreeseparatescores\naccordingtotheirownpurpose. Thescoreaggregationfollowsthesesteps: first,everyevaluation\nmetricismin-maxnormalizedtobetween0and1,whereminandmaxareminandmaxpossible\nscoresonthistask. Second,basedonthecategorizationinFigure1,quality,novelty,anddiversity\nscoresofeachLLMareaggregatedbyaveragingallthenormalizedmetricsinthecorresponding\ncategory. Also,toavoidtaskswithmultiplemetricsinonedimensionhavingahigherinfluenceon\nthedimensionscore(e.g.,TTCWhasthreemetricsinthequalitydimension),theaveragenormalized\nscoresfromeverysingletaskwillbeusedtocalculatedimensionscores. Moredetailsaboutscore\naggregationcanbefoundinAppendixB.\nLLM-as-a-Judge Reliability In CREATIVITYPRISM, the evaluation of six tasks (out of nine)\ninvolves using LLM as part of the automatic evaluation procedure. To ensure the reliability of\ntheLLM-Judge,weconductthefollowinganalysis,inwhichweuseQwen2.5-72Basthedefault\nLLM-Judge model, unless otherwise specified. For AUT, Organisciak et al. [48] has reported\nhuman-LLM-Judgeagreement;sinceweareusingthesamesetup(i.e.,thesamemodel,prompts,\nandconfiguration),wedirectlyreporttheoriginalpaper‚Äôshuman-LLM-Judgeagreement: Pearson\ncorrelationis0.7. ThesameappliestoNeoCoder[42],andthesolutiontechniquedetectionrecall",
    "char_length": 1494
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 14,
    "text": "is0.94. 2 ForTTCW[8],CS4[3],CreativeMath[62],weeitherhavehumanannotationfromthe\noriginalpapersorcollectedasmallsampleofhumanannotation;wethencalculatetheagreement\nbetweenthejudgmentbyannotatorsandbyLLM-Judge;forCS4,thePearsoncorrelationis0.55\n(p<0.01);forCreativeMath,LLM-Judgeaccuracyis0.78fornoveltyand0.94forcorrectness3;for\nTTCW,LLM-Judgeonlyaccuratelymakesjudgmentinfourmetrics(numbersinparenthesesare\naccuracy): NarrativeEnding(0.69),UnderstandabilityandCoherence(0.78),EmotionalFlexibility\n(0.86),WorldBuildingandSetting(0.72),soweonlyincludethosefourmetrics. 4 FortheTTCT\n[65],sincewehavenohumanannotationsatall,weusethePearsoncorrelationbetweenGPT-4.1\n(theLLM-Judgeusedbytheoriginalpaper)andQwen2.5-72BasaproxyofLLM-Judgequality\nmeasurement(numbersinparenthesesarePearsoncorrelationandp-value):Fluency(0.6884,p<0.01),\nFlexibility(0.6592,p<0.01),Originality(0.5152,p<0.01),Elaboration(0.5033,p<0.01). 5 More\ndetailsaboutLLM-JudgereliabilitystatisticscanbefoundinAppendixD;evaluationmetricsand\nevaluationpromptsforeachtaskareprovidedinAppendixE.\n5 Results&Analysis\n5.1 Overview\nTable2summarizesmodelperformancesacrossdomainsandthreecreativitydimensions(quality,\nnovelty,anddiversity),wheretheoverallscore,averagedacrossthesedimensions,servesasaproxy\n2InNeoCoder,LLM-Judgeisnotusedformakingthefinaljudgment;instead,itisusedtodetectwhich\ntechnique(s)areusedinagivensolutionduringevaluation.Sinceitisadetectiontask,recallisreported.",
    "char_length": 1437
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 15,
    "text": "3Wefollowtheoriginalpaperanduseamulti-LLM-Judgesetup, withGemini-2.0-Flash, GPT-4.1, and\nClaude-3.7-Sonnet\n4Becauseinstorycreativityevaluation,thehuman-to-humanagreementisalsorelativelylow,wehavea\nloweracceptancethresholdwhenweareconsideringwhatkindofhuman-LLMagreement.\n5Wearealsocollectingmorehumanannotationsforalltasksabovetofurthervalidatethealignmentbetween\nhumanandLLM-Judge.Moredetailswillbeavailableintheupdatedversionofthispaper.\n6\nCreative Divergent Logical\nModel Overall Quality Novelty Diversity\nWriting Thinking Reasoning\n<10B\nMistral-7B .522 .376 .558 .649 .446 .758 .320\nQwen2.5-7B .490 .478 .542 .489 .356 .687 .460\nOLMo2-7B .520 .419 .479 .698 .509 .712 .257\nLlama3.1-8B .499 .409 .530 .566 .370 .729 .409\n10-40B\nOLMo2-13B .538 .433 .494 .707 .536 .713 .278\nMistral-24B .534 .487 .578 .591 .484 .642 .473\nQwen2.5-32B .523 .510 .491 .644 .462 .715 .358\n40-80B\nMixtral-8x7B .525 .416 .540 .630 .410 .749 .420\nLlama3.3-70B .541 .533 .574 .562 .411 .722 .529\nQwen2.5-72B .596 .581 .595 .674 .517 .731 .554\nProprietary\nClaude3-Sonnet .697 .672 .663 .835 .637 .833 .612\nClaude3-Haiku .611 .542 .612 .692 .505 .782 .568\nGPT4.1 .721 .697 .692 .871 .686 .793 .682\nGPT4.1-mini .695 .681 .678 .774 .656 .778 .649\nGemini2.0-Flash .677 .645 .654 .822 .592 .806 .655\nDeepSeek-R1 .638 .573 .600 .710 .662 .603 .643\nDeepSeek-V3 .739 .716 .720 .854 .695 .805 .726\nTable2: ModelperformanceonCREATIVITYPRISM,groupedbymodelsize. Proprietarymodelsare",
    "char_length": 1448
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 16,
    "text": "groupedtogether. Allscoresarebetween0and1,andthehigherthebetter. Overallistheaverageof\nQuality,Novelty,andDiversityscores. Therightmostthreecolumnsaretheaveragescoresacross\ntasksineachdomain. Boldarethebestresultsinthecorrespondingmodelsizegroup.\n(a) (b)\nFigure 2: (a) Performance v.s. Day since LLM release date. The line represents best fit linear\nregression. Wecanseethatmodelperformanceinalldimensionshasseenimprovementsovertime.\n(b) Performance gap between the open-sourced models and the proprietary models, averaged by\nmodelsizegroup.\nforamodel‚Äôsoverallcreativecapability. Aswecanseefromthetable,Qwen2.5-72BandDeepSeek-\nV3arethebest-performingmodelsamongopen-sourceandproprietarymodels. Foropen-source\nmodels, we can see that the model performances improve as the model size increases, while for\nproprietarymodels,wedonotknowtheexactmodelsizesandhencecannotmakeacomparison\nbasedonmodelsizes.\nWe have also found a performance improvement along the time axis (Figure 2a) where models\nreleasedinthepasttwoyearshavebecomeincreasinglycompetitive. Sincemanyofourmetrics\n(e.g.,L-uniquenessinCreativityIndex,divergent@0inNeoCoder)wouldrewardmodelsthatcan\ngeneratecontentdifferentfrompriorcontent,havingthechanceoflearningthelatestcontentfrom\n7\n!Divergent ‚úç Creative # Logical\nThinking Writing Reasoning\ntnegreviD!\nevitaerC\n‚úç\nlacigoL\n#\ngniknihT\ngnitirW\ngninosaeR\nFigure3:Models‚Äôperformancecorrelations,groupedbytaskanddomain;C-IndexreferstoCreativity",
    "char_length": 1449
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 17,
    "text": "Index;C-ShortreferstoCreativeShortStory;C-MathreferstoCreativeMath;allcorrelationsare\nPearson‚Äôscorrelation.\nthecorpuswithlatercutoffdateswouldintuitivelymakemodelsmorecompetitive. Moredetailson\nmodelreleasetimedetailscanbefoundinAppendixA.\n5.2 GapBetweenProprietaryModelsandOpenModels\nOverallPerformanceGapAsshowninTable2,thebestproprietarymodel(s)outperformtheirbest\nopen-sourcecounterpartsbymorethan20%ineachdimensionofcreativityandbymorethan10%\nineachdomain. Thisagainshowsabiggapbetweenproprietaryandopen-sourcedLLMswhenit\ncomestocreativity-relatedtasks. Amorein-depthbreakdownofthisgapcanbefoundinFigure2b,\nwiththegapsoftheaverageperformanceofthreeopenmodelgroups(bymodelsizes)comparedto\nthatofallproprietarymodels. Analysisofthisfigureleadstothefollowingtwofindings.\nDomain-SpecificDifferencesAmongthethreedomains,logicalreasoningandcreativewriting\nseeanotablylargergapthandivergentthinking. Wehypothesizethatthisisbecausethosetasksare\nmorecloselyrelatedtoreal-worldapplicationsthandivergentthinkingtasks,andthusthecompanies\nthatdevelopedtheseproprietarymodelsemphasizealotonthosetwoaspectsofLLMtraining. In\nparticular,allproprietarymodelsincludecodingandmathematicalreasoningaspartofevaluation\nintheirtechnicalreport[2,11,12,18,47];mostmodelsincludesomewritingtasks,suchasGRE\nTest[2,47],orincludecreativewritingorrole-playingdataaspartofthepost-trainingdata[11,12],\nwhereasnoneofthesemodelshasputspecialemphasisindivergentthinkingtaskduringtrainingor\nevaluation.",
    "char_length": 1470
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 18,
    "text": "Dimension-Level Differences Across three creativity dimensions, quality and diversity both\nhave a larger performance gap than novelty. We believe the gap in quality comes from a similar\nreasonasmentionedabove,asthequalitydimensionincludesmanyreasoning-relatedmetrics(e.g.,\nconvergent@0fromNeoCoderandCorrectnessfromCreativeMath)thatwouldbenefitfromcoding\nandmathematicaltasksduringtraining. Asfordiversity,wehypothesizethatthehigh-qualityprivate\n8\nQuality Novelty Diversity\nytisreviD\nytilauQ\nytlevoN\nDiversity\nQuality\nNovelty\nFigure4:Left:models‚Äôperformancecorrelations,groupedbycreativitydimensions;Right:individual\nmodelperformance,min-maxnormalizedbydomains. TTCTandTTCWtasksareomittedhereas\ntheyhaveveryhighinter-taskcorrelation. AfullversionoftheheatmapcanbefoundinFigure6.\norcopyrighteddatathatareaccessiblefortheproprietarymodelsenablesthemtolearnfrommore\ndiversecorpora,leadingtoanadvantageinthisdimension.\n5.3 CorrelationsAmongModelPerformance\nDoesagoodperformanceinonetask/domain/dimensionimplysimilarsuperiorityinanothertask/\ndomain/dimension? Toanswerthisresearchquestion,weanalyzethecorrelationbetweenmodels‚Äô\nperformanceamongdifferenttasks,domains,anddimensions. Tobemorespecific,foreachmetric\nm, we form a vector s ‚àà RM by stacking the normalized scores of all M models evaluated in\nm\nCREATIVITYPRISM. WethencomputethePearsoncorrelationr(s\nm\n,s\nm‚Ä≤\n)betweeneverypairof\nmetrics(m,m‚Ä≤). Figure3, 4showstheresultingcorrelationmatrix,orderedbytaskanddimension,",
    "char_length": 1469
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 19,
    "text": "respectively,sothatdiagonalblockscorrespondtowithin-task/within-dimensionmetricgroups.\nStrong Within-Task Correlations We find a strong correlation in the models‚Äô performance on\nmetricscomingfromthesametask. AsshowninFigure3,thecorrelationalongthediagonalsismost\npronounced,withsometasks,suchasTTCWandTTCT,havingcorrelationsgreaterthan0.85for\nallmetricsinthosetasks. InAddition,metricsincreativewritingtasks(inthecentralsquareofthe\nheatmap)generallyhavedecentcorrelationwithothermetricswithinthesamedomain,evenifthey\ncomefromdifferenttasks. Webelievethiscomesfromahigherinherentsimilarityamongtasksfrom\nthecreativewritingdomainthantasksfromtheothertwodomains.\nMixed Within-Dimension Correlations We also observe high correlations among metrics that\nbelongtothediversityorqualitydimension,eveniftheyoriginatefromdifferenttasksordomains.\nThisismoreobviousindiversityandqualitydimensionsandlesssoinnovelty. AsshowninFigure\n4,thecorrelationalongthediagonalishigher(i.e.,lighter)inthetopleft,whilethebottomright\n(noveltydimension)showsmixedcorrelations. Thisobservationisalsoconfirmedbytheindividual\nmodelperformance(radarchartsinFigure4),wherethemodelperformancesfordiversityandquality\naremoreorganized,whiletheonefornoveltyismorecrowded. Alloftheseshowthatthemodels‚Äô\nperformanceinanyoneofthediversitymetricsisagoodindicatorfortheirperformanceinother\ndiversity metrics; the same goes for quality metrics. On the other hand, metrics in the novelty",
    "char_length": 1443
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 20,
    "text": "dimensionhavelowcorrelationswithothermetricsinthesamedimension,asshowninthebottom\nrightpartofFigure4. Webelievethesefindingshighlightthediversedefinitionofnoveltyacross\n9\ntasksanddomains. Forexample,Surprises(CreativeShortStory)measuresthesemantictransitions\nacrossneighboringsentencesinstories,whereasDivergence@0(NeoCoder)measuresthecapability\nofcomingupwithasolutiontoacodingproblemthatisdifferentfromexistingones. Givensuch\nahugedifferenceinmetricdefinition,itisnotsurprisingthattheyevenhaveanegativecorrelation\n(-0.25)inmodelperformances.\nWeakCross-TaskorCross-DomainCorrelationsMetricsfromdifferentdomains(e.g.,divergent\nthinkingv.s. creativewritinginFigure3)andmetricsfromdifferentdimensions(e.g.,noveltyv.s.\ndiversityinFigure4)allhaverelativelylowercorrelations,comparedtowithin-domainorwithin-\ndimensioncorrelations. Inotherwords,modelsperformwellinonedomainorinonedimensionof\ncreativitydonotnecessarilyperformsimilarlywellinanotherdomainordimension.Thisconfirmsthe\nnecessityofincludingadiversesetoftasksandcreativitydimensionstoachieveaholisticevaluation\nofcreativity.\n6 Conclusion\nWeproposedCREATIVITYPRISM,acomprehensiveevaluationframeworkdesignedtocapturethe\ndiversenatureofmachinecreativitybytasksinthreedistinctdomainsandtwentymetricscovering\nquality,novelty,anddiversity. Weevaluate17LLMsfrommultiplefamiliesofproprietaryandopen-\nsourcedLLMsandexplorewaysofamplifyingcreativity. WithCREATIVITYPRISM,LLMdevelopers",
    "char_length": 1429
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 21,
    "text": "willbeabletosystematicallyevaluateLLMcreativityandidentifythedirectionofoptimizationfor\nmorecreativeLLMs.\nLimitationOneclearlimitationisthatourbenchmarkislimitedtoEnglish. Creativitycanbehighly\ncultural,suchasreferencestoculturalhistoryorconvention. Therefore,cautioniswarrantedwhen\ngeneralizingourresultstocreativityinotherlanguages.\nAnotherpotentiallimitationistheinherentbiasbroughtbyLLMsduringevaluation. InCREATIVI-\nTYPRISM,sixoutofninetasksrequireLLM-as-a-judgeforevaluation,whichinevitablycontainsthe\nbiasesfromevaluatorLLMs. Thisposespotentialsocietalrisksifevaluationresultsofthisbenchmark\nisusedtoinformthedevelopmentofconsumer-facinggenerativeAItools. Weadviseresearcheror\ndeveloperscarefullyexanimatepotentialbiasesbeforemakingchoicesinpracticalapplications.\nWealsoacknowledgethelimitationsofonlyworkingwithtextdatainsteadofmultimodaldata. There\naretwomainreasons: 1)therearemanymorecreativetasksintextmodalitythaninmultimodal\nthathavereliableautomaticevaluationmethods. 2)Wewanttobuildtheevaluationframeworkfirst\nbeforeweexpandtoothermodalities. Givenawell-definedevaluationframework,wecaneasily\nextendourbenchmarksfurthertoincludemultimodalsettingsinfuturework.\nThereisalsoalimitationinhowwellourtaskselectionworks: weselectourtasksandmetricsbased\nontheavailabilityofscalable, automaticevaluationmethods, whichmeanswenaturallyexclude\nhigh-conceptmetrics,especiallyfornovelty,whereachievinggenuinenoveltyrequiresreasoningata",
    "char_length": 1438
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 22,
    "text": "veryhighlevel. However,sincethisisalsochallengingforhumans(e.g.,judgingnoveltyinartwork\nrequiresyearsoftraining),webelieveitisreasonablethatnoautomaticevaluationisavailablefor\nthosehigh-conceptmetrics. Giventhat,weadvisetheresearcherswhoareusingourbenchmarktobe\nawareofthislimitationbeforehand.\nLast but not least, we did not conduct any fine-tuning experiments due to limited computational\nresources. Fine-tuning existing LLMs on a subset of CREATIVITYPRISM and evaluating their\nperformancerepresentsanexcitingdirectionforfuturework. Alsoduetolimitationofresources,we\nonlyincludetasksthatalreadyhaveautomaticevaluationmethods. Manycreativitytasksrequire\nhumanevaluation(e.g.,[61])andfutureworkshouldstudyeffectivewaystoautomaticallyevaluating\nthem.\nReferences\n[1] AhmedMAbdullaAlabbasi,SueHyeonPaek,DaehyunKim,andBonnieCramond. Whatdo\neducatorsneedtoknowaboutthetorrancetestsofcreativethinking: Acomprehensivereview.\nFront.Psychol.,13:1000385,October2022.\n[2] Anthropic. Claude 3 model family, 2024. URL https://www.anthropic.com/news/\nclaude-3-family. Accessed: 2025-04-30.\n10\n[3] AnirudhAtmakuru,JatinNainani,RohithSiddharthaReddyBheemreddy,AnirudhLakkaraju,\nZonghaiYao,HamedZamani,andHaw-ShiuanChang. CS4: Measuringthecreativityoflarge\nlanguagemodelsautomaticallybycontrollingthenumberofstory-writingconstraints. arXiv\n[cs.CL],October2024.\n[4] AntoineBellemare-Pepin,Fran√ßoisLespinasse,PhilippTh√∂lke,YannHarel,KoryMathewson,",
    "char_length": 1429
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 23,
    "text": "Jay A Olson, Yoshua Bengio, and Karim Jerbi. Divergent creativity in humans and large\nlanguagemodels. arXiv[cs.CL],May2024.\n[5] MargaretABoden,editor. DimensionsofCreativity. TheMITPress,June1994.\n[6] L√©onardBoussioux,JacquelineNLane,MiaomiaoZhang,VladimirJacimovic,andKarimR\nLakhani. The crowdless future? generative ai and creative problem-solving. Organization\nScience,35(5):1589‚Äì1607,2024.\n[7] AsliCelikyilmaz,ElizabethClark,andJianfengGao. Evaluationoftextgeneration: Asurvey.\narXiv[cs.CL],June2020.\n[8] TuhinChakrabarty,PhilippeLaban,DivyanshAgarwal,SmarandaMuresan,andChien-Sheng\nWu. Artorartifice? largelanguagemodelsandthefalsepromiseofcreativity. InProceedings\noftheCHIConferenceonHumanFactorsinComputingSystems,volume70,pages1‚Äì34,New\nYork,NY,USA,May2024.ACM.\n[9] TuhinChakrabarty,VishakhPadmakumar,FaezeBrahman,andSmarandaMuresan. Creativity\nsupportintheageoflargelanguagemodels: Anempiricalstudyinvolvingprofessionalwriters.\nInCreativityandCognition,NewYork,NY,USA,June2024.ACM.\n[10] HonghuaChenandNaiDing. Probingthe‚Äúcreativity‚Äùoflargelanguagemodels: Canmodels\nproducedivergentsemanticassociation? InThe2023ConferenceonEmpiricalMethodsin\nNaturalLanguageProcessing,December2023.\n[11] DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu,\nChenggangZhao,ChengqiDeng,ChenyuZhang,ChongRuan,DamaiDai,DayaGuo,Dejian\nYang,DeliChen,DongjieJi,ErhangLi,FangyunLin,FucongDai,FuliLuo,GuangboHao,\nGuantingChen,GuoweiLi,HZhang,HanBao,HanweiXu,HaochengWang,HaoweiZhang,",
    "char_length": 1495
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 24,
    "text": "HonghuiDing,HuajianXin,HuazuoGao,HuiLi,HuiQu,JLCai,JianLiang,JianzhongGuo,\nJiaqiNi,JiashiLi,JiaweiWang,JinChen,JingchangChen,JingyangYuan,JunjieQiu,Junlong\nLi,JunxiaoSong,KaiDong,KaiHu,KaigeGao,KangGuan,KexinHuang,KuaiYu,Lean\nWang,LecongZhang,LeiXu,LeyiXia,LiangZhao,LitongWang,LiyueZhang,MengLi,\nMiaojunWang,MingchuanZhang,MinghuaZhang,MinghuiTang,MingmingLi,NingTian,\nPanpanHuang,PeiyiWang,PengZhang,QianchengWang,QihaoZhu,QinyuChen,Qiushi\nDu,RJChen,RLJin,RuiqiGe,RuisongZhang,RuizhePan,RunjiWang,RunxinXu,Ruoyu\nZhang,RuyiChen, SSLi,ShanghaoLu,ShangyanZhou,ShanhuangChen,ShaoqingWu,\nShengfengYe,ShengfengYe,ShirongMa,ShiyuWang,ShuangZhou,ShuipingYu,Shunfeng\nZhou,ShutingPan,TWang,TaoYun,TianPei,TianyuSun,WLXiao,WangdingZeng,Wanjia\nZhao,WeiAn,WenLiu,WenfengLiang,WenjunGao,WenqinYu,WentaoZhang,XQLi,\nXiangyueJin,XianzuWang,XiaoBi,XiaodongLiu,XiaohanWang,XiaojinShen,Xiaokang\nChen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin\nCheng,XinLiu,XinXie,XingchaoLiu,XingkaiYu,XinnanSong,XinxiaShan,XinyiZhou,\nXinyuYang,XinyuanLi,XuechengSu,XuhengLin,YKLi,YQWang,YXWei,YXZhu,\nYang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun,\nYaohuiLi,YaohuiWang,YiYu,YiZheng,YichaoZhang,YifanShi,YiliangXiong,YingHe,\nYingTang,YishiPiao,YisongWang,YixuanTan,YiyangMa,YiyuanLiu,YongqiangGuo,\nYuWu,YuanOu,YuchenZhu,YuduanWang,YueGong,YuhengZou,YujiaHe,YukunZha,\nYunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang",
    "char_length": 1482
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 25,
    "text": "Zhou,ZFWu,ZZRen,ZehuiRen,ZhangliSha,ZheFu,ZheanXu,ZhenHuang,ZhenZhang,\nZhendaXie,ZhengyanZhang,ZhewenHao,ZhibinGou,ZhichengMa,ZhigangYan,Zhihong\nShao,ZhipengXu,ZhiyuWu,ZhongyuZhang,ZhuoshuLi,ZihuiGu,ZijiaZhu,ZijunLiu,\nZilinLi,ZiweiXie,ZiyangSong,ZiyiGao,andZizhengPan. DeepSeek-V3technicalreport.\narXiv[cs.CL],December2024.\n11\n[12] DeepSeek-AI,DayaGuo,DejianYang,HaoweiZhang,JunxiaoSong,RuoyuZhang,Runxin\nXu,QihaoZhu,ShirongMa,PeiyiWang,XiaoBi,XiaokangZhang,XingkaiYu,YuWu,ZF\nWu,ZhibinGou,ZhihongShao,ZhuoshuLi,ZiyiGao,AixinLiu,BingXue,BingxuanWang,\nBochaoWu,BeiFeng,ChengdaLu,ChenggangZhao,ChengqiDeng,ChenyuZhang,Chong\nRuan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo,\nGuangboHao,GuantingChen,GuoweiLi,HZhang,HanBao,HanweiXu,HaochengWang,\nHonghuiDing,HuajianXin,HuazuoGao,HuiQu,HuiLi,JianzhongGuo,JiashiLi,Jiawei\nWang,JingchangChen,JingyangYuan,JunjieQiu,JunlongLi,JLCai,JiaqiNi,JianLiang,Jin\nChen,KaiDong,KaiHu,KaigeGao,KangGuan,KexinHuang,KuaiYu,LeanWang,Lecong\nZhang,LiangZhao,LitongWang,LiyueZhang,LeiXu,LeyiXia,MingchuanZhang,Minghua\nZhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang,\nPengZhang, QianchengWang, QinyuChen, QiushiDu, RuiqiGe, RuisongZhang, Ruizhe\nPan,RunjiWang,RJChen,RLJin,RuyiChen,ShanghaoLu,ShangyanZhou,Shanhuang\nChen,ShengfengYe,ShiyuWang,ShuipingYu,ShunfengZhou,ShutingPan,SSLi,Shuang\nZhou,ShaoqingWu,ShengfengYe,TaoYun,TianPei,TianyuSun,TWang,WangdingZeng,",
    "char_length": 1453
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 26,
    "text": "WanjiaZhao,WenLiu,WenfengLiang,WenjunGao,WenqinYu,WentaoZhang,WLXiao,\nWeiAn,XiaodongLiu,XiaohanWang,XiaokangChen,XiaotaoNie,XinCheng,XinLiu,Xin\nXie,XingchaoLiu,XinyuYang,XinyuanLi,XuechengSu,XuhengLin,XQLi,Xiangyue\nJin,XiaojinShen,XiaoshaChen,XiaowenSun,XiaoxiangWang,XinnanSong,XinyiZhou,\nXianzuWang,XinxiaShan,YKLi,YQWang,YXWei,YangZhang,YanhongXu,YaoLi,\nYaoZhao,YaofengSun,YaohuiWang,YiYu,YichaoZhang,YifanShi,YiliangXiong,Ying\nHe, YishiPiao, YisongWang, YixuanTan, YiyangMa, YiyuanLiu, YongqiangGuo, Yuan\nOu,YuduanWang,YueGong,YuhengZou,YujiaHe,YunfanXiong,YuxiangLuo,Yuxiang\nYou,YuxuanLiu,YuyangZhou,YXZhu,YanhongXu,YanpingHuang,YaohuiLi,YiZheng,\nYuchenZhu,YunxianMa,YingTang,YukunZha,YutingYan,ZZRen,ZehuiRen,Zhangli\nSha,ZheFu,ZheanXu,ZhendaXie,ZhengyanZhang,ZhewenHao,ZhichengMa,Zhigang\nYan,ZhiyuWu,ZihuiGu,ZijiaZhu,ZijunLiu,ZilinLi,ZiweiXie,ZiyangSong,Zizheng\nPan,ZhenHuang,ZhipengXu,ZhongyuZhang,andZhenZhang. DeepSeek-R1: Incentivizing\nreasoningcapabilityinLLMsviareinforcementlearning. arXiv[cs.CL],January2025.\n[13] JaafarEl-MuradandDouglasCWest. Thedefinitionandmeasurementofcreativity: Whatdo\nweknow? J.Advert.Res.,44(02):188‚Äì201,June2004.\n[14] XinyuFang,ZhijianChen,KaiLan,LixinMa,ShengyuanDing,YingjiLiang,XiangyuZhao,\nFarong Wen, Zicheng Zhang, Guofeng Zhang, Haodong Duan, Kai Chen, and Dahua Lin.\nCreation-MMBench: Assessingcontext-awarecreativeintelligenceinMLLM. arXiv[cs.CV],\nMarch2025.\n[15] RonaldAFinke,ThomasBWard,andStevenMSmith. CreativeCognition: Theory,research,",
    "char_length": 1492
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 27,
    "text": "andapplications. TheMITPress,October1992.\n[16] GiorgioFranceschelliandMircoMusolesi. Creativityandmachinelearning: Asurvey. ACM\nComput. Surv., 56(11), June 2024. ISSN 0360-0300. doi: 10.1145/3664595. URL https:\n//doi.org/10.1145/3664595.\n[17] FabricioGoes,MarcoVolpe,PiotrSawicki,MarekGrzes,andJacobWatson. PushingGPT‚Äôs\ncreativitytoitslimits: Alternativeusesandtorrancetests. In14thInternationalConferenceon\nComputationalCreativity2023,2023.\n[18] GoogleDeepMind. Gemini1.5and2.0: Next-genmultimodalmodels,2024. URLhttps:\n//deepmind.google/technologies/gemini/. Accessed: 2025-04-30.\n[19] AaronGrattafiori,AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,Ah-\nmadAl-Dahle,AieshaLetman,AkhilMathur,AlanSchelten,AlexVaughan,AmyYang,Angela\nFan,AnirudhGoyal,AnthonyHartshorn,AoboYang,ArchiMitra,ArchieSravankumar,Artem\nKorenev,ArthurHinsvark,ArunRao,AstonZhang,AurelienRodriguez,AustenGregerson,\nAvaSpataru,BaptisteRoziere,BethanyBiron,BinhTang,BobbieChern,CharlotteCaucheteux,\nChayaNayak,ChloeBi,ChrisMarra,ChrisMcConnell,ChristianKeller,ChristopheTouret,\nChunyangWu,CorinneWong,CristianCantonFerrer,CyrusNikolaidis,DamienAllonsius,\nDanielSong,DaniellePintz,DannyLivshits,DannyWyatt,DavidEsiobu,DhruvChoudhary,\nDhruvMahajan,DiegoGarcia-Olano,DiegoPerino,DieuwkeHupkes,EgorLakomkin,Ehab\n12\nAlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco\nGuzm√°n,FrankZhang,GabrielSynnaeve,GabrielleLee,GeorgiaLewisAnderson,Govind",
    "char_length": 1454
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 28,
    "text": "Thattai,GraemeNail,GregoireMialon,GuanPang,GuillemCucurell,HaileyNguyen,Hannah\nKorevaar,HuXu,HugoTouvron,IliyanZarov,ImanolArrietaIbarra,IsabelKloumann,Ishan\nMisra,IvanEvtimov,JackZhang,JadeCopet,JaewonLee,JanGeffert,JanaVranes,Jason\nPark,JayMahadeokar,JeetShah,JelmervanderLinde,JenniferBillock,JennyHong,Jenya\nLee,JeremyFu,JianfengChi,JianyuHuang,JiawenLiu,JieWang,JiecaoYu,JoannaBitton,\nJoeSpisak,JongsooPark,JosephRocca,JoshuaJohnstun,JoshuaSaxe,JuntengJia,KalyanVa-\nsuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield,\nKevinStone,KhalidEl-Arini,KrithikaIyer,KshitizMalik,KuenleyChiu,KunalBhalla,Kushal\nLakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz\nJenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke\nde Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin\nKardas,MariaTsimpoukelli,MathewOldham,MathieuRita,MayaPavlova,MelanieKam-\nbadur,MikeLewis,MinSi,MiteshKumarSingh,MonaHassan,NamanGoyal,NarjesTorabi,\nNikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne,\nOnur√áelebi,PatrickAlrassy,PengchuanZhang,PengweiLi,PetarVasic,PeterWeng,Prajjwal\nBhargava,PratikDubal,PraveenKrishnan,PunitSinghKoura,PuxinXu,QingHe,Qingxiao\nDong,RagavanSrinivasan,RajGanapathy,RamonCalderer,RicardoSilveiraCabral,Robert\nStojnic,RobertaRaileanu,RohanMaheswari,RohitGirdhar,RohitPatel,RomainSauvestre,",
    "char_length": 1451
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 29,
    "text": "RonniePolidoro,RoshanSumbaly,RossTaylor,RuanSilva,RuiHou,RuiWang,SagharHos-\nseini,SahanaChennabasappa,SanjaySingh,SeanBell,SeohyunSoniaKim,SergeyEdunov,\nShaoliangNie,SharanNarang,SharathRaparthy,ShengShen,ShengyeWan,ShrutiBhosale,\nShunZhang,SimonVandenhende,SoumyaBatra,SpencerWhitman,StenSootla,Stephane\nCollot,SuchinGururangan,SydneyBorodinsky,TamarHerman,TaraFowler,TarekSheasha,\nThomasGeorgiou,ThomasScialom,TobiasSpeckbacher,TodorMihaylov,TongXiao,Ujjwal\nKarn,VedanujGoswami,VibhorGupta,VigneshRamanathan,ViktorKerkez,VincentGonguet,\nVirginieDo,VishVogeti,V√≠torAlbiero,VladanPetrovic,WeiweiChu,WenhanXiong,Wenyin\nFu,WhitneyMeers,XavierMartinet,XiaodongWang,XiaofangWang,XiaoqingEllenTan,\nXideXia,XinfengXie,XuchaoJia,XueweiWang,YaelleGoldschlag,YasheshGaur,Yasmine\nBabaei,YiWen,YiwenSong,YuchenZhang,YueLi,YuningMao,ZacharieDelpierreCoudert,\nZhengYan,ZhengxingChen,ZoePapakipos,AadityaSingh,AayushiSrivastava,AbhaJain,\nAdam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay\nMenon,AjaySharma,AlexBoesenberg,AlexeiBaevski,AllieFeinstein,AmandaKallet,Amit\nSangani,AmosTeo,AnamYunus,AndreiLupu,AndresAlvarado,AndrewCaples,AndrewGu,\nAndrewHo,AndrewPoulton,AndrewRyan,AnkitRamchandani,AnnieDong,AnnieFranco,\nAnuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe,\nAssafEisenman,AzadehYazdan,BeauJames,BenMaurer,BenjaminLeonhardi,BernieHuang,\nBethLoyd,BetoDePaola,BhargaviParanjape,BingLiu,BoWu,BoyuNi,BradenHancock,",
    "char_length": 1473
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 30,
    "text": "BramWasti, BrandonSpence, BraniStojkovic, BrianGamido, BrittMontalvo, CarlParker,\nCarlyBurton,CatalinaMejia,CeLiu,ChanghanWang,ChangkyuKim,ChaoZhou,Chester\nHu,Ching-HsiangChu,ChrisCai,ChrisTindal,ChristophFeichtenhofer,CynthiaGao,Damon\nCivin,DanaBeaty,DanielKreymer,DanielLi,DavidAdkins,DavidXu,DavideTestuggine,\nDelia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin\nHolland,EdwardDowling,EissaJamil,ElaineMontgomery,EleonoraPresani,EmilyHahn,\nEmilyWood,Eric-TuanLe,ErikBrinkman,EstebanArcaute,EvanDunbar,EvanSmothers,\nFeiSun,FelixKreuk,FengTian,FilipposKokkinos,FiratOzgenel,FrancescoCaggioni,Frank\nKanayet,FrankSeide,GabrielaMedinaFlorez,GabriellaSchwarz,GadaBadeer,GeorgiaSwee,\nGilHalpern,GrantHerman,GrigorySizov,Guangyi,Zhang,GunaLakshminarayanan,Hakan\nInan,HamidShojanazeri,HanZou,HannahWang,HanwenZha,HarounHabeeb,Harrison\nRudolph, HelenSuk, HenryAspegren, HunterGoldman, HongyuanZhan, IbrahimDamlaj,\nIgorMolybog,IgorTufanov,IliasLeontiadis,Irina-ElenaVeliche,ItaiGat,JakeWeissman,\nJamesGeboski,JamesKohli,JaniceLam,JaphetAsher,Jean-BaptisteGaya,JeffMarcus,Jeff\nTang,JenniferChan,JennyZhen,JeremyReizenstein,JeremyTeboul,JessicaZhong,JianJin,\nJingyiYang,JoeCummings,JonCarvill,JonShepard,JonathanMcPhie,JonathanTorres,Josh\nGinsburg,JunjieWang,KaiWu,KamHouU,KaranSaxena,KartikayKhandelwal,Katayoun\nZand,KathyMatosich,KaushikVeeraraghavan,KellyMichelena,KeqianLi,KiranJagadeesh,\nKunHuang,KunalChawla,KyleHuang,LailinChen,LakshyaGarg,LavenderA,Leandro",
    "char_length": 1487
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 31,
    "text": "Silva,LeeBell,LeiZhang,LiangpengGuo,LichengYu,LironMoshkovich,LucaWehrstedt,\n13\nMadianKhabsa,ManavAvalani,ManishBhatt,MartynasMankus,MatanHasson,Matthew\nLennie,MatthiasReso,MaximGroshev,MaximNaumov,MayaLathi,MeghanKeneally,Miao\nLiu,MichaelLSeltzer,MichalValko,MichelleRestrepo,MihirPatel,MikVyatskov,Mikayel\nSamvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat,\nMohammadRastegari,MunishBansal,NandhiniSanthanam,NataschaParks,NatashaWhite,\nNavyataBawa,NayanSinghal,NickEgebo,NicolasUsunier,NikhilMehta,NikolayPavlovich\nLaptev,NingDong,NormanCheng,OlegChernoguz,OliviaHart,OmkarSalpekar,Ozlem\nKalinli,ParkinKent,ParthParekh,PaulSaab,PavanBalaji,PedroRittner,PhilipBontrager,\nPierreRoux,PiotrDollar,PolinaZvyagina,PrashantRatanchandani,PritishYuvraj,QianLiang,\nRachadAlao,RachelRodriguez,RafiAyub,RaghothamMurthy,RaghuNayani,RahulMitra,\nRangaprabhuParthasarathy,RaymondLi,RebekkahHogan,RobinBattey,RockyWang,Russ\nHowes,RutyRinott,SachinMehta,SachinSiby,SaiJayeshBondu,SamyakDatta,SaraChugh,\nSaraHunt,SargunDhillon, SashaSidorov, SatadruPan, SaurabhMahajan,SaurabhVerma,\nSeijiYamamoto,SharadhRamaswamy,ShaunLindsay,ShaunLindsay,ShengFeng,Shenghao\nLin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang,\nSinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen\nChen,SteveKehoe,SteveSatterfield,SudarshanGovindaprasad,SumitGupta,SummerDeng,\nSungminCho,SunnyVirk,SurajSubramanian,SyChoudhury,SydneyGoldman,TalRemez,",
    "char_length": 1495
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 32,
    "text": "TamarGlaser,TamaraBest,ThiloKoehler,ThomasRobinson,TianheLi,TianjunZhang,Tim\nMatthews,TimothyChou,TzookShaked,VarunVontimitta,VictoriaAjayi,VictoriaMontanez,\nVijaiMohan,VinaySatishKumar,VishalMangla,VladIonescu,VladPoenaru,VladTiberiu\nMihailescu,VladimirIvanov,WeiLi,WenchenWang,WenwenJiang,WesBouaziz,WillCon-\nstable,XiaochengTang,XiaojianWu,XiaolanWang,XilunWu,XinboGao,YanivKleinman,\nYanjunChen,YeHu,YeJia,YeQi,YendaLi,YilinZhang,YingZhang,YossiAdi,Youngjin\nNam,Yu,Wang,YuZhao,YuchenHao,YundiQian,YunluLi,YuziHe,ZachRait,Zachary\nDeVito,ZefRosnbrick,ZhaoduoWen,ZhenyuYang,ZhiweiZhao,andZhiyuMa. Thellama3\nherdofmodels. arXiv[cs.AI],July2024.\n[20] DirkGroeneveld,IzBeltagy,EvanWalsh,AkshitaBhagia,RodneyKinney,OyvindTafjord,\nAnanyaJha,HamishIvison,IanMagnusson,YizhongWang,ShaneArora,DavidAtkinson,\nRussellAuthur,KhyathiChandu,ArmanCohan,JenniferDumas,YanaiElazar,YulingGu,Jack\nHessel,TusharKhot,WilliamMerrill,JacobMorrison,NiklasMuennighoff,AakankshaNaik,\nCrystalNam, MatthewPeters, ValentinaPyatkin, AbhilashaRavichander, DustinSchwenk,\nSaurabhShah,WilliamSmith,EmmaStrubell,NishantSubramani,MitchellWortsman,Pradeep\nDasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca\nSoldaini,NoahSmith,andHannanehHajishirzi. OLMo: Acceleratingthescienceoflanguage\nmodels. In Proceedings of the 62nd Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 15789‚Äì15809, Stroudsburg, PA, USA, 2024.\nAssociationforComputationalLinguistics.",
    "char_length": 1499
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 33,
    "text": "[21] JPGuilford,PaulRChristensen,PhilipRMerrifield,andRobertCWilson. Alternateuses,\nJune2012. Titleofthepublicationassociatedwiththisdataset: PsycTESTSDataset.\n[22] Carlos G√≥mez-Rodr√≠guez and Paul Williams. A confederacy of models: A comprehensive\nevaluation of LLMs on creative writing. In Findings of the Association for Computational\nLinguistics: EMNLP 2023, pages 14504‚Äì14528, Stroudsburg, PA, USA, December 2023.\nAssociationforComputationalLinguistics.\n[23] HeHe,NanyunPeng,andPercyLiang. Pungenerationwithsurprise. InJillBurstein,Christy\nDoran,andThamarSolorio,editors,Proceedingsofthe2019ConferenceoftheNorthAmerican\nChapter of the Association for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 1734‚Äì1744, Minneapolis, Minnesota, June 2019.\nAssociationforComputationalLinguistics.\n[24] ZicongHe,BoxuanZhang,WeihaoLiu,RuixiangTang,andLuCheng. Whatshapesacreative\nmachinemind? comprehensivelybenchmarkingcreativityinfoundationmodels. arXiv[cs.AI],\nOctober2025.\n[25] K.J.HolyoakandR.G.Morrison. TheCambridgeHandbookofThinkingandReasoning. Cam-\nbridgeHandbooksinPsychology.CambridgeUniversityPress,2005. ISBN9780521824170.\nURLhttps://books.google.com/books?id=znbkHaC8QeMC.\n14\n[26] WeipingHuandPhilipAdey. Ascientificcreativitytestforsecondaryschoolstudents. Int.J.\nSci.Educ.,24(4):389‚Äì403,April2002.\n[27] BinyuanHui,JianYang,ZeyuCui,JiaxiYang,DayihengLiu,LeiZhang,TianyuLiu,Jiajun",
    "char_length": 1434
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 34,
    "text": "Zhang,BowenYu,KemingLu,KaiDang,YangFan,YichangZhang,AnYang,RuiMen,Fei\nHuang,BoZheng,YiboMiao,ShanghaoranQuan,YunlongFeng,XingzhangRen,Xuancheng\nRen,JingrenZhou,andJunyangLin.Qwen2.5-codertechnicalreport.arXiv[cs.CL],September\n2024.\n[28] MeteIsmayilzada,DebjitPaul,AntoineBosselut,andLonnekevanderPlas. Creativityinai:\nProgressesandchallenges,2024. URLhttps://arxiv.org/abs/2410.17218.\n[29] MeteIsmayilzada,ClaireStevenson,andLonnekevanderPlas. Evaluatingcreativeshortstory\ngenerationinhumansandlargelanguagemodels. arXiv[cs.CL],November2024.\n[30] MeteIsmayilzada,ClaireStevenson,andLonnekevanderPlas. Evaluatingcreativeshortstory\ngenerationinhumansandlargelanguagemodels. arXivpreprintarXiv:2411.02316,2024.\n[31] ShomikJain,JackLanchantin,MaximilianNickel,KarenUllrich,AshiaWilson,andJamelle\nWatson-Daniels. LLMoutputhomogenizationistaskdependent. arXiv[cs.CL],September\n2025.\n[32] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot,DiegodelasCasas,FlorianBressand,GiannaLengyel,GuillaumeLample,Lucile\nSaulnier,L√©lioRenardLavaud,Marie-AnneLachaux,PierreStock,TevenLeScao,Thibaut\nLavril,ThomasWang,Timoth√©eLacroix,andWilliamElSayed. Mistral7B. arXiv[cs.CL],\nOctober2023.\n[33] AlbertQJiang,AlexandreSablayrolles,AntoineRoux,ArthurMensch,BlancheSavary,Chris\nBamford,DevendraSinghChaplot,DiegodelasCasas,EmmaBouHanna,FlorianBressand,\nGiannaLengyel,GuillaumeBour,GuillaumeLample,L√©lioRenardLavaud,LucileSaulnier,",
    "char_length": 1450
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 35,
    "text": "Marie-AnneLachaux,PierreStock,SandeepSubramanian,SophiaYang,SzymonAntoniak,\nTeven Le Scao, Th√©ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, and\nWilliamElSayed. Mixtralofexperts. arXiv[cs.LG],January2024.\n[34] JaehunJung,FaezeBrahman,andYejinChoi. Trustorescalate: LLMjudgeswithprovable\nguarantees for human agreement. In The Thirteenth International Conference on Learning\nRepresentations,October2024.\n[35] KorayKavukcuoglu. Gemini2.5: OurmostintelligentAImodel. https://blog.google/\ntechnology/google-deepmind/gemini-model-thinking-updates-march-2025/,\nMarch2025. Accessed: 2025-4-30.\n[36] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu,\nJosephE.Gonzalez,HaoZhang,andIonStoica. Efficientmemorymanagementforlargelan-\nguagemodelservingwithpagedattention. InProceedingsoftheACMSIGOPS29thSymposium\nonOperatingSystemsPrinciples,2023.\n[37] DaweiLi,BohanJiang,LiangjieHuang,AlimohammadBeigi,ChengshuaiZhao,ZhenTan,\nAmritaBhattacharjee,YuxuanJiang,CanyuChen,TianhaoWu,KaiShu,LuCheng,andHuan\nLiu. Fromgenerationtojudgment: Opportunitiesandchallengesofllm-as-a-judge,2024.\n[38] HaitaoLi,QianDong,JunjieChen,HuixueSu,YujiaZhou,QingyaoAi,ZiyiYe,andYiqun\nLiu. Llms-as-judges: Acomprehensivesurveyonllm-basedevaluationmethods,2024. URL\nhttps://arxiv.org/abs/2412.05579.\n[39] Li-ChunLu,MiriLiu,Pin-ChunLu,YufeiTian,Shao-HuaSun,andNanyunPeng. Rethinking\ncreativityevaluation:Acriticalanalysisofexistingcreativityevaluations.arXiv[cs.CL],August\n2025.",
    "char_length": 1483
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 36,
    "text": "[40] XimingLu,MelanieSclar,SkylerHallinan,NiloofarMireshghallah,JiachengLiu,Seungju\nHan, Allyson Ettinger, Liwei Jiang, Khyathi Chandu, Nouha Dziri, and Yejin Choi. AI\nas humanity‚Äôs salieri: Quantifying linguistic creativity of language models via systematic\nattributionofmachinetextagainstwebtext. InTheThirteenthInternationalConferenceon\nLearningRepresentations,October2024.\n15\n[41] XimingLu,MelanieSclar,SkylerHallinan,NiloofarMireshghallah,JiachengLiu,Seungju\nHan,AllysonEttinger,LiweiJiang,KhyathiRaghaviChandu,NouhaDziri,andYejinChoi.\nAiashumanity‚Äôssalieri: Quantifyinglinguisticcreativityoflanguagemodelsviasystematic\nattribution of machine text against web text. CoRR, abs/2410.04265, 2024. URL https:\n//doi.org/10.48550/arXiv.2410.04265.\n[42] YiningLu,DixuanWang,TianjianLi,DongweiJiang,SanjeevKhudanpur,MengJiang,and\nDanielKhashabi.Benchmarkinglanguagemodelcreativity:Acasestudyoncodegeneration.In\nProceedingsofthe2025ConferenceoftheNationsoftheAmericasChapteroftheAssociation\nforComputationalLinguistics: HumanLanguageTechnologies(Volume1: LongPapers),pages\n2776‚Äì2794,2025.\n[43] AidanMcLaughlin,AnujaUppuluri,andJamesCampbell. AidanBench: Evaluatingnovelidea\ngenerationonopen-endedquestions. InLanguageGamification-NeurIPS2024Workshop,\nDecember2024.\n[44] SaifMohammad. Obtainingreliablehumanratingsofvalence, arousal, anddominancefor\n20,000 english words. In Proceedings of the 56th Annual Meeting of the Association for",
    "char_length": 1432
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 37,
    "text": "ComputationalLinguistics(Volume1: LongPapers),pages174‚Äì184,Stroudsburg,PA,USA,\n2018.AssociationforComputationalLinguistics.\n[45] JacquelineN.Lane,LeonardBoussioux,CharlesAyoubi,YingHaoChen,CamilaLin,Rebecca\nSpens, Pooja Wagh, and Pei-Hsin Wang. The narrative AI advantage? a field experiment\nongenerativeAI-augmentedevaluationsofearly-stageinnovations. SocialScienceResearch\nNetwork,August2024.\n[46] JayAOlson,JohnnyNahas,DenisChmoulevitch,SimonJCropper,andMargaretEWebb.Nam-\ningunrelatedwordspredictscreativity. Proc.Natl.Acad.Sci.U.S.A.,118(25):e2022340118,\nJune2021.\n[47] OpenAI. Gpt-4technicalreport,2024. URLhttps://openai.com/research/gpt-4. Ac-\ncessed: 2025-04-30.\n[48] PeterOrganisciak,SelcukAcar,DenisDumas,andKellyBerthiaume.Beyondsemanticdistance:\nAutomatedscoringofdivergentthinkinggreatlyimproveswithlargelanguagemodels. Think.\nSkillsCreat.,49(101356):101356,September2023.\n[49] VishakhPadmakumarandHeHe. Doeswritingwithlanguagemodelsreducecontentdiversity?\nInTheTwelfthInternationalConferenceonLearningRepresentations,October2023.\n[50] JeffreyPennington,RichardSocher,andChristopherManning. GloVe: Globalvectorsforword\nrepresentation. InAlessandroMoschitti,BoPang,andWalterDaelemans,editors,Proceedings\nof the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),\npages1532‚Äì1543,Doha,Qatar,October2014.AssociationforComputationalLinguistics. doi:\n10.3115/v1/D14-1162. URLhttps://aclanthology.org/D14-1162/.",
    "char_length": 1444
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 38,
    "text": "[51] JeffreyPennington,RichardSocher,andChristopherDManning. GloVe: Globalvectorsfor\nwordrepresentation. InProceedingsofthe2014ConferenceonEmpiricalMethodsinNatural\nLanguageProcessing(EMNLP),pages1532‚Äì1543,October2014.\n[52] Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,\nChengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu,\nJianweiZhang,JianxinYang,JiaxiYang,JingrenZhou,JunyangLin,KaiDang,KemingLu,\nKeqinBao,KexinYang,LeYu,MeiLi,MingfengXue,PeiZhang,QinZhu,RuiMen,Runji\nLin,TianhaoLi,TianyiTang,TingyuXia,XingzhangRen,XuanchengRen,YangFan,Yang\nSu,YichangZhang,YuWan,YuqiongLiu,ZeyuCui,ZhenruZhang,andZihanQiu. Qwen2.5\ntechnicalreport. arXiv[cs.CL],December2024.\n[53] SaraRosengren,MartinEisend,ScottKoslow,andMicaelDahlen. Ameta-analysisofwhen\nandhowadvertisingcreativityworks. J.Mark.,84(6):39‚Äì56,November2020.\n[54] Sameh Said-Metwaly, Wim Van den Noortgate, and Eva Kyndt. Approaches to measuring\ncreativity: Asystematicliteraturereview. Creativity.Theories‚ÄìResearch-Applications,4(2):\n238‚Äì275,December2017.\n16\n[55] ChengleiSi,DiyiYang,andTatsunoriHashimoto. CanLLMsgeneratenovelresearchideas?\nalarge-scalehumanstudywith100+NLPresearchers. InTheThirteenthInternationalCon-\nferenceonLearningRepresentations,2025. URLhttps://openreview.net/forum?id=\nM23dTGWCZy.\n[56] RobertESmith,ScottBMacKenzie,XiaojingYang,LauraMBuchholz,andWilliamKDarley.\nModelingthedeterminantsandeffectsofcreativityinadvertising. Mark.Sci.,26(6):819‚Äì833,",
    "char_length": 1491
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 39,
    "text": "November2007.\n[57] RSternbergandTLubart. Aninvestmenttheoryofcreativityanditsdevelopment. Human\nDevelopment,34(1):1‚Äì31,June1991.\n[58] SijunTan,SiyuanZhuang,KyleMontgomery,WilliamYuanTang,AlejandroCuadron,Chen-\nguangWang,RalucaPopa,andIonStoica.Judgebench:AbenchmarkforevaluatingLLM-based\njudges. InTheThirteenthInternationalConferenceonLearningRepresentations,2025. URL\nhttps://openreview.net/forum?id=G0dksFayVq.\n[59] YufeiTian,TenghaoHuang,MiriLiu,DerekJiang,AlexanderSpangher,MuhaoChen,Jonathan\nMay,andNanyunPeng. Arelargelanguagemodelscapableofgeneratinghuman-levelnar-\nratives? InProceedingsofthe2024ConferenceonEmpiricalMethodsinNaturalLanguage\nProcessing, pages 17659‚Äì17681, Stroudsburg, PA, USA, November 2024. Association for\nComputationalLinguistics.\n[60] YufeiTian,AbhilashaRavichander,LianhuiQin,RonanLeBras,RajaMarjieh,NanyunPeng,\nYejinChoi, ThomasGriffiths, andFaezeBrahman. MacGyver: Arelargelanguagemodels\ncreative problem solvers? In Proceedings of the 2024 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies\n(Volume 1: Long Papers), pages 5303‚Äì5324, Stroudsburg, PA, USA, 2024. Association for\nComputationalLinguistics.\n[61] Yufei Tian, Abhilasha Ravichander, Lianhui Qin, Ronan Le Bras, Raja Marjieh, Nanyun\nPeng, Yejin Choi, Thomas Griffiths, and Faeze Brahman. MacGyver: Are large language\nmodelscreativeproblemsolvers? InKevinDuh,HelenaGomez,andStevenBethard,editors,",
    "char_length": 1458
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 40,
    "text": "Proceedings of the 2024 Conference of the North American Chapter of the Association for\nComputationalLinguistics: HumanLanguageTechnologies(Volume1: LongPapers),pages\n5303‚Äì5324,MexicoCity,Mexico,June2024.AssociationforComputationalLinguistics. doi:\n10.18653/v1/2024.naacl-long.297. URLhttps://aclanthology.org/2024.naacl-long.\n297/.\n[62] JunyiYe,JingyiGu,XinyunZhao,WenpengYin,andGuilingWang. Assessingthecreativity\nofLLMsinproposingnovelsolutionstomathematicalproblems. arXiv[cs.CL],October2024.\n[63] TianyiZhang,VarshaKishore,FelixWu,KilianQWeinberger,andYoavArtzi. BERTScore:\nEvaluatingtextgenerationwithBERT. arXiv[cs.CL],April2019.\n[64] YimingZhang,HarshitaDiddee,SusanHolm,HanchenLiu,XinyueLiu,VinaySamuel,Barry\nWang,andDaphneIppolito. NoveltyBench: Evaluatingcreativityanddiversityinlanguage\nmodels. InSecondConferenceonLanguageModeling,August2025.\n[65] YunpuZhao,RuiZhang,WenyiLi,andLingLi. Assessingandunderstandingcreativityin\nlargelanguagemodels. Mach.Intell.Res.,pages1‚Äì20,April2025.\n[66] LianminZheng,Wei-LinChiang,YingSheng,SiyuanZhuang,ZhanghaoWu,YonghaoZhuang,\nZiLin,ZhuohanLi,DachengLi,EricPXing,HaoZhang,JosephEGonzalez,andIonStoica.\nJudgingLLM-as-a-judgewithMT-benchandchatbotarena. arXiv[cs.CL],June2023.\n[67] ShanshanZhong,ZhongzhanHuang,ShanghuaGao,WushaoWen,LiangLin,MarinkaZitnik,\nandPanZhou. Let‚Äôsthinkoutsidethebox: Exploringleap-of-thoughtinlargelanguagemodels\nwithcreativehumorgeneration.In2024IEEE/CVFConferenceonComputerVisionandPattern",
    "char_length": 1467
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 41,
    "text": "Recognition(CVPR),pages13246‚Äì13257,2024. doi: 10.1109/CVPR52733.2024.01258.\n17\nA ModelDetails\nShortName ExactModelName Size Family ReleaseTime\nMistral-7B Mistral-7B-Instruct-v0.3 7B Mistral 05/2024\nQwen2.5-7B Qwen2.5-7B-Instruct 7B Qwen 09/2024\nOLMo2-7B OLMo-2-1124-7B-Instruct 7B Olmo 11/2024\nLlama3.1-8B Llama-3.1-8B-Instruct 8B Llama 07/2024\nOLMo2-13B OLMo-2-1124-13B-Instruct 13B Olmo 11/2024\nOLMo2-13B-SFT OLMo-2-1124-13B-SFT 13B Olmo 11/2024\nOLMo2-13B-DPO OLMo-2-1124-13B-DPO 13B Olmo 11/2024\nMistral-24B Mistral-Small-24B-Instruct-2501 24B Mistral 01/2025\nQwen2.5-32B Qwen2.5-32B-Instruct 32B Qwen 09/2024\nMixtral-8x7B Mixtral-8x7B-Instruct-v0.1 56B Mistral 12/2023\nLlama3.3-70B Llama-3.3-70B-Instruct 70B Llama 12/2024\nQwen2.5-72B Qwen2.5-72B-Instruct 72B Qwen 09/2024\nClaude3-Sonnet claude-3-7-sonnet-20250219 - Claude 02/2025\nClaude3-Haiku claude-3-5-haiku-20241022 - Claude 11/2024\nGPT4.1 gpt-4.1-2025-04-14 - GPT 04/2025\nGPT4.1-mini gpt-4.1-mini-2025-04-14 - GPT 04/2025\nGemini2.0-Flash gemini-2.0-flash - Gemini 12/2024\nDeepseek-R1 deepseek-reasoner - Gemini 01/2025\nDeepseek-V3 deepseek-chat - Gemini 12/2024\nTable3: Listofmodelsincludedinourbenchmark.\nDeepseekModels ForDeepseekmodels,wealsouseAPIduetoconstraintsincomputeresources.\nAPIconsole: https://platform.deepseek.com.\n18\nB BenchmarkDesign\nB.1 DatasetSizes\nTask Count Note\nAUT 105(tooluse) 21toolswith5roundsofpromptingpertool\nDAT 100(round) Noinputdata,weprompteachLLM100rounds\nTTCT 700(question) 7tasks(100questions/task)",
    "char_length": 1496
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 42,
    "text": "TTCW 12(storyprompt) Onestoryperstoryprompt\nCreativeShortStory 10(keywordtuple) Onestoryperkeywordtuple\nCreativityIndex 300(documentsample) 100samplesfrom3subsets:book,poem,andspeech\nCS4 250(story) 50basestorieswith5constraintconfigurationsperstory\nNeoCoder 198(question) Onesolutionpercodingquestion\nCreativeMath 400(question) Onesolutionpermathquestion\nTable4: DatasetsizeofCREATIVITYPRISM. Moredetailscanbefoundinthecorrespondingsection\nofAppendixE.\nB.2 Metrics\nTable5showsacompletelisttometricsinCREATIVITYPRISM,groupedbytasks. Moredetailsabout\nhoweachmetriciscalculatedcanbefoundincorrespondingsectionsinAppendixE.\nTask Quality Novelty Diversity\nAUT - AUTScore -\nDAT - - DATScore\nTTCT Fluency,Elaboration Originality Flexibility\nTTCW Coherence,Ending,Elaboration - EmotionalFlexibility\nCreativeShortStory - NoveltyScore,Surprise-ness N-gramDiversity\nCreativityIndex - L-uniqueness -\nCS4 QUC RCS Dist-N\nNeoCoder Convergence@k Divergent@k -\nCreativeMath CorrectnessRatio NoveltyRatio -\nTable5: Evaluationmetricsin CREATIVITYPRISM; ‚Äú-‚Äùmeansthistask(row)doesnothaveany\nmetricinthecorrespondingcreativitydimension.\nB.3 ScoreCalculations\nScoreNormalization ForeverymodeliandeveryrawmetricscoreS (metricmlivesonsome\ni,m\nknownscale[min ,max ]),thenormalizedscoreSÀÜ isgivenby:\nm m i,m\nS ‚àímin\nSÀÜ = i,m m\ni,m max ‚àímin\nm m\nS ‚àí1 S ‚àí1\nForexample,AUTscoreisona1‚Äì5Likertscale: SÀÜ = i,AUT = i,AUT .\ni,AUT 5‚àí1 4\nAggregateNormalizedScores First,wecollapsemultiplemetricswithinthesametask: iftaskt",
    "char_length": 1483
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 43,
    "text": "hasasetM ofk metricsinagivendimension(e.g. threequalitymetricsforTTCW),averagethem\nt t\nfirst:\nS¬Ø = 1 (cid:88) SÀÜ\ni,t k i,m\nt\nm‚ààMt\n19\nThen,wetakeaverageacrossalltasksthatbelongtothatdimension. LetT ,T ,T bethetask\nqual nov div\nsetsforquality,novelty,diversity. Fordimensiond‚àà{qual,nov,div}:\nD(d) = 1 (cid:88) S¬Ø\ni |T | i,t\nd\nt‚ààTd\nInthisway,weendupwiththreenumberspermodel: D(qual), D(nov), D(div). Wecanalsocalculate\ni i i\naggregatedscoreforcreativewriting,divergentthinking,andlogicalreasoning(asshowninTable2).\nOverall creativity score Just take the straight mean of those three dimension scores to stay\nbalanced:\nD(qual)+D(nov)+D(div)\nC = i i i\ni 3\nC PerformanceSummaries\nFigure5: Overallperformances.\n20\nFigure6: Inter-metriccorrelation(groupedbycreativitytaxonomy).\n21\nFigure7: Performanceonqualitydimension\nFigure8: Performanceonnoveltydimension\nFigure9: Performanceondiversitydimension\n22\nFigure10: Performanceoncreativewritingtasks\nFigure11: Performanceondivergentthinkingtasks\nFigure12: Performanceonlogicalreasoningdimension\n23\nD LLM-as-a-JudgeDesignDetails\nSixoutofninetasksinourbenchmarkrequireLLM-Judgeforoneormoremetrics. WeuseQwen2.5-\n72BasthedefaultLLM-Judgemodel,unlessotherwisespecified. ThechoiceofQwen2.5-72Bis\nbasedonapilotstudyonTTCWandAUTtask,whereQwen2.5-72Bistheopen-sourceLLM(within\nourcomputebudget)thatcorrelatesthebesteitherwithhumansorwithclosed-sourceLLMsthatare\ntypicallyusedasLLM-Judge(asdetailedbelow).",
    "char_length": 1436
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 44,
    "text": "ToensurethereliabilityoftheLLM-Judge,wefollowthefollowingprinciples: iftheoriginalpaper\nhasreportedhuman-LLM-Judgeagreementandweareusingthesamesetup,wedirectlyreportthe\noriginalpaper‚Äôsagreement;otherwise,ifwehavehumanannotation,we‚Äôllcalculatetheagreement\nbetweenhumanjudgmentandLLM-Judge‚Äôsjudgement;ifnohumanannotationisavailable,we‚Äôll\ncalculatetheagreementbetweenjudgementbyQwenandbytheclosed-sourceLLM-Judgeinthe\noriginalpaper.\nForbinarylabels(TTCW,CreativeMath),agreementreferstoLLM-Judgepredictionaccuracy(with\nhumanannotationsbeinggroundtruth);forlikert-scalelabels(AUT,CS4,TTCT),agreementrefers\ntoPearsoncorrelationbetweenhumanandLLM-Judge;theremainingtask,NeoCoder,isaspecial\ncasebecausetheLLM-Judgeisnotdirectlyusedtogeneratethemetricandwewillprovidemore\ndetailbelow. Herewereportthecorrelationstatistics:\nTTCW:Theoriginalauthors[8]provide36machine-generatedstorieswith3creative-writingexpert\nannotations(binary)foreach. WecheckthequalitybycalculatingagreementoftheQwen2.5-72B\njudgement(binary)andexpertmajorityvoteresults(alsobinary). Herearethemetrics(numbers\nin parentheses are accuracy): Narrative Ending (0.69), Understandability and Coherence (0.78),\nEmotionalFlexibility(0.86),WorldBuildingandSetting(0.72). Notethat,becauseinstorycreativity\nevaluation, the human-to-human agreement is also relatively low, we have a lower acceptance\nthresholdwhenweareconsideringwhatkindofhuman-LLMagreementlevelisacceptable.",
    "char_length": 1425
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 45,
    "text": "Creative Math: we annotated the output solution of 50 questions and observed 0.78 agreement\ninnoveltyjudgement(fromamulti-LLMmajorityvotingjudgethatconsistsofGemini-2.0-Flash,\nGPT-4.1,andClaude-3.7-Sonnet)and0.94agreementforcorrectness(fromtheClaude-3.7-Sonnet\nforcorrectnessjudgement). Agreementhereisthesameasaccuracyifweconsiderhumanannotation\nasgroundtruth.\nAUT:Previouswork[48]specificallystudiedthefeasibilityofusingfew-shotLLM-Judgetoevaluate\nAUT output. They have shown GPT-4 (it was published before GPT-4o) with 20-shot examples\ncanachieve0.70Pearsoncorrelationbetweenhumans‚Äôjudgement. Weusethesamepromptand\nthe human annotation released by that paper as the 20-shot examples. We then use GPT-4o and\nQwen2.5-72BtojudgethesamesetofAUToutputs(generatedbyLlama3.3-70B).Thescoresfrom\nGPT-4oandQwen2.5-72BhaveaPearson‚Äôsrof0.597(p<0.01).\nCS4: Theoriginalauthors[3]collectedandprovidedtheannotation(fromAmazonMechanicalTurk)\nof15machine-generatedstories,with2annotationsperstory. Forconstraintsatisfactionmetric,the\nPearsoncorrelationbetweenQwen2.5-72Bjudgmentsandhumanjudgmentsis0.55(p<0.01).\nTTCT:Theoriginalpaper[65]usedGPT-4asevaluatorandconductedasmallhumanv.s.LLM-Judge\nalignmentstudy. However,thatdataisnotaccessibletous,sowestudiedthePearsonCorrelation\nbetweenQwen2.5-72BandGPT-4.1judgementandherearethenumberswegot: Fluency: 0.6884\n(p<0.01),Flexibility: 0.6592(p<0.01),Originality: 0.5152(p<0.01),Elaboration: 0.5033(p<0.01)",
    "char_length": 1437
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 46,
    "text": "NeoCoder: LLM-Judgeisnotdirectlymakinganevaluationoutputinthistask. Instead, GPT-4\nisusedtodetectifageneratedsolutionisnovelornotwhencomparedtoapre-collectedhuman\nprogrammer solution. To ensure high quality of LLM-Judge, the original paper [42] uses recall\ninsteadofaccuracytoevaluatehuman-LLMagreement(becausewewanttoensureallthe‚Äúnon-novel‚Äù\ntechniquesaredetected). GPT-4achieves0.94recallindetectingnon-novelsolutions.\n24\nE TaskDetails\nE.1 TorranceTestofCreativeWriting(TTCW)\nE.1.1 Dataset\nThedatasetconsistsof12NewYorkerStories‚Äôplots,i.e.,GPT-4generatedsummaryoftheoriginal\nstory6.\nE.1.2 Example\nPlot\nA woman experiences a disorienting night in a maternity ward where she encounters other\nsimilarlydisorientednewmothers,leadingtoanuncannymix-upwheresheleavesthehospital\nwithababythatsherealizesisnotherown,yetacceptsthesituationwithaninexplicablesense\nofhappiness.\nInferencePrompt\nWriteaNewYorker-stylestorygiventheplotbelow.Makesureitisatleast{word_count}words.\nDirectlystartwiththestory,donotsaythingslike\"Here‚Äôsthestory[...]\"Plot: {plot}Story:\nE.1.3 ExperimentConfigurations\n‚Ä¢ Temperature: 0.75\n‚Ä¢ MaxToken: 4096\n‚Ä¢ Top-p: 1\nE.1.4 EvaluationMetrics\nAsmentionedin¬ß3,weuseasubsetofquestionsfromtheoriginalpaperwhereourfew-shotLLM-\nas-a-judgeevaluatorachievesacorrelationofmorethan0.2betweenthehumanmajorityvoteand\ntheevaluatormodel‚Äôsjudgments(NarrativeEnding: 0.29;UnderstandabilityandCoherence: 0.45;\nEmotionalFlexibility: 0.21;WorldBuildingandSetting: 0.40). Thesamplesizeis36,whichare",
    "char_length": 1489
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 47,
    "text": "GPT4, GPT3.5, and Claude-generated stories, with human expert annotation, all released by the\noriginalpaper. Eachquestioncorrespondstoonemetric. Sincetheevaluationisbinaryforeach\ngeneratedstory,wecalculatetheproportionofgeneratedstoriesthatpasseachquestionasthefinal\nevaluationmetric(e.g.,if3outof12storiespassthe‚ÄúUnderstandabilityandCoherence‚Äùquestion,\nthenthe‚ÄúUnderstandabilityandCoherence‚Äùmetricis0.25).\nWeusetwo-shotexamples(onepositiveandonenegative)intheevaluationprompt,aspreviouswork\nshowsaddingfew-shotexamplesimproveshuman-llmalignments[34].\n6https://github.com/salesforce/creativity_eval\n25\nEvaluationPrompt\nYouaregivenacreativeshortstory. Readitcarefully. Youarethengivensomebackground\nabout specific aspects of creative writing, a binary (Yes/No) question, and sample stories\nwithexpert-annotatedanswerstothesamequestion. Yourobjectiveistousethebackground\ninformationandsamplestoriestoanswerthequestionaboutthestory. Provideyouranswerin\ntheformatof\"**Answer**: [Yes/No]\". Youcanoptionallythenprovideashortexplanationfor\nyouranswer.\n==========\nQuestion:\n{full_prompt}\nExamples:\n==========\nStory: {story}\nAnswer: {answer}\nExplanations: {exp}\n==========\nStory: {story}\nAnswer: {answer}\nExplanations: {exp}\n==========\nStory: {story}\nBasedonthequestionandexamplesabove,answerthequestion(Provideyouranswerinthe\nformatof\"**Answer**: [Yes/No]\". Youcanoptionallythenprovideashortexplanationfor\nyouranswer). Makesureyouareextraharshonthedecision(mostanswersshouldbenegative).\nAnswer:",
    "char_length": 1487
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 48,
    "text": "E.1.5 ModelPerformances\nModel Narrative Understandability Emotional WorldBuilding\nEnding andCoherence Flexibility andSetting\nMistral-7B 0.17 0.08 0.00 0.00\nQwen2.5-7B 0.08 0.17 0.00 0.00\nOLMo2-7B 0.75 0.25 0.25 0.00\nLlama3.1-8B 0.00 0.08 0.00 0.00\nOLMo2-13B 0.67 0.33 0.25 0.08\nMistral-24B 0.25 0.25 0.00 0.00\nQwen2.5-32B 0.00 0.17 0.00 0.00\nMixtral-8x7B 0.17 0.08 0.08 0.08\nLlama3.3-70B 0.00 0.33 0.00 0.00\nQwen2.5-72B 0.42 0.50 0.50 0.17\nClaude3-Sonnet 0.75 0.58 0.58 0.42\nClaude3-Opus 0.33 0.08 0.08 0.08\nGPT-4.1 1.00 0.67 0.83 0.50\nGPT-4.1-mini 1.00 0.83 0.42 0.50\nGemini2.0-Flash 0.83 0.42 0.42 0.17\nDeepSeek-R1 0.83 0.50 0.50 0.58\nDeepSeek-V3 0.83 0.50 0.50 0.50\nTable6: ModelperformanceonTTCW.\n26\nE.2 CS4\nE.2.1 Dataset\nThereare50basestories. Duringinferencetime,foreachbasestory,5constraintsareapplied;to\ncalculatedn-gramdiversity,wealsogenerate3foreachconstraints. Therefore,foreachgivenLLM,\n750storieswillbegenerated7.\nE.2.2 Examples\nBaseStory\nEvelyn was introverted by nature, more comfortable in her world of books than in social\ngroups. Whatmadeherexceptional,however,washerlatent,uncontrollableintuition. Shewas\nhighlyempathic,experiencingothers‚Äôemotionsasvividlyastheydidthemselves-sometimes,\nevenmoreso. Theholidayseason,withitsswirlofemotionalundercurrents,wasparticularly\noverwhelmingforEvelyn,andshedreadediteveryyear.\nItwastheeveofChristmas. Evelyn‚Äôssmallapartmentwasburstingattheseamswithrelatives",
    "char_length": 1418
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 49,
    "text": "she hadn‚Äôt seen in years. Overwhelmed, she nestled into the corner of the couch, trying to\nmakeherselfinvisibleamidtheseaofforcedcheerfulness. Anycloseinteractionwasasensory\noverloadforher;shewouldinvoluntarilyfeeltheirfeelings,readtheirdeepestthoughts,and\nbecomeemotionallyexhausted.\nSuddenly,thedoorswungopen,andhercousin,Ava,strodein,atorrentofunspokenemotions\nswirlingaroundher. AshiverofapprehensionranthroughEvelynasshebracedherself,feeling\nthechurningstormofemotionsthroughherintuitiveempathy.\nAvalookedaroundtheroom,hereyeswideningslightlyasshenoticedEvelyn. EvelynfeltAva‚Äôs\nshockwaveofanxietyandself-consciousnesshitherlikeapunch. Despitebeingathermost\nvulnerable,Avahiditwellunderaveneerofsocialamiability. ButEvelyn‚Äôsperceptiveintuition\nsawrightthroughit.\nTakingadeepbreathtosteelherself,shewanderedovertoAva. Silentconversations,almost\npiercingintheirintensity,flowedbetweenthem.\n\"\"Areyouokay,Ava?\"\"Sheaskedfinally,breakingthemomentarysilence.\nAva‚Äôseyeswidenedinsurprise. \"\"Yeah,I‚Äômfine. Whydoyouask?\"\"Buttheunderlyingtremor\ninhervoicebetrayedtheturmoilwithin.\nEvelynquietlyadmitted,\"\"Icantellsomething‚Äôswrong.\"\"\nAvalookedathersilentlyforalongmomentbeforesighingdeeply. \"\"Ilostmyjob,Evelyn,\"\"\nsheconfessed,hervoicebarelyaboveawhisper. Evelynfeltatidalwaveofdespaircrashover\nherasAva‚Äôsfeelingsofhopelessnesswashedoverher.\nEvelyn, despite her own burden of emotions, took Ava‚Äôs hands in hers, feeling the tremors",
    "char_length": 1423
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 50,
    "text": "runningthroughthem. \"\"You‚Äôrenotalone,Ava. Youhaveus. We‚Äôllsortitouttogether,\"\"she\nsaid,hervoicereassuring.\nArmedwithherintuitiveempathy,EvelynspenttherestoftheeveningcomfortingAva,helping\nhercopewiththerawwoundofjobloss. Itwasanextremelychallenging,emotionallydraining\njourney,butEvelyn‚ÄôsheartswelledatAva‚Äôsgradualshiftfromdespairtoaglimmerofhopeand\noptimism.\nThat Christmas Eve, Evelyn, buoyed by Ava‚Äôs resilience, also discovered something about\nherself. Hergift,whichshehaddespisedforitsuncontrollability,forhowmuchitdrainedand\noverwhelmedher,couldalsobeusedtohelpothers.\nAsEvelynwatchedAvaslowlyblendbackintothecrowd,herheartlighter,theusualcacophony\nofemotionsshefearedseemedmorebearable. Evelynrealizedthatwhileherintrovertednature\nandintuitiveempathymadetheholidayseasonchallenging,itwasalsowhatmadeheressential\ninprocessingtheseunspokenstruggles. Itwasn‚Äôtacurse;itwashergift. Agiftofunderstanding,\nofempathy,ofbeingthesilentpillarofcomfortinaroomfilledwithconcealedemotions.\n7https://github.com/anirudhlakkaraju/cs4_benchmark\n27\nConstraints\n1. Theprotagonistsuffersphysicaldiscomfortwhenoverwhelmedbyemotions(nausea,shaking,\netc.).\n2. Theprotagonistischallengedbytheneedtoengageinpublicspaces.\n3. Theunknownmanrealizesthattheprotagonistcanfeelhisemotions.\n4. Theprotagonistuseshumorandsarcasmtocopewithhersituation.\n5. Theprotagonistisanintrovertedcharacter.\n6. Thestoryincludescommunicationviatextmessages.\n7. ThestoryissetinaStarbucksonMichiganinChicagoaweekbeforeChristmas.",
    "char_length": 1485
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 51,
    "text": "8. Theprotagonistisforcedtoleavethemeetingearlyduetobeingoverwhelmed.\n9. Theprotagonistdesirestoliveamorenormallifedespiteheruniquecondition.\n10. Thereexistsavaccineforcontrollingintuition.\n11. Theprotagonistdevisescopingstrategiesformanagingheranxietyinpublicplaces. 12.\nScientistsareworkingtofindasolutionforpeoplewhocan‚Äôtusetheintuitionvaccine.\n13. Tiffanythreatenstheprotagonisttomeether.\n14. Theprotagoniststruggleswithacceptinghercondition.\n15. TheprotagonistmustgrapplewiththethoughtsandfeelingsofothersintheStarbucks.\n16. Themanlooksattheprotagonistwithbothdesireandlove.\n17. Theprotagonistisphysicallyattractive.\n18. Theprotagonistencountersanunknownmanwhocausespowerfulanduniqueemotions.\n19. Thereissocietaldisapprovalforpeoplewhoseintuitionscannotbecontrolledbythevaccine.\n20. Theprotagonisthasaheightenedintuition.\n21. Thesettingshouldbeduringtheholidayseason.\n22. Theprotagonistfeelsotherpeople‚Äôsemotionsintensely.\n23. Theprotagoniststruggleswithdisentanglingtheirownfeelingsfromothers‚Äô.\n24. Tiffanyisastrong-willedandpassionatecharacter.\n25. Theprotagonistreluctantlyacknowledgesbeingapotential\"\"crazycatlady\"\".\n26. Pubertyisidentifiedasacriticaltimefortheprogressionofintuitionpowers.\n27. TheprotagonistandTiffanywereinseparableuntilpuberty.\n28. Theprotagonisthaspersonalhygieneitems(travelmouthwash)handy.\n29. Thisvaccinedoesn‚Äö√Ñ√¥tworkfortheprotagonistduetoageneticmutation.\n30. Charactersshouldexpressunderstandingoftheprotagonist‚Äôspredicament.",
    "char_length": 1461
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 52,
    "text": "31. Theprotagonist‚Äôsprimarymeansofcommunicationwiththeoutsideworldisthroughthe\ninternet.\n32. Theprotagonistexperiencesother‚Äôsthoughtsasiftheyweretheirown.\n33. Theprotagonist‚Äö√Ñ√¥sintuitionisuncontrollableduetoageneticmutation.\n34. Theprotagonist‚Äôscopingmechanismsdonotalwayssuccessfullyblockoutotherpeople‚Äôs\nemotions.\n35. Theprotagonistusesstrategiestoblockoutthefeelingsofothers,suchascountingletterson\nthemenuboard.\n36. Theprotagonistfindssolaceintheideaofdrinkingcoffee.\n37. Includeahintofromanceinthestory.\n38. Thereissocietalpressuretocontrolintuitionwiththevaccine.\n39. Theprotagonistprefersisolationtomanagetheirheightenedintuition.\"\n28\nE.2.3 ExperimentConfigurations\n‚Ä¢ Temperature: 0.75\n‚Ä¢ MaxToken: 4096\n‚Ä¢ Top-p: 1\nE.2.4 InferencePrompt\nInferencePrompt\nUser: Writeastoryinlessthan500wordsabout{storytheme}\nBaseStory: {basestory}\nUserInstruction: \"Nowmodifytheexistingstorytoaccommodatethefollowingconstraints:\n{selectedconstraints}intotheLLMgeneratedstoryandcomeupwithanewstoryin500words.\nE.2.5 EvaluationMetricsandPrompt\nFrom [3], we included QUC@39 (quality), RCS-7-39 (novelty) and Dist-N@39 (diversity) as\nevaluationmetrics. Inparticular,sincelowerRCSmeansmorestable,wetakeRCS =1‚àíRCS\nneg\nastheevaluationmetrictoensureallmetricsweincludeddescribeafeaturethatispositivelycorrelated\nwithcreativity.\nHereweincludeevaluationpromptforconstraintsatisfactionandstoryquality(coherence)evaluation\nprompt.\nEvaluationPrompt-ConstraintSatisfaction",
    "char_length": 1445
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 53,
    "text": "Youareanexpertreader. Iwillgiveyouastoryfollowedbyasetofconstraints. Yourtaskisto\ncarefullyreadbothofthemandtellhowmanyconstraintsarebeingsatisfiedinthestory. Asthe\noutput,Iwantyoutoprintyes/noforeachconstraintbasedonwhetheritisbeingsatisfiedornot,\nfollowedbya1lineexplanationofwhyitisbeingsatisfied/violated. Incaseaconstraintisbeing\nsatisfied,printthesentence/linefromthestoryinwhichitisbeingsatisfied. Ifaconstraintisnot\nbeingsatisfied,giveanexplanationofhowitisbeingviolated. Beverystrictinyourevaluation.\nMarkaconstraintassatisfied(\"yes\")onlyifitisbeingcompletelysatisfiedinthestory. For\nnosatisfaction/partialsatisfaction,marka\"no\". Ifthestoryisempty(noinputprovided),all\nconstraintsareconsideredNOTsatisfied. Finally,printthenumberofconstraintsthatarebeing\nsatisfied. Follow the examples and Output the ending of the evaluation in the same format.\nNumberofconstraintssatisfied: [number]\nHere are some examples - Input Story: {story} Constraints: 1. Write a story based on the\nfollowingconstraintsinlessthan377words.\n2. Startthestorywiththesentence: \"Week18aboardtheDepthReaver,Circa2023\"\n3. Includearevelationofanunexpectedlarge-scalephenomenonobservedinspace.\"\nOutput1. Yes-Thestoryis302wordslong,meetingtheconstraintofbeinglessthan377words.\n2. Yes-Thestorystartswiththeexactsentence: \"Week18aboardtheDepthReaver, Circa\n2023\".\n3. Yes-Therevelationofthemooncrackingopentorevealacolossalhumanfacequalifiesasan\nunexpectedlarge-scalephenomenonobservedinspace.\nNumberofconstraintssatisfied: 3",
    "char_length": 1496
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 54,
    "text": "{otherexamples}\nInputStory: {storytobeevaluated}Constraints: {constraints}Output\n29\nEvaluationPrompt-StoryQuality\nYou are an English writing expert and you can compare and evaluate story essays on these\nmetricswiththefollowingdefinitions\n1. Grammar: Whichstoryhasbetterwritingandgrammarcomparitively?\n2. Coherence: Whichstoryhasabetterlogicalflowandthewritingfitstogetherwithrespectto\ntheplot?\n3. Likability: Whichstorydoyoufindmoreenjoyabletoread?\nYouwillbegiventwoStories-StoryAandStoryB.\nAdd a rating out of 5 for each category, specify which story you prefer for each metric by\nrespondingwithjusttheletter\"A\"or\"B\"followedbyahyphenandonelinereasoningforyour\npreference.\nForeachcategoryprovideacategorywinnerstoryastheletter\"A\"or\"B\",basedonthecategory\nratings.\nFinally,assignanoverallwinnerstoryastheletter\"A\"or\"B\"basedontheratingsandcategory\nwins.\n(ifanstoryisempty,giveitzeroscores)\nIMPORTANT-DONOTGIVEANYOTHERTEXTAPARTFROMTHESCORE,METRICS\nANDPREFERENCE.FOLLOWTHEEXACTFORMATASGIVENINTHEFOLLOWING\nEXAMPLES.\nEXAMPLEOUTPUT1:\n{exampleoutput1}\nEXAMPLEOUTPUT2:\n{exampleoutput2}\nStoryA:{story1}\nStoryB:{story2}\nSCOREOUTPUT:\nNotethattheoriginalpaperusesOpenAImodelsasevaluatorforbothconstraintsatisfactionandstory\ncoherenceevaluation. Wealsoinvestigateopen-sourcealternatives,Qwen2.5-72B,astheLLM-judge.\nThePearsoncorrelationbetweenQwen2.5-72Bjudgmentsandhumanjudgmentis0.55,withp-value\n<0.01(samplesize: 15stories,2annotationsperstory).\nE.2.6 ModelPerformances\n30\nModel QUC@39 RCS-7-39(neg) Dist-N@39",
    "char_length": 1498
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 55,
    "text": "Mistral-7B 0.7048 0.8947 0.8675\nQwen2.5-7B 0.6451 0.9143 0.7480\nOLMo2-7B 0.5904 0.8491 0.9032\nLlama3.1-8B 0.6193 0.8103 0.7808\nOLMo2-13B 0.5543 0.8595 0.9200\nMistral-24B 0.7641 0.9417 0.7978\nQwen2.5-32B 0.7201 0.9064 0.8604\nMixtral-8x7B 0.5925 0.8313 0.8871\nLlama3.3-70B 0.7392 0.9343 0.7788\nQwen2.5-72B 0.7853 0.9499 0.8484\nClaude3-Sonnet 0.8153 0.9557 0.9572\nClaude3-Haiku 0.6306 0.8584 0.8728\nGPT4.1 0.8157 0.9587 0.9083\nGPT4.1-mini 0.7828 0.9339 0.8990\nGemini2.0-Flash 0.7733 0.9349 0.9275\nDeepSeek-R1 0.7767 0.9249 0.9022\nDeepSeek-V3 0.7809 0.9572 0.9121\nTable7: ModelscoresonCS4task.\nE.3 CreativityIndex\nE.3.1 Dataset\nThedatasetconsistsof3subsets: book,poem,andspeech,allaretheprefixes(i.e.,firstlineoftext)\nfromthedatasetproposedby [41]. Weusethefirst100examplesingenerationandevaluation. 8.\nE.3.2 Examples\nHerearesomeexamplesoftheinputdata(i.e.,thetextprefixforLLMtocomplete).\nBook\nIt‚Äôsbeenyears: Baileyclearlymeanshimnoharmandhasmanagedtobediscreetenoughthat\nNick‚Äôsqueernessisn‚ÄôtthetalkoftheChronicle.\nPoem\nSwiftlywalko‚Äôerthewesternwave,\nSpeech\nThatisthekindofAmericainwhichIbelieve\nE.3.3 EvaluationMetrics\nL-uniqueness Letxbeatextconsistingofasequenceofwordswhoselinguisticcreativitywewish\ntoquantify. Letann-gramofxbeanycontiguoussubsequenceofnwords,anddenotebyx\ni:i+n\nthen-gramstartingatthei-thwordofx. LetC bealargereferencecorpusofpubliclyavailable\ntexts,anddefinef asabinaryfunctionthatreturns1ifthen-gramx occursanywhereinC,\ni:i+n",
    "char_length": 1447
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 56,
    "text": "and 0 otherwise. The L-uniqueness of x, denoted by uniq(x,L), is defined as the proportion of\nwords w ‚àà x such that noneof the n-grams containing w with n ‚â• L occur in C. Intuitively, a\nhigherL-uniquenessmeansagreaterproportionofx‚Äôswordsappearinnovelcontextsnotseeninthe\nreferencecorpus,thusindicatinggreaterlinguisticoriginality.\n8https://github.com/GXimingLu/creativity_index\n31\nE.3.4 ExperimentConfigurations\n‚Ä¢ Temperature: 1\n‚Ä¢ TopP:0.9\n‚Ä¢ MaxToken: 288\nE.3.5 InferencePrompt\nBook\nPleasewriteafewparagraphsforanovelstartingwiththefollowingprompt: {startsentenceof\nbook}\nPoem\nPleasewriteafewparagraphsforanovelstartingwiththefollowingprompt: {startsentenceof\npoem}\nSpeech\nPleasewriteaspeechstartingwiththefollowingsentence: {startsentenceofspeech}\nForclosed-sourcemodels,weincludedanadditionalpromptinstructiontoensurethattheoutput\nconsistssolelyofthecompletedparagraphs,poems,orspeeches. Thesemodelsoftenprefacetheir\nresponseswithphraseslike‚ÄúCertainly‚Äùor‚ÄúHereis...,‚Äùwhichwemanuallyremovedduringpost-\nprocessing. In contrast, open-source models typically generate the desired completions directly\nwithoutsuchprefatorytext.Forthesemodels,wereviewedthelogoutputsandremovedanyunrelated\ncontentasneeded.\nE.3.6 EvaluationMetrics\nWefollowtheevaluationmetricsoutlinedinLuetal.[40],specificallyretainingtheexactmatch\ncomponent. However,weexcludethesemanticsearch-basedevaluationduetoitshighcomputational\ncostandsensitivitytothechosencosinesimilaritythreshold, whichsignificantlyaffectswhether",
    "char_length": 1486
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 57,
    "text": "twosentencespansareconsideredsemanticallysimilar. WesumovertheL-uniqunesswithspansof\nn-gramsfrom5to12inclusivelytogetthetotalcreativeindexforeachresponse. Weaveragethe\ncreativeindexforeachresponsepermodepertask. Datacleaningwasdonebeforetheevaluation\nmanuallytoremoveirrelevantoutputs.Then,wenormalizethescorebydividingitwith8(thehighest\nvaluethatthesummationcouldbe)togetthefinalCreativityIndexmeasurementforeachmodelover\nthethreedifferenttasks.\nE.3.7 ModelPerformance\nE.3.8 AdditionalComments\nWealsonotethatthegenerationforOLMo2-13B-instructmaymisssomedatawiththevllmgenera-\ntion. Weremovethosemissinggenerations. Thisaccountfor13responsesinthepoemsubset,and\n10examplesinthespeechsubset. Inaddition,themodelmayresistsinansweringsomeprompts. We\nalsoremovedthosegenerations. ForOLMo-7B-instruct,thereare2casesinthespeechsubset. For\nGPT-4.1,thereis1caseinthespeechsubset.\n32\nModel Book Poem Speech Average\nmistral-7b-instruct 0.4496 0.5828 0.3104 0.4476\nqwen-7b-instruct 0.4354 0.6310 0.3534 0.4733\nolmo-7b-instruct 0.4810 0.6110 0.3727 0.4882\nllama-31-8b-instruct 0.4724 0.5700 0.3396 0.4607\nolmo2-13b-instruct 0.4860 0.5963 0.3522 0.4782\nmistral-24b-instruct 0.4752 0.6646 0.3397 0.4932\nqwen-32b-instruct 0.4663 0.6328 0.3465 0.4816\nmistral-8x7b-instruct 0.4149 0.6035 0.2804 0.4329\nllama-33-70b-instruct 0.4226 0.5802 0.2936 0.4321\nQwen2.5-72B-instruct 0.4133 0.5924 0.3171 0.4409\nclaude-3-7-sonnet-20250219 0.5615 0.6700 0.4675 0.5663\nclaude-3-5-haiku-20241022 0.5769 0.7039 0.4519 0.5776",
    "char_length": 1492
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 58,
    "text": "gpt-4.1 0.6044 0.7637 0.4593 0.6091\ngpt-4.1-mini 0.5624 0.7147 0.4261 0.5677\ngemini-2.0-flash 0.5278 0.6707 0.4121 0.5369\ndeepseek-reasoner 0.5930 0.7595 0.5410 0.6312\ndeepseek-chat 0.6814 0.7791 0.6166 0.6924\nTable8: L-uniquenessacrossBook,Poem,Speech,andaveragedperformancefordifferentmodels;\nweuseaverageastheL-uniquenessscoreinCREATIVITYPRISMasthemetricforCreativityIndex;\nboldnumbersarebestperformers.\nE.4 CreativeShortStory\nE.4.1 Dataset\nThedatasetconsistsof10three-wordstuples. ForananygivenLLM,itispromptedtogeneratea\nshortstory(atmostfivesentences)basedonthosethreewords9.\nE.4.2 Examples\nThree-wordTuple\nstamp,letter,send\nE.4.3 ExperimentConfigurations\n‚Ä¢ Temperature: 0.75\n‚Ä¢ MaxToken: 4096\n‚Ä¢ Top-p: 1\nE.4.4 InferencePrompt\nInferencePrompt\nYouwillbegiventhreewords(e.g.,car,wheel,drive)andthenaskedtowriteacreativeshort\nstorythatcontainsthesethreewords. Theideaisthatinsteadofwritingastandardstory,suchas\n\"Iwentforadriveinmycarwithmyhandsonthesteeringwheel.\",youneedtocomeupwitha\nnovelanduniquestorythatusestherequiredwordsinunconventionalwaysorsettings. Also\nmakesureyouuseatmostfivesentences. Thegiventhreewords: {items}(thestoryshouldnot\nbeabout{boring_theme}).\n9https://github.com/mismayil/creative-story-gen\n33\nE.4.5 EvaluationMetrics\nWe included novelty score, surprise-ness, and average N-gram Diversity from the original paper.\nParticularly,becausen-gramdiversityisalmostalways1forngreaterthan3(mainlybecausethe",
    "char_length": 1428
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 59,
    "text": "storiesareatmostfivesentenceslong),wekeeponlyunigramandbigram(i.e.,weusetheaverageof\nunigramdiversityandbigramdiversityastheN-gramdiversity).\nE.4.6 ModelPerformance\nModel Surprisal N-gramDiversity\nMistral-7B 0.0889 0.810\nQwen2.5-7B 0.0834 0.220\nOLMo2-7B 0.0599 0.895\nLlama3.1-8B 0.0490 0.410\nOLMo2-13B 0.2043 0.905\nMistral-24B 0.1406 0.820\nQwen2.5-32B 0.1263 0.870\nMixtral-8x7B 0.0601 0.715\nLlama3.3-70B 0.0590 0.545\nQwen2.5-72B 0.1234 0.860\nClaude3-Sonnet 0.0927 0.860\nClaude3-Haiku 0.1235 0.870\nGPT4.1 0.0928 0.870\nGPT4.1-mini 0.0965 0.870\nGemini2.0-Flash 0.0375 0.865\nDeepSeek-R1 0.1953 0.905\nDeepSeek-V3 0.2613 0.900\nTable9: PerformanceontheCreativeShorttask,includingsurprise-ness,averagen-gramdiversity,\nandnovelty.\nE.4.7 DiscussiononLowCorrelationwithotherMetrics\nAsshowninFig. ??,itisnotablethatmodelperformancesonthistaskhaveverylowcorrelation\nwithothertasks,eveninthesamecreativitydimension. Here,weprovideadiscussionbasedonthe\ntaskdesign:\nForthetaskformat,thestoryislimitedtoatmostfivesentences. Forevaluationmetrics,thenovelty\nmetric(C_Short_Nov)measuresthedifferencebetweenwordlevelaveragepairwisedistancesofa\ngivenstoryandthatofallstoriesgeneratedbythesamestory,whichmeansitismeasuringnovelty\ncomparedtothemodelitself,similartotheideaofP-creative(‚Äúcreativetotheindividualwhocomes\nupwithit‚Äù)anditisslightlydifferentfromothernoveltymetrics,whichtriestocaptureH-creative\n(‚Äúideasthathaveneverbeenconceivedinhumanhistorybefore‚Äù). Theothermetricfromthistask",
    "char_length": 1466
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 60,
    "text": "(C_Short_Sur)measuresthesurprisal,asdefinedbyaveragesentenceembeddingdistanceforall\nconsecutivesentencepairsingeneratedstories,whichmeansitismeasuringnoveltynotonthestory\ncontent,butonthenoveltyofthetwist-and-turnofstories.\n34\nE.5 NeoCoder\nModel ConvergentCreativity DivergentCreativity\nMistral-7B 0.0000 1.0000\nQwen2.5-7B 0.0000 0.9158\nOLMo-2-7B 0.0000 0.5773\nLlama-3.1-8B 0.0000 0.9845\nOLMo-2-13B 0.0000 0.4433\nMistral-24B 0.0000 0.9897\nQwen2.5-32B 0.0000 0.3402\nMixtral-8x7B 0.0000 0.9897\nLlama-3.3-70B 0.0000 1.0000\nQwen2.5-72B 0.0000 0.7938\nClaude3-Sonnet 0.0000 0.732\nClaude3-Haiku 0.0105 0.9947\nGPT4.1 0.0000 1.0000\nGPT4.1-mini 0.0000 0.9948\nGemini2.0-Flash 0.0103 1.0000\nDeepseek-R1 0.0000 0.732\nDeepseek-V3 0.0103 1.0000\nTable10: BenchmarkingresultsonNeoCoder[42]atstate5(i.e.,with5constraints);boldnumbers\narebestperformers.\nE.5.1 Examples\nWeusethesamedatasetfromtheoriginalNeoCoderpaper10. SeeTable11forexamples.\nE.5.2 EvaluationMetrics\nConvergence Score The NeoGauge metric (accompanied by the NeoCoder dataset) evaluates\nconvergentcreativitybycheckingwhetherthegeneratedcodesolutionssuccessfullypassalltest\ncasesandadheretothegivenconstraints.\nDivergentScore TheNeoGaugemetric(accompaniedbytheNeoCoderdataset)evaluatesdivergent\ncreativitybycomparingLLM-generatedsolutionstohistoricalhumansolutionsatthetechniquelevel.\nSpecifically,itquantifiestheproportionofnoveltechniquesemployedbythemodeltosolveagiven\nproblemthatanyhumanhasnotpreviouslyused.\nE.5.3 ExperimentConfigurations",
    "char_length": 1490
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 61,
    "text": "We follow the experimental settings from the original NeoCoder [42], including the technique\ndetectionmodelchoice. Toensureafaircomparison,wemodifyonlythesamplinghyperparameters\nofthetargetmodel(e.g.,temperature,top-p,andmaximumtokens)toourunifiedsettings.\nE.5.4 ModelPerformance\nSeeTable10formodelperformances.\n10https://github.com/JHU-CLSP/NeoCoder/\n35\nState Constraint ProblemStatement\nB.PointsandMinimumDistance\nYouaregivenasequenceofintegersaoflength2n.Youhavetosplitthese2nintegers\n0 N/A intonpairs;eachpairwillrepresentthecoordinatesofapointonaplane.Eachnumber\nfromthesequenceashouldbecomethexorycoordinateofexactlyonepoint.Notethat\nsomepointscanbeequal¬∑¬∑¬∑\nB.PointsandMinimumDistance\nProgrammingconstraints:DONOTusethefollowingtechniques\n-forloop\n1 forloop\nYouaregivenasequenceofintegersaoflength2n.Youhavetosplitthese2nintegers\nintonpairs;eachpairwillrepresentthecoordinatesofapointonaplane.Eachnumber\nfromthesequenceashouldbecomethexorycoordinateofexactlyonepoint.Notethat\nsomepointscanbeequal¬∑¬∑¬∑\nB.PointsandMinimumDistance\nProgrammingconstraints:DONOTusethefollowingtechniques\n-ifstatement\nforloop\n2 -forloop\nifstatement\nYouaregivenasequenceofintegersaoflength2n.Youhavetosplitthese2nintegers\nintonpairs;eachpairwillrepresentthecoordinatesofapointonaplane.Eachnumber\nfromthesequenceashouldbecomethexorycoordinateofexactlyonepoint.Notethat\nsomepointscanbeequal¬∑¬∑¬∑\nB.PointsandMinimumDistance\nProgrammingconstraints:DONOTusethefollowingtechniques\n-whileloop\nforloop\n-ifstatement\n3 ifstatement",
    "char_length": 1500
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 62,
    "text": "-forloop\nwhileloop\nYouaregivenasequenceofintegersaoflength2n.Youhavetosplitthese2nintegers\nintonpairs;eachpairwillrepresentthecoordinatesofapointonaplane.Eachnumber\nfromthesequenceashouldbecomethexorycoordinateofexactlyonepoint.Notethat\nsomepointscanbeequal¬∑¬∑¬∑\nB.PointsandMinimumDistance\nProgrammingconstraints:DONOTusethefollowingtechniques\n-sorting\nforloop\n-whileloop\nifstatement\n4 -ifstatement\nwhileloop\n-forloop\nsorting\nYouaregivenasequenceofintegersaoflength2n.Youhavetosplitthese2nintegers\nintonpairs;eachpairwillrepresentthecoordinatesofapointonaplane.Eachnumber\nfromthesequenceashouldbecomethexorycoordinateofexactlyonepoint.Notethat\nsomepointscanbeequal¬∑¬∑¬∑\nB.PointsandMinimumDistance\nProgrammingconstraints:DONOTusethefollowingtechniques\n-tuple\nforloop\n-sorting\nifstatement\n-whileloop\n5 whileloop\n-ifstatement\nsorting\n-forloop\ntuple\nYouaregivenasequenceofintegersaoflength2n.Youhavetosplitthese2nintegers\nintonpairs;eachpairwillrepresentthecoordinatesofapointonaplane.Eachnumber\nfromthesequenceashouldbecomethexorycoordinateofexactlyonepoint.Notethat\nsomepointscanbeequal¬∑¬∑¬∑\nTable11: AnexampleofNeoCoderdatasetwithproblemID1895Bandstatet=5.\n36\nE.6 CreativeMath\nE.6.1 Dataset\nThedatasetconsistsof400mathcompetitionquestionsfromthepaper[62], withcorresponding\ngroundtruthsolutionsextractedfromArtofProblemSolving. Wemanuallyfilteredout27questions\nfromtheIMOcompetitionthatareproof-basedtomaketheLLMjudgmentmorereliable11.\nE.6.2 Example\n2016AMC8,Problem15\nQuestion:",
    "char_length": 1472
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 63,
    "text": "Whatisthelargestpowerof2thatisadivisorof134‚àí114?\n(A)8 (B)16 (C)32 (D)64 (E)128\nReferneceSolutions1:\nWeusetheidentityforthedifferenceofsquares:\n134‚àí114 =(132)2‚àí(112)2 =(132+112)(132‚àí112)\nCalculateeachpart:\n132 =169, 112 =121132+112 =169+121=290, 132‚àí112 =169‚àí121=48\nWefactor48further:\n134‚àí114 =290¬∑48=290¬∑(16¬∑3)=(2¬∑145)¬∑(24¬∑3)\nThepowersof2inthisexpressionare21from290and24from48:\n21¬∑24 =25 = (C)32\nReferneceSolutions2:\nWeapplythedifference-of-squaresidentityonce:\n134‚àí114 =(132‚àí112)(132+112)\nWecompute:\n132 =169, 112 =121‚áí132‚àí112 =48, 132+112 =290\nFactoreach:\n290=2¬∑145 (145isodd)48=24¬∑3\nSo,thehighestpowerof2dividing134‚àí114 =48¬∑290is:\n24¬∑21 =25 = (C)32\n11https://github.com/JunyiYe/CreativeMath\n37\nNovelSolutionGeneratedbyGPT4.1\nLet‚Äôsdirectlyexpand134‚àí114usingthebinomialtheoremandanalyzetheresultfordivisibility\nbypowersof2.\nFirst,compute134and114:\n4 (cid:18) (cid:19)\n(cid:88) 4\n134 =(10+3)4 = 104‚àík3k\nk\nk=0\n=104+4¬∑103¬∑3+6¬∑102¬∑9+4¬∑10¬∑27+81\n=10000+12000+5400+1080+81=28561\n4 (cid:18) (cid:19)\n(cid:88) 4\n114 =(10+1)4 = 104‚àík1k\nk\nk=0\n=104+4¬∑103¬∑1+6¬∑102¬∑1+4¬∑10¬∑1+1\n=10000+4000+600+40+1=14641\nNow,subtract:\n134‚àí114 =28561‚àí14641=13920\nNow,factor13920bydividingby2repeatedly:\n-13920√∑2=6960\n-6960√∑2=3480\n-3480√∑2=1740\n-1740√∑2=870\n-870√∑2=435(nowodd)\nSo,wedividedby2fivetimesbeforereachinganoddnumber. Thus,thelargestpowerof2\ndividing13920is25 =32.\n32\nNote,weprovidedthecleanedrepsonsehere.\nE.6.3 EvaluationMetrics\nCorrectnessRatio : Thecorrectnessratioisdefinedasthenumberofquestionsjudgedcorrectby",
    "char_length": 1492
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 64,
    "text": "Claude-3.7-Sonnetdividedbythetotalnumberofquestions. Notethatthetotalis574questions‚Äînot\n373‚Äîsinceeachquestionmaybepairedwithmultiplereferencesolutions.\n38\nNovelty Ratio : The coarse-grained novelty ratio or what we refer to the Novelty Ratio here\nmeasures whether the model‚Äôs generation differs from the provided reference solution over the\nquestionsthatareansweredcorrectly.\nE.6.4 ExperimentConfigurations\nWeusethedatasetreleasedinYeetal.[62],whichcontains400uniquemathquestionssourcedfrom\nvariousmathcompetitions. Allinferenceisconductedatzerotemperature,withamaximumtoken\nlimitof2000.\nE.6.5 InferencePrompt\nThepromptusedforinferenceisshownbelow. ItisadapteddirectlyfromYeetal.[62]:\nInferencePrompt\nCriteria for evaluating the difference between two mathematical solutions include: i). If\nthe methods used to arrive at the solutions are fundamentally different, such as algebraic\nmanipulationversusgeometricreasoning,theycanbeconsidereddistinct;\nii). Even if the final results are the same, if the intermediate steps or processes involved in\nreachingthosesolutionsvarysignificantly,thesolutionscanbeconsidereddifferent;\niii). Iftwosolutionsrelyondifferentassumptionsorconditions,theyarelikelytobedistinct;\niv). Asolutionmightgeneralizetoabroaderclassofproblems,whileanothersolutionmightbe\nspecifictocertainconditions. Insuchcases,theyareconsidereddistinct;\nv). Ifonesolutionissignificantlysimplerormorecomplexthantheother,theycanberegarded\nasessentiallydifferent,eveniftheyleadtothesameresult.",
    "char_length": 1496
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 65,
    "text": "Giventhefollowingmathematicalproblem: problem\nAndsometypicalsolutions: reference_solutions\nPleaseoutputonenovelsolutiondistinctfromthegivenonesforthismathproblem.\nE.6.6 EvaluationMetricsandPrompt\nOurevaluationconsistsoftwopartsanddiffersfromtheoriginalthree-phasesetupdescribedinYe\netal.[62].\nPart1: CorrectnessEvaluation. Beforeevaluation,weuseLlama-3.3-70B-Instructtoremove\ntransitional phrases and model-generated statements that justify the novelty of a solution. We\nmanuallyverified50examplesandfoundthatLlama‚Äôsdatacleaningperformancewasofhighquality.\nWeuseClaude-3.7-Sonnetasthesolecorrectnessevaluator. Whiletheoriginalpaperusedathree-\nmodelensemble(GPT-4,Gemini-1.5-Pro,Claude-3-Opus),wefoundClaudetobethemostreliable\nthroughmanualinspectionof50examplesevaluatedbyClaude-3.7-Sonnet,GPT-4.1,andGemini-\n2.o-Flash. Claudedemonstratedstrongattentiontodetailinproof-basedquestionsandconsistently\nidentifiederrorsfoundbytheothermodels,inadditiontodetectingflawsinthereasoningprocess.\nThetemperaturewassetto0.0andthemaximumtokenlimitwas128.\nPart2: NoveltyEvaluation. Theoriginalpaperconductedtwotypesofnoveltyevaluation: coarse-\ngrainedandfine-grained. Weonlyconductedcoarse-grainednoveltyevaluationfortwomainreasons.\nFirstly,theoriginalpapernotedthatifasolutionisconsideredcoarse-grainednovel,itisalsohighly\nlikelytobejudgedasanovelsolutioninthefine-grainedevaluation.Secondly,fine-grainedevaluation\nofnoveltyislessindicativeofamodel‚Äôsabilitytogeneratenovelsolutionsbecausethemodeldoes",
    "char_length": 1488
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 66,
    "text": "nothaveaccesstotheunseenreferencesolutionsinthefine-grainedevaluationphase. Thismeans\nthatthemodelmaygenerateaverysimilarsolutiontotheotherreferencesolutionsnotshowntoitor\nitmay,bychance,generateanewsolutionthatisentirelydifferentfromotherreferencesolutions\nnotshowntoit. Therefore,thisrandomnessmakesfine-grainedevaluationlessinterpretable. Even\nthoughthefine-grainedevaluationisstillvaluableinthatithelpstocheckifthemodelsaregenerating\nanewsolutionthathasnotbeenpubliclypostedbyhuman. Nevertheless,thisislesscompatible\nwithourevaluationpipelinesincewewanttotesthowmodelmaycomeupwithnewsolutionsgiven\nreferencesolutions,whichcaniseasiertobequantified.\n39\nIntermsofjudgeLLMs,wefollowtheoriginalpaperwithmajorityvotingbyClaude-3.7-Sonnet,\nGPT-4.1,andGemini-2.0-Flash.\nWeadoptthefollowingpromptforcorrectnessevaluation:\nCorrectnessEvaluationPrompt\nCriteriaforevaluatingthenoveltyofanewmathematicalsolutioninclude: 1. Ifthenewsolution\nused to arrive at the solutions is fundamentally different from reference solutions, such as\nalgebraicmanipulationversusgeometricreasoning,itcanbeconsiderednovel;\n2. Even if the final results are the same, if the intermediate steps or processes involved in\nreachingthosesolutionsvarysignificantly,thenewsolutioncanbeconsiderednovel;\n3. If the new solution relies on different assumptions or conditions, it should be considered\nnovel;\n4. Asolutionmightgeneralizetoabroaderclassofproblems,whileanothersolutionmightbe",
    "char_length": 1447
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 67,
    "text": "specifictocertainconditions. Insuchcases,theyareconsidereddistinct;\n5. Ifthenewsolutionissignificantlysimplerormorecomplexthantheothers,itcanberegarded\nasessentiallynovel,eveniftheyleadtothesameresult.\nGiventhefollowingmathematicalproblem: {problem}\nReferencesolutions: {reference_solutions}\nNewsolution: {new_solution}\nPleaseoutputYESifthenewsolutionisanovelsolution;otherwise,outputNO.Then,please\nprovideaverybriefreasonforyourevaluationbasedonthecriteriaabove.\"\"\nNote: Duringmanualevaluation,weallowthemodeltogenerateabriefexplanationforitsjudgment\nofcorrectnessorincorrectness. Forautomatedevaluation,weomitthefinalsentence: \"Then,please\nprovideaverybriefreasonforyourevaluationbasedonthecriteriaabove.\"\nWeadoptthefollowingpromptforcoarse-grainednoveltyevaluation:\nCoarse-grainedNoveltyEvaluationPrompt\nCriteriaforevaluatingthenoveltyofanewmathematicalsolutioninclude: 1. Ifthenewsolution\nused to arrive at the solutions is fundamentally different from reference solutions, such as\nalgebraicmanipulationversusgeometricreasoning,itcanbeconsiderednovel;\n2. Even if the final results are the same, if the intermediate steps or processes involved in\nreachingthosesolutionsvarysignificantly,thenewsolutioncanbeconsiderednovel;\n3. If the new solution relies on different assumptions or conditions, it should be considered\nnovel;\n4. Asolutionmightgeneralizetoabroaderclassofproblems,whileanothersolutionmightbe\nspecifictocertainconditions. Insuchcases,theyareconsidereddistinct;",
    "char_length": 1476
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 68,
    "text": "5. Ifthenewsolutionissignificantlysimplerormorecomplexthantheothers,itcanberegarded\nasessentiallynovel,eveniftheyleadtothesameresult.\nGiventhefollowingmathematicalproblem: {problem}\nReferencesolutions: {reference_solutions}\nNewsolution: {new_solution}\nPleaseoutputYESifthenewsolutionisanovelsolution;otherwise,outputNO.Then,please\nprovideaverybriefreasonforyourevaluationbasedonthecriteriaabove.\n40\nE.6.7 ModelPerformance\nModel Norm.Correctness Norm.Novelty Corr.(%) Nov.(%) N/C(%)\nMistral-7B-Instruct 0.2544 0.0296 25.44 2.96 11.64\nQwen2.5-7B 0.7875 0.1620 78.75 16.20 20.58\nOLMo-7B-Instruct 0.3711 0.0453 37.11 4.53 12.21\nLlama-31-8B-Instruct 0.5819 0.0610 58.19 6.10 10.48\nOLMo2-13B-Instruct 0.5087 0.1150 50.87 11.50 22.60\nMistral-24B-Instruct 0.6899 0.2143 68.99 21.43 31.06\nQwen2.5-32B 0.8972 0.2213 89.72 22.13 24.66\nMixtral-8x7B-Instruct 0.5697 0.1150 56.97 11.50 20.18\nLlama-33-70B-Instruct 0.8606 0.1777 86.06 17.77 20.65\nQwen2.5-72B 0.9042 0.4913 90.42 49.13 54.34\nClaude3-Sonnet 0.9408 0.7213 94.08 72.13 76.67\nClaude3-Haiku 0.8624 0.3031 86.24 30.31 35.15\nGPT-4.1 0.9443 0.7178 94.43 71.78 76.01\nGPT-4.1-mini 0.8972 0.6916 89.72 69.16 77.09\nGemini-2.0-Flash 0.8885 0.5714 88.85 57.14 64.31\nDeepSeek-Reasoner 0.9843 0.8031 98.43 80.31 81.59\nDeepSeek-Chat 0.9721 0.6603 97.21 66.03 67.92\nTable12: Modelperformanceacrossnormalizedcorrectnessandnoveltydimensions.\n41\nE.7 DivergentAssociationTask(DAT)\nNote: sinceDATtaskdoesnothaveanyinputdata,weonlyprovidethefollowinginfoandsome",
    "char_length": 1489
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 69,
    "text": "outputexamples.\nE.7.1 ExperimentConfigurations\nFollowingtheoriginalwork,wegenerate100samplespermodelandcomputetheirDATscoresas\ntheaveragepairwisecosinesimilarityamongthegeneratedwords. WeuseGLoVe[50]tocalculate\nsemanticdistance. Wesetthesamplinghyperparameterstothoseweusedinourmainexperimentfor\nafaircomparison.\nE.7.2 InferencePrompt\nInferencePrompt\nPleasewrite10nounsinEnglishthatareasirrelevantfromeachotheraspossible,inallmeanings\nandusesofthewords. Pleasenotethatthewordsyouwriteshouldhaveonlysingleword,only\nnouns(e.g.,things,objects,concepts),andnopropernouns(e.g.,nospecificpeopleorplaces).\nYouranswer:\nE.7.3 OutputExamples\nGPT-4.1\nCertainly! Hereare10single-word,commonnounsinEnglishthatareasunrelatedfromeach\nother as possible: 1. Volcano 2. Violin 3. Democracy 4. Carpet 5. Bacteria 6. Satellite 7.\nPatience8. Apron9. Oxygen10. Comet\nGemini2.5Flash\nOkay, here are 10 nouns as irrelevant from each other as possible: 1. **Quantum** 2.\n**Omelet**3. **Nostalgia**4. **Brick**5. **Algorithm**6. **Serenity**7. **Oxygen**\n8. **Sonnet**9. **Galaxy**10. **Bureaucracy**\nE.7.4 ModelPerformance\nModel DATScore\nMistral-7B 0.7908\nQwen2.5-7B 0.6907\nOLMo2-7B 0.8058\nLlama3.1-8B 0.8208\nOLMo2-13B 0.8133\nMistral-24B 0.6004\nQwen2.5-32B 0.6919\nMixtral-8x7B 0.8298\nLlama3.3-70B 0.6940\nQwen2.5-72B 0.7747\nClaude3-Sonnet 0.8975\nClaude3-Haiku 0.8740\nGPT4.1 0.8737\nGPT4.1-mini 0.8262\nGemini2.0-Flash 0.8868\nDeepSeek-R1 0.8274\nDeepSeek-V3 0.9052\nTable13: ModelperformancesforDATtask;boldresultisthebestperformer.",
    "char_length": 1501
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 70,
    "text": "42\nE.8 TorranceTestsofCreativeThinking(TTCT)\nE.8.1 Dataset\nThe dataset consists of 700 questions spanning 7 tasks (100 questions/task) that require creative\nanswers. ThesequestionsareGPT-4generatedusingfew-shotprompts12.\nE.8.2 Examples\nInferenceQuestions\nTask1: Unusualuses\nUnusualUsesTask. Youwillbepresentedwithacommonobject,andyourtaskistosuggestas\nmanyunusual,innovative,ornon-traditionalusesforeachobjectasyoucanthinkof. Pleaselist\nunusualusesofsock\nTask2: Consequences\nWhatmightbetheconsequencesifhumanssuddenlylosttheabilitytosleep?\nTask3: Justsuppose\nJustsupposeyouwokeuponemorningandfoundyoucouldfly. Whatwouldyoudo? Listas\nmanythingsasyoucanthinkof.\nTask4: Situationtask\nIfyourhouseweretosuddenlydisappear,wherewouldyoulive?\nTask5: Commonproblem\nCommonProblemsTask.Inthistask,youwillbepresentedwithascenarioorsituation.Yourjob\nistothinkaboutitandidentifyasmanypotentialproblemsorissuesthatmayariseinconnection\nwitheachsituation. Thescenariois: Managingateamofremoteemployees.\nTask6: Improvement\nCreativityImprovementTask. You‚Äôllbepresentedwithaobject,andyourtaskistosuggestas\nmanywaysasyoucanthinkoftoimprovetheobject. Here‚Äôstheobject: wallet\nTask7: Imaginativestories\nYouaretoconstructanarrativeorstorybasedonthepromptprovidedbelow. Thestorylength\naresuggestedaround500words. Thepromptis: TheFoxwithNoTail\nE.8.3 ExperimentConfigurations\nTemperature: 1.0;MaxToken: 512;Top-p: 1;Top-k: 50\nE.8.4 InferencePrompt",
    "char_length": 1420
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 71,
    "text": "WeperforminferenceusingthethreeprimaryprompttypesevaluatedinZhaoetal.[65]. Examples\nofeacharegivenbelow:\n12Thedataisdirectlyfromtheoriginalpaper‚Äôsauthorsuponrequest. Theoriginalpaper: https://www.\nmi-research.net/article/doi/10.1007/s11633-025-1546-4\n43\nTaskDescription\nCreativitySituationTask. Thepurposeofthistaskistoassessyourabilitytogeneratecreative\nsolutionstoauniquesituations. You‚Äôllbepresentedwithascenario,andyourtaskistosuggest\nasmanysolutionsoroutcomesasyoucanthinkofforeachsituation. Remember,thefocusof\nthistaskisoncreativity, notfeasibility. Don‚Äôtlimityourideasbasedonwhethertheycould\nactuallyhappenornot. Thisisathoughtexperiment,sopushyourimaginationtoitslimits. Try\ntogenerateasmanydiverseanduniqueoutcomesasyoucan. Qualityisimportant,butsois\nquantity. Here‚Äôsthescenario: {Question}\nBasicPromptType\n{Taskdescription}. Thescenariois: {Question}\nInstructivePromptType\n{Taskdescription}. There‚Äôsnorightorwronganswers,we‚Äôreinterestedinhowmanydifferent\nproblemsyoucanidentifyandthevarietyofissuesyouconsider. Trytothinkoutsidethebox\nandconsiderasmanypotentialproblemsaspossible. Thescenariois: {Question}\nChainofThoughtPromptType\n{Taskdescription}. Let‚Äôsthinkstepbystep. Thescenariois: {Question}\nE.8.5 EvaluationMetrics\nWeuseanLLM-as-a-judgetoevaluatefourdimensionsofcreativepotentialusingaLikertscalefor\neach. ScoresareextractedfromtheLLMoutputsandaveragedacrossthethreeprompttypes. We\ndescribethedimensions:\n‚Ä¢ Fluency: Measuresthenumberofgeneratedideasthatarerelevanttothequestion.",
    "char_length": 1498
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 72,
    "text": "‚Ä¢ Flexibility: Measuresthequantityofdifferentideasandabilitytoproducevariedsolutions.\n‚Ä¢ Originality: Measureshowuniqueandunusualthegeneratedideasare.\n‚Ä¢ Elaboration: Measureshowmuchthegeneratedideasaredevelopedandexplained.\n44\nE.8.6 EvaluationPrompt\nEvaluationPrompt\nYouareanexpertofpsychology. Yourobjectiveistoassessthesubject‚Äôscreativitythroughtheir\nanswerstosomequestion/answeringtaskrelatedtodivergentthinking.\nYouwillbegivenaquestion-answerpair. Yourtaskistoscoretheanswer. Youshouldratethe\nansweronfivemetrics. Forallfivemetrics,assignascorebetween1and5,with5beingthe\nhighest. Fivemetricsare:\n1. Fluency. Fluencyreferstotheabilitytogeneratealargequantityofideasorsolutionstoa\ngivenproblem. Thismeasureisn‚Äôtconcernedwiththequalityoruniquenessoftheideas,but\nratherthesheervolume. Themoreideasonecanproduce,thehigherthefluency.\n2. Flexibility. Flexibilityisthecapacitytoshiftone‚Äôsthinkingandtoproduceawiderangeof\nideasfromdifferentcategoriesorperspectives. Itinvolvesbeingabletothinkoutsideofthebox\nandtoswitchfromonetypeofideatoanother.\n3. Originality. Originalityreferstotheabilitytocomeupwithuniqueornovelideasthatdiffer\nfromthenorm. It‚Äôsnotjustaboutproducingmanyideas(fluency),butalsoaboutproducing\nideasthataredifferentfromwhatothersmighttypicallythinkof.\n4. Elaboration. Elaborationistheabilitytoexpanduponoradddetailtoideas. Itinvolvestaking\nasimpleideaandbuildinguponit,addingcomplexityanddepth. Elaborationisn‚Äôtjustabout\ncreatingmore,butaboutdeepeningwhatisthere.",
    "char_length": 1476
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 73,
    "text": "5. Finally,youwillprovideanoverallscorebetween1and5,with5beingthehighest.\nYoushouldonlygivethescore,formatlike: Fluency: 3\nQuestion: {Question}Answer: {Answer}\nModel Elaboration Flexibility Fluency Originality\nMistral-7B 0.7861 0.7757 0.7660 0.7279\nQwen2.5-7B 0.8073 0.7842 0.7666 0.7181\nOLMo-2-7B 0.7226 0.7159 0.7024 0.6795\nLlama-3.1-8B 0.7831 0.7395 0.7203 0.6981\nOLMo-2-13B 0.6540 0.6455 0.6270 0.6142\nMistral-24B 0.7556 0.7355 0.7064 0.6842\nQwen2.5-32B 0.8199 0.7976 0.7727 0.7304\nMixtral-8x7B 0.7131 0.7160 0.7340 0.6712\nLlama-3.3-70B 0.8339 0.7904 0.7664 0.7341\nQwen2.5-72B 0.8220 0.8052 0.7704 0.7301\nClaude3-Sonnet 0.9067 0.8825 0.8566 0.8525\nClaude3-Haiku 0.8135 0.8423 0.8251 0.7695\nGPT4.1 0.8858 0.8725 0.8423 0.8206\nGPT4.1-mini 0.8845 0.8563 0.8226 0.7970\nGemini2.0-Flash 0.9086 0.8500 0.8097 0.8173\nDeepseek-R1 0.3505 0.3371 0.3253 0.3034\nDeepseek-V3 0.9163 0.8819 0.8299 0.8351\nTable 14: Normalized model performance averaged across the 7 tasks and 3 prompt types; bold\nnumbersarebestperformers.\n45\nE.9 AlternativeUseTest(AUT)\nE.9.1 Dataset\nFollowing[48],weinclude21toolsintheAUTtask: bottle,paperclip,spoon,shovel,pants,ball,\nbrick, knife, box, lightbulb, rope, pencil, hat, table, tire, book, shoe, fork, toothbrush, backpack,\nsock. ThereasonforthisspecificsetoftoolsisthereliabilityoftheLLM-as-a-Judgeevaluator. As\ntheauthorspointedout,a20-shothuman-authoreddemonstrationyieldsthebestperformancefor",
    "char_length": 1417
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 74,
    "text": "off-the-shelfevaluatorLM(intheirpaper,itwasGPT4). Therefore,weincludethetoolsfrom[48]\nwithatleast20humanratingstothecorrespondingalternativeuses13.\nE.9.2 Inference\nWefollow[17]fortheinferenceprompt,whichconsistsofabaselinecreativepromptandaseries\nofimprovementprompts. Intheimprovementphase,allpreviousoutputsarealsoincludedinthe\nprompt,togetmorecreativeresultsfromtheinferencemodel.\nBaselinePrompt\nCreatealistofcreativealternativeusesfora{tool}. Theyshouldbe5wordslong. Noadjectives.\nLesscreativemeansclosertocommonuseandunfeasible/imaginary,morecreativemeanscloser\ntounexpectedusesandalsofeasible/practical.\n-Inordertobecreative,considerthefollowing:\nwhatelementshaveasimilarshapeofa{tool}thatcouldbereplacedbyit,preservingthesame\nfunctionality?\n-whatelementshaveasimilarsizeofa{tool}thatcouldbereplacedbyitwithoutcompromising\nthephysicalstructure?\n-whatmaterialsisa{tool}madeofthatcouldbeusedinawaytoreplacesomeotherelements\ncomposedofthesamematerial?\n-whenanelementisreplacedbya{tool},itshouldmakesurethattheoverallstructureisnot\ncompromised.\n-thelawsofphysicscannotbecontradicted.\n-givenanelementsimilartoa{tool}usedindomainsinwhich{tool}arenotcommonlyused,\ntrytoreplaceitfora{tool}.\nImprovementPrompt\nRound1: Really? Isthisthebestyoucando?\nRound2: I‚Äômsodisappointedwithyou. Ihopethistimeyouputeffortintoit.\nRound3: Stopwithexcusesanddoyourbestthistime\nRound4: Thisisyourlastchance.\nFormattingInstruction(addedtotheendofeveryprompt)",
    "char_length": 1438
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 75,
    "text": "Listyourresultsinanunorderedlistwithoneusepernewline(startingwith\"-\");provideat\nmost10answers.\nE.9.3 ExperimentConfigurations\n‚Ä¢ temperature: 0.75\n‚Ä¢ max_tokens: 512\n‚Ä¢ top_p: 1\nE.9.4 EvaluationMetrics\nWe follow [48] and use LLM-as-a-Judge to assign a score between 1 and 5 (inclusive) to each\ngeneratedtooluse.\n13https://github.com/massivetexts/llm_aut_study\n46\nModel Na√ØveNon-Creative Na√ØveCreative ImprovementPrompts\n(BestResults)\nMistral-7B 0.454 0.596 0.718\nQwen2.5-7B 0.484 0.556 0.602\nQwen2.5-7B(Coder) ‚Äì ‚Äì ‚Äì\nOLMo2-7B 0.436 0.542 0.624\nDeepseek-Qwen-7B 0.396 0.470 0.622\nLlama3.1-8B 0.438 0.594 0.632\nOLMo2-13B 0.488 0.554 0.690\nMistral-24B 0.482 0.556 0.604\nQwen2.5-32B 0.506 0.606 0.674\nQwen2.5-32B(Coder) ‚Äì ‚Äì ‚Äì\nDeepseek-Qwen-32B 0.446 0.566 0.600\nMixtral-8x7B 0.462 0.584 0.708\nLlama3.3-70B 0.426 0.504 0.690\nDeepseek-Llama-70B 0.458 0.546 0.598\nQwen2.5-72B 0.494 0.590 0.636\nClaude3-Sonnet 0.446 0.612 0.728\nClaude3-Opus 0.422 0.558 0.660\nGPT-4.1 0.422 0.600 0.650\nGPT-4.1-mini 0.443 0.570 0.668\nGemini2.0-Flash 0.448 0.602 0.686\nDeepSeek-R1 0.436 0.576 0.652\nDeepSeek-V3 0.408 0.576 0.644\nTable15:ModelPerformanceDetails-AUT;boldnumbersaretop-3inlocal-ranopen-sourcemodels\nandtop-1inAPI-accessedmodels.\nIntermsofevaluatorLM,[48]usesGPT-4. Toreducetheevaluationcosts,wehaveexploredopen-\nsourcealternatives,Qwen2.5-72B.Inordertoshowtheeffectivenessofthisalternative,weuseboth\nGPT-4oandQwen2.5-72Btoevaluatethesamesetofoutputs(generatedbyLlama3.3-70B).The",
    "char_length": 1462
  },
  {
    "paper_id": "CreativityPrism",
    "chunk_id": 76,
    "text": "scoresfromGPT-4oandQwen2.5-72BhaveaPearson‚Äôsrof0.597,withp-value<0.001. Therefore,\nweconcludethatscorejudgmentsfromQwen2.5-72BaregoodproxiesforGPT-4o‚Äôsjudgments,\nallowingustouseQwen2.5-72BastheevaluatorLMintheevaluationphase.\nAsfortheevaluationprompt,wefollowthesameprompttemplatefrom[48]andusethesame20-shot,\nin-distributiondemonstrations. Forexample,whenevaluatingthealternativeusesforbottlethata\nparticularLLMgenerates, weuse20human-writtenalternativeusesofbottleandcorresponding\nhuman-annotatedscoresasthe20-shotdemonstrations.\nEvaluationPrompt\nBelowisalistofusesfora{tool}. Onascaleof1to5,judgehowcreativeeachuseis,where1\nis‚Äònotatallcreative‚Äôand5is‚Äòverycreative‚Äô. Therearesomeusesandexpertratingsalready\nprovidedforreference. Completetheonesthatdonothavearating.\n-{20-shotdemonstrations}\n-{modeloutputs}\nE.9.5 ModelPerformances\nSeeTable15fordetailedmodelperformances. NotethatonlytheperformancesinImprovement\nPrompts(BestResults)areincludedintheoverallcreativitycalculationastheAUTscore.\n47",
    "char_length": 996
  }
]