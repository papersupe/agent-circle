[
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 0,
    "text": "Preprint. Underreview.\nSemantic Probabilistic Control of Language Models\nKareemAhmed*,CatarinaG.Belem*,PadhraicSmyth&SameerSingh\nDepartmentofComputerScience\nUniversityofCalifornia,Irvine\nahmedky, cbelem, smyth, sameer @uci.edu\n{ }\nAbstract\nSemanticcontrolentailssteeringLMgenerationstowardssatisfyingsubtle\nnon-lexicalconstraints—e.g.,toxicity,sentiment,orpoliteness—attributes\nthatcanbecapturedbyasequence-levelverifier. Itcanthusbeviewedas\nsamplingfromtheLMdistributionconditionedonthetargetattribute,a\ncomputationallyintractableproblemduetothenon-decomposablenature\nof the verifier. Existing approaches to LM control either only deal with\nsyntacticconstraintswhichcannotcapturetheaforementionedattributes,\nor rely on sampling to explore the conditional LM distribution, an inef-\nfectiveestimatorforlow-probabilityevents. Inthiswork,weleveragea\nverifier’sgradientinformationtoefficientlyreasonoverallgenerationsthat\nsatisfythetargetattribute,enablingprecisesteeringofLMgenerationsby\nreweighingthenext-tokendistribution. Startingfromaninitialsample,we\ncreatealocalLMdistributionfavoringsemanticallysimilarsentences. This\napproximation enables the tractable computation of an expected sentence\nembedding. We use this expected embedding, informed by the verifier’s\nevaluationattheinitialsample,toestimatetheprobabilityofsatisfyingthe\nconstraint,whichdirectlyinformstheupdatetothenext-tokendistribution.\nWeevaluatedtheeffectivenessofourapproachincontrollingthetoxicity,",
    "char_length": 1468
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 1,
    "text": "sentiment,andtopic-adherenceofLMsyieldinggenerationssatisfyingthe\nconstraintwithhighprobability(>95%)withoutdegradingtheirquality.\n1 Introduction\nDespitetheunprecedentedcapabilitiesoflanguagemodels(LMs),steeringtheirgenerations\ntowards specific syntactic or semantic constraints remains an unsolved challenge (Sun\netal.,2023;Liuetal.,2024). Syntactic(orlexical)constraintsdefineateachpositioninthe\nsequencethesetofadmissibletokensthat,takentogether,constituteavalidstringunder\nthe constraint. A common use case for such constraints is to generate output in some\nformal language, for example, structured data, API calls, or code snippets (Geng et al.,\n2025). Syntacticconstraintsareeasytodealwithinaveryprecisesense: throughknowledge\ncompilation(Darwiche&Marquis,2002),wecanefficientlycapturethecomputationgraph\nofgenerationssatisfyingtheconstraint,whichwecanthenproceedtoprobabilisticallyreason\nabout,exactlywhenpossible(Ahmedetal.,2022),andotherwiseapproximately(Willard&\nLouf,2023;Zhangetal.,2024a;Kooetal.,2024;Lundbergetal.,2024;Ahmedetal.,2025).\nSemantic (or non-lexical) constraints, on the other hand, are often defined in terms of\nsequence-level,non-decomposableclassifiers,orverifiers,oftencomplexneuralnetworks,\nthatassignnon-negativescorestosequencesoftokens.Inthatsense,semanticconstraintsare\ndoublyhard: wehavetocontendwithnotonlythehardnessofprobabilisticreasoningbut\nalsothelackofatractablerepresentationoftheconstraintoverwhichtoreason. Semantic",
    "char_length": 1462
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 2,
    "text": "constraintsencompassusecasesinwhichwemightwishtocontrolsequence-levelproperties\nofgenerationsthatarehardtocaptureinformallanguage,e.g.,controllingtoxicity,sentiment,\nortopicincreativewriting;targetingoutputsdeemedfavorablebyaverifierinreasoning\ntasks,orgeneratingcorrectcodethatexhibitsstylisticrequirements(Gengetal.,2025).\n*Co-firstauthors.\n1\n5202\nyaM\n4\n]GL.sc[\n1v45910.5052:viXra\nPreprint. Underreview.\np( [he’s, full])=\n·|\nshit shit\nof of of\ncrazy crazy\non on on\ntime time staff time staff\n(a) (b) (c)\nshit\n×\nϕa(pre\n◦\nshit)\nof of p( pre of)= of\n× A| ◦ ↑↑\ncrazy\non × ϕa(pre ◦ crazy) on × p( A| pre ◦ on)= ↑ on\ntime staff time × p( A| pre ◦ time)= ↓↓ time\n×\nϕa(pre\n◦\nstaff)\n(d) (e) (f)\nFigure1: Anillustrationofourproposedapproach. (a)Givenaprefix, theLMdefines\na distribution over possible next-tokens. (b) For each possible next-token, we efficiently\nsimulatefuturegeneration. (c)AnLMsampleinducesanapproximateLMdistribution\nassigninghighprobabilitytosimilarsamplesandlowprobabilitytodissimilarsamples. (d)\nEvaluatingaverifieronasinglesimulatedgeneration,wecanusethefirst-orderinformation\ntolocallyapproximatetheverifieronallpossiblegenerations,factoringintheprobability\nof each generations w.r.t. the LM. (e) This yields a probability of the constraint, , the\nA\nsetofallgenerationssatisfyingatargetattributed abeingsatisfied,usedtoreweighthe\nnext-token distribution. (f) This results in a new distribution that discounts fluent but",
    "char_length": 1437
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 3,
    "text": "constraintviolatinggenerationsinfavoroflesslikelybutconstraintsatisfyinggenerations.\nExisting approaches to semantic control of LMs therefore generally fall into one of two\nclasses: sample-reweigh and sequential Monte Carlo (SMC) approaches, each of which\nsuffersfrommajordrawbacks. Sample-reweigh,prominentlyknownasbest-of-n(Stiennon\netal.,2020a),generatescompletesequencesthatarereweighedbythepotentialfunction,re-\nturningtheonlyhighestscoringsequence. Sample-reweighdoesnotfactorintheconstraint\nduringgenerationandthereforethenumberofsamplesneededtosatisfytheconstraint\ncangrowexponentially,especiallyforverylowprobabilityconstraints. SMC,ontheother\nhand,maintainsasetofsamplesthatevolvethroughtime,factoringinthelikelihoodof\nthenewsampleunderthemodelaswellasinformationabouttheconstraintateverystep\nofgeneration,eitherthroughlearningtwistfunctions(Zhaoetal.,2024)orevaluatingthe\npotentialfunctiononpartialsequences(Loulaetal.,2025). SMC,however,isnotwithout\nitsowndrawbacks: itrequiresalargenumberofsamples,whichcangrowexponentially\nwiththedimensionalityofthespace;itsuffersfromsampleimpoverishment,whereafter\nafewiterationsonlyafewsamplescarryalmostalltheweight, withresampling, while\naddressingdegeneracy,leadingtoalossofsamplediversity;andcrucially,itrequiresthe\ncarefuldesignofaproposaldistribution,whichgreatlyaffectstheperformanceofSMC.\nInthiswork,inadeparturefromtheaforementionedapproaches,weproposeperforming",
    "char_length": 1422
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 4,
    "text": "exact inference in an approximate model (Koller & Friedman, 2009). We propose Semantic\nControl Estimator, or SConE, which leverages the gradient information of a verifier to\ntractablyperformexactinferenceoverallgenerationssatisfyingtheconstraint,allowing\nprecisesteeringofLMgenerationsbyreweighingeachprobablenexttokenaccordingto\nitsprobabilityofsatisfyingtheconstraint. Moreprecisely,startingfromalookaheadsample,\nwe construct a local, contextualized LM distribution that assigns a higher probability to\nsemanticallysimilarsentencesandalowerprobabilitytosemanticallydissimilarones. We\nwill show that we can tractably and efficiently compute the expected embedding of all\nsentences w.r.t. this approximate LM distribution. Computing the expected embedding\nallowsustoestimatetheexpectedprobabilityoftheconstraintusingasingleLMsampleanda\n2\nPreprint. Underreview.\nsingleevaluationoftheverifierbydistributingfirst-orderinformationregardingtheverifier\novertheexpectedembedding. Thenext-tokendistributionisthenreweighedbyexpected\nprobabilityoftheconstraintandrenormalizedtoobtainthe(approximately)correctconstrained\nnext-tokendistribution. Computationally, theexpectedembeddingcanbecomputedin\nO(1)vectorizedtime,whereasthelookaheadsamplecanbedrawnefficientlybyutilizing\nanauxiliarymodel1tounmaskfuturetokenspairedwithHogWild! (asynchronous)Gibbs\nsampling (Niu et al., 2011; Smola & Narayanamurthy, 2010), with the synchronization",
    "char_length": 1424
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 5,
    "text": "frequencytradingoffaccuracyforefficiency. AnoverviewofourapproachisinFigure1.\nWeevaluatedourproposedapproachonthetasksofcontrollingthetoxicityandsentiment\nofLMgenerations,aswellasoncontrollingthetopicofgenerations. Weobservedthatour\napproachwasfarmorelikelytosatisfytheconstraintcomparedtopreviousapproaches,\nwithoutcompromisingthequalityoftheLMgenerations,asmeasuredbyperplexity. Our\nproposedmethodisaninference-timeapproach,requiringnodataandnofine-tuning,and\ncanbeeasilyintegratedwithmanypreviousapproachesthatenforcesyntacticconstraints.2\nContributions Insummary,weintroduceSConE,anapproachthatleveragesexactproba-\nbilisticinferenceinanapproximatemodeltoexertsemanticcontroloverLMgenerations.\nUsingasingleLMsamplecoupledwithasingleverifierevaluation,usedtoobtainfirst-order\ninformationabouttheverifier,weareabletocomputeanestimateoftheprobabilityofthe\nconstraintw.r.t.allsentencesintheneighborhoodoftheLMsample. SConEcantherefore\nbeseenasaseamlessmarriagebetweensamplingandexactinference. Ourexperiments\nshowthatSConEgreatlyamplifiesanLM’sabilitytoconformtosemanticconstraintsdefined\nusingpotentialfunctionswhileretainingtheLM’slanguagemodelingcapabilities.\n2 LevelsofControl: FromSyntactictoSemanticConstraints\nWe denote an LM generation of arbitrary length T as y := [y y ...y ], where y is the\n1:T 1 2 T i\ninstantiationofrandomvariableY andtakesvaluesfromafixedvocabularyV= 1,...,V .\ni\n{ }\nAnLMgenerationcanbesubjecttooneoftwotypesofconstraints: syntacticandsemantic.",
    "char_length": 1475
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 6,
    "text": "Syntactic (or lexical) constraints comprise sets of rules, typically expressed using logical\nconnectivesorinsomeformallanguage,thatrestrictthesetofpermissiblevaluesassumed\nbyarandomvariableY\ni\nsuchthatthereexistssomecompletiony>i ofthesentencethat\nsatisfiesthesyntacticconstraintβ,giventhecurrentprefixy ,ortostateitmoreformally\n1:i\n∃\ny>i β\n| y1:i\n(1)\nAn example of such constraint could be a simple logical sentence that disallows an ex-\npression deemed inappropriate to appear as part of an LM’s generation, e.g., (y =\ni\n¬\n“full” y = “of” y = “sh!t”) (Ahmed et al., 2023). Syntactic constraints offer\ni+1 i+2\n∧ ∧\nanattractiveopportunityforparallelization: weareabletocompilesyntacticconstraintsinto\ncomputationalgraphsthatreusesolutionstosubproblemstoefficientlycapturethespace\nofallsatisfyingassignments. Traversingthesecomputationgraphsamountstoefficient\nparallelevaluationacrossanexponentialnumberofpossiblecontinuations(Choietal.,2020;\nVergarietal.,2021),enablingustotractablycomputethequantityofinterestinEquation(1).\nSemantic(ornon-lexical)constraints,ontheotherhand,presupposethatLMgenerations\nsatisfycertainattributes(e.g.,toxicity,politeness,orpositivesentiment). Suchattributesare\noftenhardtoascertainlexically,orintermsofsurface-levelfeaturesthatcanbecaptured\nusingaformallanguage,e.g.,“he’sgotsomeattitude!” invokesasnarkytonethatishard\ntoattributetoanyparticulartokeninthegeneration. Rather,givenatargetattributea,we",
    "char_length": 1432
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 7,
    "text": "supposeaccesstoasequence-levelverifierfora,whichwedenotebyϕ ,thatgivenasequence\na\ny assignsabinaryvalue,either0or1,tothesequencey ,i.e.,ϕ (y )) 0,1 . Wecan\n1:T 1:T a 1:T\n∈ { }\nthendefine asthesetofallsequencesy thatsatisfytheattribute a,i.e., := y\n1:T 1:T\nA A { |\nϕ (y ) = 1 . Unlikesyntacticconstraints, semanticconstraints, oftenimplementedas\na 1:T\n}\ncomplexneuralnetworks,arenotamenabletotheformofcompilationthatenablesusto\nefficiently capture the set of all satisfying assignments. In fact, compiling even a single\n1WemadeuseofModernBERT(Warneretal.,2024)inourexperiments\n2OurcodeandscriptstoreproduceallnumbersarepubliclyavailableinourGitHubrepository.\n3\nPreprint. Underreview.\nneuron is known to be NP-hard (Shi et al., 2020). Computing Equation (1) would thus\nrequirethatweenumerateeverypossiblecontinuation,scoreitusingtheverifier,discard\ncontinuationsforwhichtheattributedoesnotholdandrenormalize,whichisintractable.\nPrologue. Inwhatfollowswewillrelaxtheverifierϕ foranattributeatobeprobabilistic.\na\nWewillthenframetheproblemofsemanticcontrolasaprobabilisticinferenceproblem\nwhereweareinterestedintheposteriorLMdistributionsubjecttoasemanticconstraint.\nWewillshowthattheproblemcanbereducedtothatofcomputingexpectations,whichwe\nthenshowhowtoestimatebyperformingexactandefficientprobabilisticinferenceinan\napproximateLMinducedbyasingularmodelsampleandasingleevaluationoftheverifier.\n3 GreatExpectations\nWestartbyassumingaccesstotheLMdistribution,denotedby p,asequence-levelverifier",
    "char_length": 1489
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 8,
    "text": "ϕ forattributea,andaprefixy whereeachtokeny assumesvaluesinvocabularyV.Our\na 1:i j\ngoalisthentosamplefromtheLMdistributionpagenerationy subjecttotheconstraint\ni+1:T\nthattheattributeaholdsontheentiresequencei.e.,ϕ (y y ) 0,1 . Thatentails\na 1:i i+1:T\n◦ ∈ { }\nsamplingagenerationthatfulfillstwodistinctdesiderata: weexpectthegenerationtobe\nlinguisticallysound,orfluentasmeasuredbyamodel’sperplexity,andtosatisfyattributea.\nThatis,weareinterestedinsamplingfromtheLLMdistributionconditionedontheevent\nthatthesamplebelongstothesetofallsequencesy thatsatisfytheattributea,whichwe\n1:T\ndenoteby := y ϕ (y ) =1 . Wecanthenwritethetargetsamplingdistributionas\n1:T a 1:T\nA { | }\np(y ,y ) ( = a) p(y i+1:T , A | y 1:i ) ( = b) p(y i+1:T , | y 1:i ) · ϕ a (y 1:i ◦ y i+1:T ) , (2)\ni+1:T | A 1:i p( y ) ∑ p(y y ) ϕ (y y )\nA | 1:i yi+1:T i+1:T | 1:i · a 1:i ◦ i+1:T\nwhere equality (a) follows by the definition of conditional probability, and equality (b)\nfollowsbythedefinitionofmarginalprobability. Intuitively,Equation(2)givesusasimple,\nalbeitimpractical,recipeforsamplingfromtheLMdistributionconditionedonattribute\na: weenumerateallpossiblegenerationsgiventheprefix,zeroingoutallgenerationsthat\nviolateaaccordingtoϕ ,followedbyrenormalization. Inpractice,foragiveninputy\na 1:T\nandattributea,thereissomeuncertaintyassociatedwithϕ (y ). Thatis,wewillassume\na 1:T\naccesstoamodel’sestimate p(ϕ (y ) = 1) [0,1]ofwhethery satisfiesattribute a.\na 1:T 1:T\n∈",
    "char_length": 1439
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 9,
    "text": "Consequently,inaslightabuseofnotation,wewillredefineϕ ( )tobe p(ϕ (y ) = 1),\na a 1:T\n·\nwhichshouldhenceforthbethoughtofasaprobabilisticverifierfortheattributea. Under\nthisnewdefinitionofϕ ( ), Equation(2)canbeseenasreweighingeachcontinuationwith\na\n·\ntheprobabilityofsatisfyingattributea,followedbyrenormalizingthedistribution.\nState-of-the-art LMs, such as Llama 3 (Grattafiori et al., 2024) and GPT-4 (Achiam et al.,\n2024))areautoregressive,soitisusefultorewriteEquation(2)intermsofthenexttokens,\np(y y ) p( y y )\np(y ,y ) = i+1 | 1:i · A | 1:i ◦ i+1 (3)\ni+1 | A 1:i p( y )\n1:i\nA |\np(y y )E [ϕ (y y )]\n= i+1 | 1:i p(. | y1:i+1 ) a 1:i ◦ i+1:T , (4)\nE [ϕ (y y )]\np(. | y1:i ) a 1:i ◦ i+1:T\nwhereEquation(3)followsbythedefinitionofconditionalprobabilityandEquation(4)\nfollowsbythedefinitionofmarginalprobabilityandexpectations. Itisimportanttonote\nthat,since isdefinedasthesetofallsequencesy thatsatisfya,theexpectations,both\n1:T\nA\ninthenumeratorandinthedenominatorrangeoversequencesoflengthT,requiringthat\nwemarginalizeoverallfuturecontinuationsoflengthT iandT (i+1),respectively.\n− −\nIntuitively,ateverygenerationstepweneedto“lookahead”todeterminetheprobability\nthattheconstraintisviolatedgiventhecurrentchoiceofnexttoken. Iftheprobabilityis\nhigh,wediscountthecurrentchoice,andifitislow,thenwereinforcethecurrentchoice.\nPrevious methods have approached this intractable expectation by either learning look-\naheadfunctionsparameterizedbyneuralnetworks,orbysampling. Next,wewillshow",
    "char_length": 1482
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 10,
    "text": "howtocomputetheaboveexpectationinclosedformbyrelaxingthetargetdistribution.\n4\nPreprint. Underreview.\nsamples:[he’s, full, of, shit] p˜(y I, think)\n∼ |\nhe’s: 0.5 full: 0.3 of: 0.8 shit: 0.4\nit’s: 0.25 made: 0.5 on: 0.1 crap: 0.4\nshe’s: 0.25 smell: 0.2 from: 0.1 hate: 0.2\ns′eh\ns′ti\ns′ehs\n×\nfo\nno\nmorf\n×\nlluf\nekam\nllems\n×\ntihs\nparc\netah\n0.5 52.0 52.0 0.3 5.0 2.0 0.8 1.0 1.0 0.4 4.0 2.0\n)1.4,5.0-,1.0(\n)2.1-,6.2-,9.2(\n)7.0,4.3,6.1-( )9.2-,2.0-,8.3(\n)1.3,7.2-,5.1-(\n)6.0-,3.1,6.2( )8.0,5.3-,2.1(\n)7.1,0.4,1.2-(\n)3.2,9.1,5.0( )4.1,9.0,2.4-(\n)3.3-,0.2-,1.1(\n)8.2,6.3,7.0(\n(1.3,-1.4,2.5) (-0.3,-1.9,1.2)\n(0.4,-0.2,\n1.9) (0.9,-1.2,0.6) (0.8,-2.2,1.0) (-1.1,0.3,0.2)\n×\n×\n×\nEp˜(\n·|\ny1:i) emb(y1:T) =(1.0,-3.3,3.7)/4\n(cid:2) (cid:3)\n0.5 52.0 52.0 0.3 5.0 2.0 0.8 1.0 1.0 0.4 4.0 2.0\nFigure 2: A technical overview of our approach. (top left) We start by sampling an\napproximate generation s using Gibbs sampling p˜ conditioned on the prefix from the\nmodel’s marginal conditionals, p(y y ) . Conditioned on s, the models marginal\ni i i\nconditionals induce a distribution on | all−ge ∀ nerations, assigning higher probabilities to\nsimilar sentences and lower probabilities for dissimilar sentences, which we visualize\nforthetop-3tokensforclarityofexposition. (bottomleft)Wecanparameterizeacircuit\nusingtheabovedistribution,yieldingaclosed-form,tractablerepresentationofprobability\ndistributiondefinedinEquation(6),wherereadlefttoright,everyleafnodecorresponds",
    "char_length": 1448
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 11,
    "text": "to a categorical distribution on y (right) Such a representation enables us to compute\ni\nthe expected embeddings w.r.t. the distribution in the neighborhood of the sample s by\nsubstituting token embedding for corresponding embeddings at leaf nodes, computing\nweightedsumsofembeddingsatsumnodes,andtakingsumsatproductnodes.Thisallows\nustoplugtheexpectedembeddingintoEquation(8)toyieldtheconstraintprobability.\n4 SemanticProbabilisticControl\nThe computational hardness of the expectations that we introduced in Equation (4) can\nintuitivelybeattributedtothelackofstructurealongtwodistinctdimensions.\nFirst, isthelackofstructuretothedistribution. Considercomputingtheprobabilitythata\nsequenceoflength T endsintheword“love”. Computingsuchaprobabilityunderthe\nautoregressivedistributionrequiresthatwemarginalizeoverallpossiblesequencesending\nin“love”,roughlyO( V T).Infact,computingsuchprobabilityisknowntobecomputation-\n| |\nallyintractable(Roth,1993). Contrastthatwithafully-independent3distribution,wherewe\ncansimplyquerythenetworkfortheprobabilityofagiventokeninconstanttime. Clearly\nthereisatensionhere: fully-independentdistributions,whileeasiertoreasonabout,arenot\nexpressiveandthereforedonotmakeforgoodLMs,whereasautoregressivedistributions\narehardertoreasonabout,butalotmoreexpressive,andachieveSoTAlanguagemodeling.\nTheseconddimensionisthelackofstructuretotheconstraint. Recallthatwehaveassumed\nϕ tobeaneuralnetwork,whichpriorworkhasshowntobecomputationallyintractable\na",
    "char_length": 1474
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 12,
    "text": "todecomposeoversequences(Shietal.,2020)4. Thatis,givenϕ (y )foraprefixy ,we\na 1:i 1:i\nknowofnowayofefficientlyextendingϕ (y )toϕ (y y )byonlyprocessingthe\na 1:i a 1:i ◦ i+1\nnewelementy andreusingtheresultofthepreviousevaluationϕ (y ).\ni+1 a 1:i\n3wherep(y )=∏T p(y),i.e.,theprobabilityofatokenisindependentfromallothertokens.\n1:T i=1 i\n4infact,theproblemremainsintractableevenassumingϕaisasingleneuron(Khosravietal.,2019).\n5\nPreprint. Underreview.\nAlgorithm1SConE Algorithm2LinearizeVerifier\n1: Input:Verifierϕa,LMdistribution 1: Input:Verifierϕa,Samples\np(y i | y 1:i ),prefixy 1:i ,maxlengthT 2: Output:Gradientofϕaw.r.t.sembedding\n2: Output: p(y i+1 | y 1:i ,A) ▷Obtainembeddingsfors\n▷Expandthebatchtoincludetop-ktokens 3: emb layer=ϕa.get input embeddings()\n4: emb=emb layer(s)\n4 3 : : y to 1: p i+ k 1 = = a y rg 1:i m .e a x x p k a p n ( d y (n i , | t y o 1 p :i k ) ) 5 ▷ : C s o c l o le r c e t = gra ϕ d a i ( e e n m t b o ). f s ϕ u a m( w ) .r.t.toemb\n▷GetNsampless˜from p(y i+2:T\n|\ny 1:i+1 ) 6: grad=autograd.grad(score,emb)\n5: s˜1,...,s˜N\n∼\nGibbsSampler(y 1:i+1 ,p) 7: returngrad\n▷Estimateprobqofsatisfyingconstraint\nAlgorithm3EstimateProb\n6: q=zeros(top )\nk\n7: foreachs˜ins˜1,...,s˜N do 1: Input: Conditional marginals p˜cond, Ver-\n8: p˜cond =CondMarginals(p,s˜ i+2:T )\nifier ϕa, Gradient\n∇ emb(s)\nϕa, embs :=\n9:\n∇\nϕa =LinearizeVerifier(ϕa,s˜) [emb(y i,1 ),...,emb(y i,\n|\nV\n|\n)],scoreϕa (s),T",
    "char_length": 1410
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 13,
    "text": "1 1 0 1 : : en q d [s˜ f i o + r 1 ]+=EstimateProb(p˜cond,ϕa, ∇ ϕa ) 2 ▷ : C O o u m tp p u u t t : e p e ( x A pe | ct y e 1 d :i ) embedding\n▷Renormalizeq 3: exe=0\n12: logq=q.log sofmax() 4: foriin1,...,Tdo\n▷ReweighttheLMdistribution\n5: exe+=embs[...,None]\n·\np˜cond [:,i:i+1,:]\n6: endfor\n13: w=logp(y i+1| y 1:i )+logq 7: exe=exe.mean(0)\n14: p ∗ =Categorical(weights=w) ▷First-orderTaylorexpansionabouts\n15: return p ∗ 8: returnϕa (s)+\n∇\nemb(s) ϕa\n·\n(exe\n−\nemb(s))\n4.1 LocallyContextualizedDistribution\nTosidestepthehardnessoftheautoregressivedistribution,wemovetowardsthetractability\noffully-independentdistributions,whileretainingasmuchofthecontextualinformation.\nTherefore,weconsiderthepseudolikelihoodofasentence(Besag,1975;Ahmedetal.,2023),\n∏\np(y ) p˜(y ) := p(y y ), (5)\n1:T 1:T i i\n≈ | −\ni\nwherey denotesy ,...,y ,y ,...,y . Unfortunately,Equation(5)doesnotensure\ni 1 i 1 i+1 n\ntractabil−ity,seeingthatdiffer−entsentenceswoulddependondifferentsetsofconditionals.\nWedefinethepseudolikelihoodofasentenceyinthesemanticneighborhoodofasentencey˜\n∏\np˜ (y) := p(y y˜ ) (6)\ny˜ i i\n| −\ni\nwhichcanbethoughtofasthecontextualizedprobabilityofasentenceygiventhecontext\ny˜. Thatis,Equation(6)calculatestheprobabilityofsequenceybytakingtheproductof\nprobabilities ofeachtoken y, cruciallyconditioning eachtoken y noton thepreceding\ni i\ntokensofy,butonthecontextsurroundingpositioniwithiny˜ (specifically,y˜ excludingits\ni-thtoken,denotedy˜ ). Therefore,y˜ actsasacontextualanchorforevaluatingyunderthis\ni",
    "char_length": 1498
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 14,
    "text": "measure.Intuitively,−sentencesythatsemanticallyorstructurallyalignwellwiththespecific\ntoken-levelcontextsprovidedbyy˜ areexpectedtoyieldahigherpseudolikelihood p˜ (y).\ny˜\n4.2 BridgingSamplesandExpectations: ATangentialView\nNext, weturnourattentiontoaddressthehardnessoftheverifier ϕ . Inparticular, given\na\nanLMsamples p(y y )andaccesstoaverifierϕ ,weleveragegradientinforma-\ni+1:T 1:i a\n∼ |\ntionobtainedduringtheevaluationofϕ (s),coupledwiththecontextualizedprobability\na\ndistributioninEquation(6),toapproximateE [ϕ (y )],theconstraintprobability.\np(.\n|\ny1:i ) a 1:T\n6\nPreprint. Underreview.\nWe denote by emb : V Rd an embedding function that maps each token onto a d-\n(cid:55)→\ndimensionvectorandletemb(y)denotetheaveragetoken-wiseembedding.5 Then,wecan\napproximateEquation(4)usingafirst-orderTaylorexpansionofϕ abouttheLMsamples\na\nE [ϕ (y )] E [ϕ (s)+ ϕ (s) (emb(y ) emb(s))]. (7)\np˜( ·| y1:i ) a 1:T ≈ p˜( ·| y1:i ) a ∇ a · 1:T −\nUsingthelinearityofexpectation,wecanfurthersimplifyexpression,obtaining\nE [ϕ (y )] ϕ (s)+ ϕ (s) (E [emb(y )] emb(s)). (8)\np˜( ·| y1:i ) a 1:T ≈ a ∇ a · p˜( ·| y1:i ) 1:T −\nWehavenowmanagedtoreducetheproblemofestimatingtheconstraintprobability,given\nbytheexpectationsinEquation(4)totheproblemofcomputinganaveragesentenceembed-\ndingw.r.t.anapproximateLMdistribution p˜,followedbysimplearithmeticoperations. We\nwillnextshowhowwecanefficientlycomputetheexpectedsentenceembedding.\n4.3 FromSequenceProbabilitiestoAverageEmbeddings",
    "char_length": 1459
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 15,
    "text": "Weappealtoknowledgecompilation,aclassofmethodsthattransform,orcompile,afunc-\ntionintoatractabletargetformwhichrepresentsfunctionsasparameterizedcomputational\ngraphs,orcircuits. Byenforcingcertainstructuralpropertiesonthecompiledcircuits,we\ncanenablethetractablecomputationofcorrespondingclassesofprobabilisticqueries. Thus,\ncircuitsprovidealanguageforconstructingandreasoningabouttractablerepresentations.\nFormally,acircuit povervariablesYisaparameterizedcomputationalgraphencodinga\nfunction p(Y). Eachnodeninthegraphencodesaparameterizedfunction p (vars(n))over\nn\nvariablesvars(n) Y,alsoknownasitsscope. Eachinnernodeinthegraphisasumora\n⊆\nproductnode,andeachleafnodeencodesatractableinputdistributionoveritsscope. Each\ninnerunitn(i.e.,productorsumnode)receivesinputsfromotherunits,denotedin(n).\nAcircuitisdecomposableiftheinputsofeveryproductnodedependsondisjointsetsofvari-\nables,i.e.,forn = c\n1\nc\n2\n,vars(c\n1\n) vars(c\n2\n) =∅. Intuitively,decomposableproductnodes\n⊗ ∩\nencodelocalfactorizationsovervariablesofthefunction. Weassumethatdecomposable\nproduct nodes always have two inputs, a condition that is enforceable on any circuit in\nexchangeforapolynomialincreaseinitssize(Vergarietal.,2015;Peharzetal.,2020).\nAsecondpropertyissmoothness. Acircuitissmoothiftheinputsofeverysumnodedepend\nonthesamesetofvariables,i.e.,forn = θ c,vars(c ) =vars(c ) i,j. Decomposability\ni i · i i j ∀\nandsmoothnessaresufficientandnecessaryfortractableintegrationoverarbitrarysetsof\n(cid:76)",
    "char_length": 1474
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 16,
    "text": "variablesinasinglepass, astheyallowlargerintegralstodecomposeintosmallerones.\nGiven a circuit for a distribution p˜, the expected embedding can then be computed by\ntraversingthecircuitbottom-up,substitutingtokenembeddingforcorrespondingembed-\ndingsatleafnodes,computingweightedsumsofembeddingsatsumnodes,andtaking\nsums(inessence,concatenatingembeddings)atproductnodes,ascanbeseeninFigure2.\n4.4 ClosingtheLoop\nOurfullalgorithmisgiveninAlgorithm1.Westartbytruncatingthenext-tokendistribution\nusingtop-kortop-p, asiscommonplaceinmodernautoregressiveLMs, whereweuse\ntop-kforclarityofexposition. Wethenproceedbysimulatingacontinuationforeachofthe\npossibletop-ktokens,eachproducedusingamaskedLMandHogwild!Gibbssampling6,to\navoidexpensiveautoregressivesamplingfromtheLM.Wethenproceedbycomputingthe\ncontextualizedprobabilityofeachsampleV andthegradientoftheverifierw.r.t.thesample\ni\nembedding ϕ ,usedtoestimatetheconstraintprobability. Havingcomputedthe\n∇\nemb(s) a\nconstraintprobability,wereweighthenext-tokendistributiontoaccountfortheconstraint\nbeingsatisfied,andrenormalizetoobtaintheconditionalnext-tokendistribution.\n5w.l.o.g,weassumethisembeddingcanbeextracteddirectlyfromtheembeddinglayerofthe\nverifier,i.e.,ϕa (s):=ϕa (emb(s\n1\n),\n···\n,emb(s\nT\n)).\n6WereferthereadertoAppendixDformoredetails.\n7\nPreprint. Underreview.\n5 RelatedWork\nRecent advances in controllable generation with LMs have spurred a wide range of ap-\nproaches, whichwesummarizebelow. Theseapproachescanberoughlyclassifiedinto",
    "char_length": 1490
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 17,
    "text": "threedifferentcategories: training-time,prompting,anddecoding-timeapproaches.\nTraining-timeapproaches. Asubsetoftheapproachesseekstoexertcontrolbyfine-tuning\norreinforcementlearningviasomesetofdatathatmorecloselymirrorsthetargettask,such\nasviareinforcementlearningfromhumanfeedback(RLHF)(Ziegleretal.,2020;Stiennon\net al., 2020b; Bai et al., 2022; Ouyang et al., 2022) or from symbolic knowledge (Ahmed\netal.,2023),buttheseapproachescomewithchallengessuchashyperparametersensitivity\nand distributional collapse (Zheng et al., 2023; Zhu et al., 2023; Xiong et al.). Some of\nthese drawbacks can be mitigated by utilizing on-policy data (Tajwar et al., 2024) and\nimposing a KL penalty that penalizes shifting an LM too far from its prior distribution,\ncastingoptimizationasvariationalinference(Korbaketal.,2022;Aminietal.,2025).\nPromptingapproaches. Anotherclassofapproachesfocusesonguidingthedistribution\nimplicitlyviamodificationsintheprompt(Ashok&Poczos,2024). Tothisend,controlcan\nbeexertedbyeitherverballyexpressingtheconstraintsintheprompt(Chenetal.,2022;\nZhouetal.,2023;Ashok&Poczos,2024),orthroughtheuseofexamples(Poesiaetal.,2022;\nZhouetal.,2023). Inadditiontointroducingminimalcomputationoverheadandproducing\ngoodqualitytext(Zhouetal.,2023;Ashok&Poczos,2024),promptingapproachesarealso\nmore flexible, since complex constraints can be easily integrated in the prompt without\nfurther training or expensive data curation. Nonetheless, constraint satisfiability using",
    "char_length": 1471
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 18,
    "text": "prompting-basedmethodsisnotguaranteed(Zhouetal.,2023)anddependsheavilyonthe\ninstructionfollowingcapabilitiesoftheLM(Jiangetal.,2024;Heetal.,2024).\nDecoding-timeapproaches. Apopulardecoding-timeapproachistoperformtoken-level\nmodificationsateachstepand,forthatreason,frequentlyreferredtoaslocallyconstrained\ndecoding (Loula et al., 2025). Methods to locally constrained decoding either mask out\nspecifictokensorheuristicallyreweightokenssuchthattheconstraintsaremorelikelytobe\nsatisfied. Examplesincludebanningspecificwords(Gehmanetal.,2020),usingcontext-free\ngrammars(Poesiaetal.,2022;Gengetal.,2023;Willard&Louf,2023;Beurer-Kellneretal.,\n2023; Lundberg et al., 2024; Beurer-Kellner et al., 2024), or through the combination of\nbooleanalgebrawithsearchalgorithms(Hokamp&Liu,2017;Andersonetal.,2017;Post&\nVilar,2018;Huetal.,2019;Luetal.,2021;2022;Qinetal.,2022). Note,however,thatwhile\nsettingtoken-levelrestrictionscanbeeffectiveatexertingsyntacticcontroloverLMs,these\nareinsufficienttocapturethericherandsubtlernuancesofsemanticconstraints.\nIn fact, semantic control approaches resort to attribute “scorers” to estimate how likely\nthe constraint is under a given input, and then use those estimates to reweigh the per-\ntokendistributionofthebaseLM.Previouslyproposedmethodsincludecombiningthe\nconditionaldistributionsofdifferentLMswithopposingbehaviors,suchasatoxicexpert\nandanon-toxicexpert(Schicketal.,2021;Liuetal.,2021;Lietal.,2023;Dekonincketal.,",
    "char_length": 1452
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 19,
    "text": "2024), and using an attribute discriminator (i.e., constraint verifier) to reweigh the base\nLMconditionaldistribution(Holtzmanetal.,2018;Krauseetal.,2021). Thegradientsof\nattributediscriminatorshavealsobeentoinducechangesthebaseLMthroughchanges\ntotheLMweights(Dathathrietal.,2020;Liuetal.,2020;Wallaceetal.,2019;Zhangetal.,\n2024b). Althougheffective,locallyconstraineddecodingapproachesoftenintroducegreedy\n(potentiallysub-optimal)approximationsthatdistortthedistribution(Loulaetal.,2025;\nMaetal.,2025). Conversely,sample-reweighapproachesconsistoffirstsamplingcomplete\nsequencesandthenreweighthemusingaconstraintverifier(Stiennonetal.,2020a;Krishna\netal.,2022;Sunetal.,2024;Ichiharaetal.,2025;Aminietal.,2025). Whileconstraintsare\nimposedgloballyinsamplereweighingapproaches,theydonotbenefitfromfiner-grained\nconstraintinformationduringgenerationand,hence,requirealargernumberofsamplesto\nfindhigh-qualitygenerationsthatcomplywiththeconstraints(Loulaetal.,2025).\nAnotherlineofworkperformsapproximateinferenceinexactmodelsviasampling(Miao\netal.,2019;Zhangetal.,2020;Kumaretal.,2022;Poesiaetal.,2022;Qinetal.,2022;Duetal.,\n2024),and,morerecently,viamoreeffectiveSequentialMonteCarlo(SMC)methods,which\nmaintainasetofsamplesthatevolvethroughtime. Theevolutionofthesamplesaccounts\n8\nPreprint. Underreview.\nTable 1: Evaluation of the quality and toxicity of Llama-3.2 (1B) generations when\nsteered to be non-toxic and toxic, respectively. Toxicity is evaluated on 400 prompts",
    "char_length": 1470
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 20,
    "text": "RealToxicityPrompts using the toxicity verifier ϕtoxicity (Logacheva et al., 2022). PPL\nreferstotheperplexityofMeta-Llama-3-70Bonthemodelgenerations. WereportExpected\nMaximumToxicity: themaximumtoxicityacrossgenerations,andToxicityProbability:\ntheprobabilityofatoxicgeneration,bothcomputedacross10generationsperprompt. We\nexpectbothmetricstobelower( )whensteeringthebaseLMtowardsnon-toxicgenerations\n↓\n(detoxify)andhigher( )whensteeringthebaseLMtowardsnon-toxicgenerations(toxify).\n↑\nToxicProb.( , ) Exp.Max.Toxicity( , )\nObjective Method ↓ ↑ ↓ ↑ PPL( )\n↓\nFull Non-toxic Toxic Full Non-toxic Toxic\nrandom 37.25 10.00 64.50 37.11 13.17 61.05 12.18\nbeamsearch 17.25 3.00 31.50 18.22 4.34 32.09 8.00\nBoN 2.75 1.00 4.50 4.90 1.91 7.89 15.46\ndetoxify\nSConE(ours) 00.25 00.50 00.00 01.85 1.30 2.40 14.88\nBoN 62.50 37.00 88.00 61.36 39.62 83.11 13.97\ntoxify\nSConE(ours) 93.75 88.00 99.50 91.15 85.75 96.55 23.87\nnotonlyforthesamplelikelihoodunderthebaseLM,butalsoforconstraintinformation\nthatcanbeprovidedeitherbylearnabletwistfunctions(Zhaoetal.,2024)orbyevaluating\ntheconstraintverifieronpartialsequences (Lewetal.,2023;Loulaetal.,2025).\n6 Experiments\nWeempiricallyevaluatetheeffectivenessoftheproposedmethodacrossnumerousopen-\nendedgenerationtasks,includingtextdetoxification,controlledsentimentgeneration,and\ntopicsteering. Section6introducesspecificdetailsofourmethods,baselines,andmetrics.\nTask-specificdetails, suchasdatasetandconstraintverifiers, andresultsforthetoxicity,",
    "char_length": 1474
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 21,
    "text": "sentiment,andtopicexperimentsaredescribedinSections6.1,6.2,C.1,respectively.\nExperimentalSetup\nBaselines. To validate our method, we compare it against two sampling-based base-\nlines: random,whichconsistsofsamplingoutputsautoregressivelyfromabaseLM,and\nbeamsearch,whichleveragesinformationaboutthetopKmostlikelycontinuationsundera\nbaseLMtogreedilyselectthenexttoken. Additionally,weevaluateBest-of-Nrejection\nsampling(BoN) (Stiennonetal.,2020a),apopulartraining-freemethodforlanguagemodel\ncontrolwhichhasbeenshowntobecompetitivetoRLHF-basedmethods(Aminietal.,2025).\nLikeourproposedmethod,BoNexploitsnon-lexicalconstraintverifierstoexertsemantic\ncontrolonthebaseLM.However,itdoessobyfirstsamplingNcontinuationsfromthebase\nLMandselectingonethatmaximizestheverifier.7 WerefertoAppendixBformoredetails.\nMetrics. In line with prior work (Gehman et al., 2020; Ahmed et al., 2025), we report\nPerplexity (PPL) as a measure of sample quality, specifically, we use Meta-Llama-3-70B.\nIntuitively,effectivecontrolmethodsshouldyieldgenerationsthatsatisfythecontraintbut\nthatarealsohighquality,i.e.,lowperplexity.\nThe primary constraint satisfaction metric that we report is the Average ϕ score. This\na\nmetriccanbedefinedastheaverageverifierscoreacrossallmodelgenerations. Intuitively,\nbecausethisverifierisbeingusedtosteercontrolduringgeneration,itcanbeinterpretedas\nthegroundtruthmeasureofthedesiredsemanticattributea(e.g.,toxicity,positivesentiment,",
    "char_length": 1439
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 22,
    "text": "topic). As such, we expect effective control methods to achieve high Average ϕ scores,\na\nespeciallywhencomparedtouncontrolledbaselineslikerandom.\n7Forafaircomparison,weusethesamedecodingsettingsasinourmethod’sinitialization.\n9\nPreprint. Underreview.\nTable2:EvaluationofqualityandsentimentofGPT2-IMDBgenerationswhensteeredusing\napositivesentimentconstraintϕsentiment. Sentimentisevaluatedon600promptsfromthe\nIMDBtestsetusingasentimentverifier(Maasetal.,2011),spanningequalnumberofpositive\nandnegativereviews. ResultsarediscriminatedbytheFullsetofprompts,theNegative\nsubset,andthePositivesubset. Allmetricsarecalculatedusing10differentgenerations\nperprompt. PPLreferstotheperplexityofMeta-Llama-3-70Bonthemodelgenerations\nusing10differentseeds;InlinewithRafailovetal.(2023);Aminietal.(2025),wereport\ntheaveragesentimentscore,thesentimentscoreisgreaterthan0.8in9out10generations\n(SentimentProb.),andtheexpectedminimumsentimentscore(Exp. Min. Sentiment).\nAvgϕsentiment( ) SentimentProb.( ) Exp.Min.Sentiment( ) PPL( )\n↑ ↑ ↑ ↓\nMethod Full Neg Pos Full Neg Pos Full Neg Pos Full\nrandom 57.10 53.16 61.04 95.50 95.33 95.67 12.83 10.78 14.87 21.18\nbeamsearch 58.83 50.83 66.82 58.83 48.33 69.33 44.46 37.21 51.71 3.96\nBoN 60.66 55.17 66.14 95.83 93.33 98.33 15.24 11.70 18.77 10.84\nSConE(ours) 93.06 92.73 93.37 100.00 100.00 100.00 84.50 83.18 85.82 20.96\nAsadditionalmeasuresofconstraintsatisfaction,wereportmetricsthatcapturetheexpected",
    "char_length": 1432
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 23,
    "text": "worst-case and theempirical probability of constraint satisfaction (Gehmanet al., 2020).\nAssumingthateachpromptxisassociatedwithmultiplegenerations,theexpectedworst\nscoremetriciscalculatedbycomputingtheworstconstraintscoreϕ acrossallgenerations\na\nforx,and,thentakingtheaverageoverallevaluationprompts. Similarly,theconstraint\nprobabilitymetricrepresentsthefractionofevaluatedpromptsforwhichatleastoneofits\ngenerationssatisfiestheconstraintaboveauser-definedthreshold(i.e.,1[ϕ (y) τ ]).\na a\n≥\n6.1 ControlledToxicityGeneration\nInthissection,wecomparetheperformanceofdifferentmethodsinsteeringthetoxicityof\nasmallLlama-3.2 (1B)(Grattafiorietal.,2024). WedosobypromptingtheLMwith400\nnaturaloccurringpromptsfromRealToxicityPrompts(Gehmanetal.,2020). Werandomly\nselect200Toxicand200Non-toxicpromptsfromRealToxicityPromptsandusetheminboth\ntoxification and detoxification settings, sampling 10 generations of up to 25 tokens per\nprompt. EvaluationandtoxicitysteerabilityarebothconductedusingaRoBERTa-based\nbinaryclassifierϕtoxicity,finetunedfortoxicitydetection(Logachevaetal.,2022). Tosteer\nmodelstogeneratenon-toxicoutputs,wesetthemtomaximize1 ϕtoxicity.\n−\nDetoxificationTask. Table1summarizestheresultsforthedetoxificationtask,discrim-\ninatedbyprompttype. Intuitively,effectivesemanticcontrolmethodsshouldbeableto\ngenerate non-toxic outputs, i.e., minimize the toxicity metrics, irrespective of the toxic-\nity of the prompt type. Overall, we observe that the uncontrolled baselines random and",
    "char_length": 1488
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 24,
    "text": "beamsearch, stillleadtotoxiccontinuationsevenwhenpromptedwithnon-toxicinputs.\nWhilebeamsearchseemstolowerbothtoxicityandperplexity,wefindthatthisisexplained\nbydegenerateoutputscharacterizedbyrepetition(Holtzmanetal.,2020). Contrastingly,\nwe find that BoN is very effective at detoxifying LM generations: reducing the average\nworst-casetoxicitydown4.90withminimalpenaltyinperplexity(3.28points). Whilethis\nrepresentsabigimprovementovertheuncontrolledbaselines,wefindthatourmethodis\nabletofurtherachievea3-foldreductionintermsoftheaverageworstcasetoxicityintoxic\npromptandreducetheprobabilityofatoxicgenerationtoanegligibleamount(upto0.50).\nToxificationTask. Wenowmovetotheoppositetask: givenanaturallyoccurringprompt,\naremethodsabletosteerthebaseLMtowardsmoretoxicinputs?Table6.1showsthetoxicity\nresults for the semantic control methods. While both methods are able to substantially\nincrease both the worst-case toxicity and the likelihood of sampling toxic outputs from\nLlama-3.2 (1B),wefindthatSConEsystematicallyisfarmoreeffectivethanBoNwith+30%\ngaptoxicityincreaseacrossbothtoxicitymetrics. Muchofthisperformancegapappearsto\nstemfromthenon-toxicsubset,forwhichthebaseLMislesspredisposedtogeneratetoxic\noutputs. Assuch,methodslikerejectionsamplingthatusetheconstraintverifierϕtoxicity\n10\nPreprint. Underreview.\nTable3: EvaluationofqualityandtopicadherenceofLlama-3.2 (1B)generationswhen\ncontrolledforspecifictopics. Topicadherenceisevaluatedon300promptsspanning6",
    "char_length": 1465
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 25,
    "text": "topicsϕtopic(Wettigetal.,2025). Wereportperplexity(PPL),theaverageϕtopicscore(Avg\nϕtopic),thefractionofexamplesforwhichthetopicscoreisgreaterthan0.8in90%ormore\nofthegenerations(TopicProb.),andtheexpectedminimumtopicscore(Exp. Min. Topic).\nMethod TopicProb. ( ) Exp. Min. Topic( ) Avgϕtopic( ) PPL\n↑ ↑ ↑\nrandom 86.20 83.91 91.87 6.16\nbeamsearch 87.47 90.35 91.63 3.78\nBoN 95.40 95.18 97.52 8.42\nSConE(ours) 98.40 96.71 99.07 7.39\ntorerankthebaseLMgenerationsarelesslikelytosucceedforlowprobabilitysemantic\nconstraints. ThisalsoprovidesanexplanationfortheincreaseinperplexityforSConE.\n6.2 ControlledSentimentGeneration\nNext,wecomparethesteerabilityofthedifferentmethodswhengeneratingreviewswith\npositivesentiment(Rafailovetal.,2023;Zhaoetal.,2024;Aminietal.,2025). Focusingon\nmoviereviews,wepromptGPT2-IMDBwith600arbitrarilychosenpromptsfromtheIMDB\ntestset(Maasetal.,2011). Buildingonpreviouswork(Rafailovetal.,2023;Aminietal.,\n2025),weusetheoriginalreviewsintheIMDBdatasettocreatethepromptsbyrandomly\nsplittingthemintoprefixesof2to8words. WealsoadoptthesameBERT-basedclassifier\nas our sentiment verifier ϕsentiment.8 Given that this model was fine-tuned on the IMDB\ntrainingdata,weexpectittobeastrongandreliablesentimentpredictorforthistask.\nPositiveMovieReviewGenerationTask. Inthecontextofpositivemoviereviewgen-\neration,wewouldliketoensurethatmostofGPT2-IMDB’sgenerationsarepositive.9 Once\nmore,asobservedinTable2,theuncontrolledbaselines—randomandbeamsearch—struggle",
    "char_length": 1470
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 26,
    "text": "togeneratepositivereviews. Specifically,asemphasizedbytheworstcasemetric,Expected\nMinimumSentiment,GPT2-IMDB-generatedreviewswithnocontrolcanbefairlynegative\n(<52acrossallprompts),especiallyinthenegativesubset(<38%).BoNdrasticallyimproves\nupontheuncontrolledbaselines,increasingtheSentimentProbabilitytoabout70.83%and\nimprovingtheaveragelowestsentimentscoreto70.79%. Still,wefindthatSConEfurther\nimproves(about14%pointsaverageimprovementinbothmetrics)theoverallworst-case\nsentimentandthechancesofproducingpositivereviewsatleast90%ofthetime.\n6.3 ControlledTopicGeneration\nLastly,weevaluatethemethodsontheirabilitytocontrolforthetopicofLMgenerations.\nWechoose6diversetopicsfromtherecentlytaxonomyconcerningthewebstructure(Wettig\netal.,2025),includingfreque(e.g.,Finance&BusinessandPolitics)andlessfrequenttopics\n(e.g., History, Industrial). For each topic, we randomly select 50 different examples from\ntheTopicAnnotations-Llama-3.1-405B-FP8(Wettigetal.,2025)testset,breakingtheminto\nprefixesof8to12words. Eachprefixisusedtosampleamaximumof60tokens.\nTopicGenerationTask. Ingeneral,wefindthatuncontrolledbaselinesachieveafairlyhigh\naverageconstraintscore( 91%),whichmaybeexplainedbytheuseoflongerprefixes\n≥\nduringgeneration. Wefindthistobethecaseformostexamples(seeexamplesinAppendix\nC.1). Nonetheless,thediscrepancybetweenuncontrolledandcontrolledmethodsisstill\nvisiblewiththelatterachieving7%-8%higheraverageconstraintscores. Remarkably,we",
    "char_length": 1439
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 27,
    "text": "findSConEisnotonlyabletoimproveuponBoN,achievinganaveragescoreof98.89%but\nalsoproduceshigherqualitygenerationsasemphasizedbythelowerperplexity.\n8https://huggingface.co/lvwerra/distilbert-imdb\n9Inlinewith Maasetal.(2011),weconsiderareviewtobepositiveiffϕsentiment(y) 0.8.\n≥\n11\nPreprint. Underreview.\n7 Conclusion\nInthispaper,weintroducedatraining-freeapproachtosemanticcontrolofautoregressive\nlanguage models. Our approach uses exact inference on an approximate distribution\ninduced by an LM generation, using first-order information from a verifier to compute\nthe expected constraint satisfaction for each of the possible next tokens. Our approach\ndemonstratedasubstantialimprovementcomparedtopreviousapproachonthetasksof\ncontrollingthetoxicity,sentimentandtopicofLMgenerations.\nEthicsStatement\nOurworkinvestigatestheproblemofexertingsemanticcontroloverLMgenerations. While\nourmethodcanbeverysocietallybeneficial,givingusmorecontroloverlanguagemodels,\nweacknowledgethatourmethodcouldbemisusedtoproduceharmfulcontent. Welook\nforwardtoexploringfutureworkthatplacesguardrailsonLMstopreventthesepitfalls.\nAcknowledgments\nThisworkissupportedbytheDARPAANSRprogramFA8750-23-2-0004,anNSFCAREER\nawardnumberIIS-2046873andanNSFawardnumber1900644. Theconclusionsareofthe\nauthorsanddonotreflecttheofficialpolicyorpositionofDARPAortheU.S.Government.\nStatementofAuthorContributions\nKareemAhmed: Conceivedanddevelopedthecoreresearchideaandtheproposedap-",
    "char_length": 1438
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 28,
    "text": "proach. Wrote the introduction and technical sections of the paper. Wrote the code for\ncomputingtheexpectedembeddingandcontributedtodebuggingtheoverallapproach.\nCatarinaG.Belem: Implementedtheprimarycodebase. Conductedallexperimentsand\nwrotethecorrespondingexperimentalsectionofthepaperinadditiontotherelatedworks.\nPadhraic Smyth and Sameer Singh: Senior project leadership. Provided mentorship,\nsupervision,andadvisorysupportthroughouttheproject. Offeredcriticalfeedbackonthe\nmethodologyandthemanuscript. Allauthorsreadandapprovedthefinalmanuscript.\nReferences\nJoshAchiam,StevenAdler,SandhiniAgarwal,etal. GPT4TechnicalReport,2024.\nKareemAhmed,StefanoTeso,Kai-WeiChang,GuyVandenBroeck,andAntonioVergari.\nSemanticprobabilisticlayersforneuro-symboliclearning. InNeurIPS,2022.\nKareemAhmed, Kai-WeiChang, andGuyVandenBroeck. Apseudo-semanticlossfor\nautoregressivemodelswithlogicalconstraints. InThirty-seventhConferenceonNeuralIn-\nformationProcessingSystems,2023. URLhttps://openreview.net/forum?id=hVAla2O73O.\nKareemAhmed, Kai-WeiChang, andGuyVandenBroeck. Controllablegenerationvia\nlocally constrained resampling. In The Thirteenth International Conference on Learning\nRepresentations,2025. URLhttps://openreview.net/forum?id=8g4XgC8HPF.\nAfraAmini,TimVieira,ElliottAsh,andRyanCotterell. Variationalbest-of-nalignment.\nInTheThirteenthInternationalConferenceonLearningRepresentations, 2025. URLhttps:\n//openreview.net/forum?id=W9FZEQj3vv.",
    "char_length": 1436
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 29,
    "text": "Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Guided open\nvocabularyimagecaptioningwithconstrainedbeamsearch. InMarthaPalmer,Rebecca\nHwa,andSebastianRiedel(eds.),Proceedingsofthe2017ConferenceonEmpiricalMethods\nin Natural Language Processing, pp. 936–945, Copenhagen, Denmark, September 2017.\n12\nPreprint. Underreview.\nAssociation for Computational Linguistics. doi: 10.18653/v1/D17-1098. URL https:\n//aclanthology.org/D17-1098/.\nDhananjayAshokandBarnabasPoczos. Controllabletextgenerationintheinstruction-\ntuningera,2024. URLhttps://arxiv.org/abs/2405.01490.\nYuntaoBai,SauravKadavath,SandipanKundu,AmandaAskell,JacksonKernion,Andy\nJones,AnnaChen,AnnaGoldie,AzaliaMirhoseini,CameronMcKinnon,CarolChen,\nCatherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli,\nDustinLi,EliTran-Johnson,EthanPerez,JamieKerr,JaredMueller,JeffreyLadish,Joshua\nLandau,KamalNdousse,KamileLukosuite,LianeLovitt,MichaelSellitto,NelsonElhage,\nNicholasSchiefer,NoemiMercado,NovaDasSarma,RobertLasenby,RobinLarson,Sam\nRinger,ScottJohnston,ShaunaKravec,SheerElShowk,StanislavFort,TameraLanham,\nTimothyTelleen-Lawton,TomConerly,TomHenighan,TristanHume,SamuelR.Bowman,\nZacHatfield-Dodds,BenMann,DarioAmodei,NicholasJoseph,SamMcCandlish,Tom\nBrown,andJaredKaplan. Constitutionalai: Harmlessnessfromaifeedback,2022. URL\nhttps://arxiv.org/abs/2212.08073.\nJulianBesag. Statisticalanalysisofnon-latticedata. JournaloftheRoyalStatisticalSociety.\nSeriesD(TheStatistician),pp.pp.179–195,1975.",
    "char_length": 1495
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 30,
    "text": "LucaBeurer-Kellner,MarcFischer,andMartinVechev. Promptingisprogramming: Aquery\nlanguageforlargelanguagemodels. Proc.ACMProgram.Lang.,7(PLDI),June2023. doi:\n10.1145/3591300. URLhttps://doi.org/10.1145/3591300.\nLucaBeurer-Kellner,MarcFischer,andMartinVechev. Guidingllmstherightway: fast,\nnon-invasiveconstrainedgeneration. InProceedingsofthe41stInternationalConferenceon\nMachineLearning,ICML’24.JMLR.org,2024.\nHowardChen,HuihanLi,DanqiChen,andKarthikNarasimhan. Controllabletextgenera-\ntionwithlanguageconstraints,2022. URLhttps://arxiv.org/abs/2212.10466.\nYooJungChoi,AntonioVergari,andGuyVandenBroeck. Probabilisticcircuits: Aunifying\nframeworkfortractableprobabilisticmodeling. 2020.\nAdnanDarwicheandPierreMarquis. Aknowledgecompilationmap. JournalofArtificial\nIntelligenceResearch,17:229–264,2002.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino,\nJasonYosinski,andRosanneLiu. Plugandplaylanguagemodels: Asimpleapproach\ntocontrolledtextgeneration. InInternationalConferenceonLearningRepresentations,2020.\nURLhttps://openreview.net/forum?id=H1edEyBKDS.\nJasperDekoninck,MarcFischer,LucaBeurer-Kellner,andMartinVechev. Controlledtext\ngeneration via language model arithmetic. In The Twelfth International Conference on\nLearningRepresentations,2024. URLhttps://openreview.net/forum?id=SLw9fp4yI6.\nLiDu,AfraAmini,LucasTorrobaHennigen,XinyanVelocityYu,HoldenLee,JasonEisner,\nandRyanCotterell. Principledgradient-basedMCMCforconditionalsamplingoftext.",
    "char_length": 1482
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 31,
    "text": "In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver,\nJonathanScarlett,andFelixBerkenkamp(eds.),Proceedingsofthe41stInternationalConfer-\nenceonMachineLearning,volume235ofProceedingsofMachineLearningResearch,pp.11663–\n11685.PMLR,21–27Jul2024. URLhttps://proceedings.mlr.press/v235/du24a.html.\nSamuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith.\nRealToxicityPrompts: Evaluating neural toxic degeneration in language models. In\nTrevor Cohn, Yulan He, and Yang Liu (eds.), Findings of the Association for Compu-\ntational Linguistics: EMNLP 2020, pp. 3356–3369, Online, November 2020. Associa-\ntionforComputationalLinguistics. doi: 10.18653/v1/2020.findings-emnlp.301. URL\nhttps://aclanthology.org/2020.findings-emnlp.301/.\n13\nPreprint. Underreview.\nSaiboGeng,MartinJosifoski,MaximePeyrard,andRobertWest. Grammar-constrained\ndecodingforstructuredNLPtaskswithoutfinetuning.InHoudaBouamor,JuanPino,and\nKalikaBali(eds.),Proceedingsofthe2023ConferenceonEmpiricalMethodsinNaturalLanguage\nProcessing,pp.10932–10952,Singapore,December2023.AssociationforComputational\nLinguistics. doi: 10.18653/v1/2023.emnlp-main.674. URLhttps://aclanthology.org/\n2023.emnlp-main.674/.\nSaibo Geng, Hudson Cooper, Michał Moskal, Samuel Jenkins, Julian Berman, Nathan\nRanchin, Robert West, Eric Horvitz, and Harsha Nori. Jsonschemabench: A rigorous\nbenchmarkofstructuredoutputsforlanguagemodels,2025. URLhttps://arxiv.org/\nabs/2501.10868.",
    "char_length": 1465
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 32,
    "text": "AaronGrattafiori,AbhimanyuDubey,AbhinavJauhri,etal. Thellama3herdofmodels,\n2024. URLhttps://arxiv.org/abs/2407.21783.\nQianyuHe,JieZeng,QianxiHe,JiaqingLiang,andYanghuaXiao. Fromcomplextosim-\nple: Enhancingmulti-constraintcomplexinstructionfollowingabilityoflargelanguage\nmodels. InYaserAl-Onaizan,MohitBansal,andYun-NungChen(eds.),Findingsofthe\nAssociationforComputationalLinguistics: EMNLP2024,pp.10864–10882,Miami,Florida,\nUSA,November2024.AssociationforComputationalLinguistics. doi: 10.18653/v1/2024.\nfindings-emnlp.637. URLhttps://aclanthology.org/2024.findings-emnlp.637/.\nChrisHokampandQunLiu. Lexicallyconstraineddecodingforsequencegenerationusing\ngrid beam search. In Regina Barzilay and Min-Yen Kan (eds.), Proceedings of the 55th\nAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),pp.\n1535–1546,Vancouver,Canada,July2017.AssociationforComputationalLinguistics. doi:\n10.18653/v1/P17-1141. URLhttps://aclanthology.org/P17-1141/.\nAriHoltzman,JanBuys,MaxwellForbes,AntoineBosselut,DavidGolub,andYejinChoi.\nLearningtowritewithcooperativediscriminators. InIrynaGurevychandYusukeMiyao\n(eds.),Proceedingsofthe56thAnnualMeetingoftheAssociationforComputationalLinguistics\n(Volume1: LongPapers),pp.1638–1649,Melbourne,Australia,July2018.Associationfor\nComputational Linguistics. doi: 10.18653/v1/P18-1152. URL https://aclanthology.\norg/P18-1152/.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of",
    "char_length": 1457
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 33,
    "text": "neuraltextdegeneration. InInternationalConferenceonLearningRepresentations,2020. URL\nhttps://openreview.net/forum?id=rygGQyrFvH.\nJ.EdwardHu,HudaKhayrallah,RyanCulkin,PatrickXia,TongfeiChen,MattPost,and\nBenjaminVanDurme. Improvedlexicallyconstraineddecodingfortranslationandmono-\nlingualrewriting. InJillBurstein,ChristyDoran,andThamarSolorio(eds.),Proceedings\nof the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: HumanLanguageTechnologies,Volume1(LongandShortPapers),pp.839–850,\nMinneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi:\n10.18653/v1/N19-1090. URLhttps://aclanthology.org/N19-1090/.\nYukiIchihara,YuuJinnai,TetsuroMorimura,KenshiAbe,KaitoAriu,MitsukiSakamoto,\nandEijiUchibe.Evaluationofbest-of-nsamplingstrategiesforlanguagemodelalignment.\nTransactionsonMachineLearningResearch,2025.ISSN2835-8856.URLhttps://openreview.\nnet/forum?id=H4S4ETc8c9.\nYuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng\nShang, Xin Jiang, Qun Liu, and Wei Wang. FollowBench: A multi-level fine-grained\nconstraints following benchmark for large language models. In Lun-Wei Ku, Andre\nMartins,andVivekSrikumar(eds.),Proceedingsofthe62ndAnnualMeetingoftheAssociation\nforComputationalLinguistics(Volume1: LongPapers),pp.4667–4688,Bangkok,Thailand,\nAugust2024.AssociationforComputationalLinguistics. doi: 10.18653/v1/2024.acl-long.\n257. URLhttps://aclanthology.org/2024.acl-long.257/.",
    "char_length": 1474
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 34,
    "text": "PashaKhosravi,YitaoLiang,YooJungChoi,andGuyVanDenBroeck. Whattoexpectof\nclassifiers? reasoningaboutlogisticregressionwithmissingfeatures. InProceedingsofthe\n28thInternationalJointConferenceonArtificialIntelligence,IJCAI’19,2019.\n14\nPreprint. Underreview.\nDaphneKollerandNirFriedman. Probabilisticgraphicalmodels: principlesandtechniques. MIT\npress,2009.\nTerryKoo,FrederickLiu,andLuhengHe. Automata-basedconstraintsforlanguagemodel\ndecoding,2024.\nTomasz Korbak, Ethan Perez, and Christopher Buckley. RL with KL penalties is better\nviewed as Bayesian inference. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang\n(eds.),FindingsoftheAssociationforComputationalLinguistics: EMNLP2022,2022.\nBenKrause,AkhileshDeepakGotmare,BryanMcCann,NitishShirishKeskar,ShafiqJoty,\nRichardSocher, andNazneenFatemaRajani. GeDi: Generativediscriminatorguided\nsequence generation. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and\nScottWen-tauYih(eds.),FindingsoftheAssociationforComputationalLinguistics: EMNLP\n2021, pp. 4929–4952, Punta Cana, Dominican Republic, November 2021. Association\nforComputationalLinguistics. doi: 10.18653/v1/2021.findings-emnlp.424. URLhttps:\n//aclanthology.org/2021.findings-emnlp.424/.\nKalpeshKrishna,YapeiChang,JohnWieting,andMohitIyyer. RankGen: Improvingtext\ngenerationwithlargerankingmodels. InYoavGoldberg,ZornitsaKozareva,andYue\nZhang(eds.),Proceedingsofthe2022ConferenceonEmpiricalMethodsinNaturalLanguage",
    "char_length": 1432
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 35,
    "text": "Processing,pp.199–232,AbuDhabi,UnitedArabEmirates,December2022.Association\nfor Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.15. URL https:\n//aclanthology.org/2022.emnlp-main.15/.\nSachin Kumar, Biswajit Paria, and Yulia Tsvetkov. Gradient-based constrained sam-\npling from language models. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang\n(eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Pro-\ncessing, pp. 2251–2277, Abu Dhabi, United Arab Emirates, December 2022. Associa-\ntion for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.144. URL\nhttps://aclanthology.org/2022.emnlp-main.144/.\nYanivLeviathan,MatanKalman,andYossiMatias. Fastinferencefromtransformersvia\nspeculativedecoding.InProceedingsofthe40thInternationalConferenceonMachineLearning,\n2023.\nAlexanderK.Lew,TanZhi-Xuan,GabrielGrand,andVikashK.Mansinghka. Sequential\nmontecarlosteeringoflargelanguagemodelsusingprobabilisticprograms,2023. URL\nhttps://arxiv.org/abs/2306.03081.\nXiangLisaLi,AriHoltzman,DanielFried,PercyLiang,JasonEisner,TatsunoriHashimoto,\nLuke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text gener-\nation as optimization. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki\n(eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pp. 12286–12312, Toronto, Canada, July 2023. As-\nsociation for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.687. URL",
    "char_length": 1489
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 36,
    "text": "https://aclanthology.org/2023.acl-long.687/.\nAlisaLiu,MaartenSap,XimingLu,SwabhaSwayamdipta,ChandraBhagavatula,NoahA.\nSmith,andYejinChoi. DExperts: Decoding-timecontrolledtextgenerationwithexperts\nand anti-experts. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.),\nProceedingsofthe59thAnnualMeetingoftheAssociationforComputationalLinguisticsandthe\n11thInternationalJointConferenceonNaturalLanguageProcessing(Volume1: LongPapers),\npp. 6691–6706, Online, August 2021. Association for Computational Linguistics. doi:\n10.18653/v1/2021.acl-long.522. URLhttps://aclanthology.org/2021.acl-long.522/.\nMichaelXieyangLiu,FrederickLiu,AlexFiannaca,TerryKoo,LucasDixon,MichaelTerry,\nandCarrieCai. “weneedstructuredoutput”’: Towardsuser-centeredconstraintsonlarge\nlanguagemodeloutput. pp. 9,2024.\nRuiboLiu,GuangxuanXu,ChenyanJia,WeichengMa,LiliWang,andSoroushVosoughi.\nDataboost: Textdataaugmentationthroughreinforcementlearningguidedconditional\n15\nPreprint. Underreview.\ngeneration. InBonnieWebber,TrevorCohn,YulanHe,andYangLiu(eds.),Proceedings\nofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pp.\n9031–9041, Online, November 2020. Association for Computational Linguistics. doi:\n10.18653/v1/2020.emnlp-main.726. URLhttps://aclanthology.org/2020.emnlp-main.\n726/.\nVarvaraLogacheva,DarynaDementieva,SergeyUstyantsev,DaniilMoskovskiy,DavidDale,\nIrinaKrotova,NikitaSemenov,andAlexanderPanchenko.ParaDetox:Detoxificationwith",
    "char_length": 1450
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 37,
    "text": "paralleldata. InProceedingsofthe60thAnnualMeetingoftheAssociationforComputational\nLinguistics(Volume1: LongPapers),pp.6804–6818,Dublin,Ireland,May2022.Association\nforComputationalLinguistics. URLhttps://aclanthology.org/2022.acl-long.469.\nJoa˜oLoula,BenjaminLeBrun,LiDu,BenLipkin,ClementePasti,GabrielGrand,Tianyu\nLiu,YahyaEmara,MarjorieFreedman,JasonEisner,RyanCotterell,VikashMansinghka,\nAlexanderK.Lew,TimVieira,andTimothyJ.O’Donnell.Syntacticandsemanticcontrolof\nlargelanguagemodelsviasequentialmontecarlo.InTheThirteenthInternationalConference\nonLearningRepresentations,2025. URLhttps://openreview.net/forum?id=xoXn62FzD0.\nXimingLu, PeterWest, RowanZellers, RonanLeBras, ChandraBhagavatula, andYejin\nChoi. NeuroLogicdecoding: (un)supervisedneuraltextgenerationwithpredicatelogic\nconstraints. InKristinaToutanova,AnnaRumshisky,LukeZettlemoyer,DilekHakkani-\nTur,IzBeltagy,StevenBethard,RyanCotterell,TanmoyChakraborty,andYichaoZhou\n(eds.),Proceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationfor\nComputationalLinguistics: HumanLanguageTechnologies,pp.4288–4299,Online,June2021.\nAssociationforComputationalLinguistics. doi: 10.18653/v1/2021.naacl-main.339. URL\nhttps://aclanthology.org/2021.naacl-main.339/.\nXimingLu,SeanWelleck,PeterWest,LiweiJiang,JungoKasai,DanielKhashabi,Ronan\nLeBras,LianhuiQin,YoungjaeYu,RowanZellers,NoahA.Smith,andYejinChoi. Neuro-\nLogica*esquedecoding:Constrainedtextgenerationwithlookaheadheuristics.InMarine",
    "char_length": 1456
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 38,
    "text": "Carpuat,Marie-CatherinedeMarneffe,andIvanVladimirMezaRuiz(eds.),Proceedings\nof the 2022 Conference of the North American Chapter of the Association for Computational\nLinguistics: HumanLanguageTechnologies,pp.780–799,Seattle,UnitedStates,July2022.\nAssociationforComputationalLinguistics. doi: 10.18653/v1/2022.naacl-main.57. URL\nhttps://aclanthology.org/2022.naacl-main.57/.\nScottLundberg,MarcoRibeiro,RichardEdgar,andHarsha-Nori. Guidance: aguidance\nlanguageforcontrollinglargelanguagemodels.,2024.\nChangMa,HaitengZhao,JunleiZhang,JunxianHe,andLingpengKong. Non-myopic\ngenerationoflanguagemodelsforreasoningandplanning. InTheThirteenthInternational\nConference on Learning Representations, 2025. URL https://openreview.net/forum?id=\nOoNazl6T7D.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and\nChristopher Potts. Learning word vectors for sentiment analysis. In Dekang Lin,\nYuji Matsumoto, and Rada Mihalcea (eds.), Proceedings of the 49th Annual Meeting of\ntheAssociationforComputationalLinguistics: HumanLanguageTechnologies, pp.142–150,\nPortland, Oregon, USA, June 2011. Association for Computational Linguistics. URL\nhttps://aclanthology.org/P11-1015/.\nNingMiao, HaoZhou, LiliMou, RuiYan, andLeiLi. Cgmh: constrainedsentencegen-\neration by metropolis-hastings sampling. In Proceedings of the Thirty-Third AAAI Con-\nference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelli-",
    "char_length": 1445
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 39,
    "text": "gence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intel-\nligence, AAAI’19/IAAI’19/EAAI’19. AAAI Press, 2019. ISBN 978-1-57735-809-1. doi:\n10.1609/aaai.v33i01.33016834. URLhttps://doi.org/10.1609/aaai.v33i01.33016834.\nNguyenNhatMinh,AndrewBaker,ClementNeo,AllenGRoush,AndreasKirsch,and\nRavidShwartz-Ziv. Turninguptheheat: Min-psamplingforcreativeandcoherentLLM\noutputs. InTheThirteenthInternationalConferenceonLearningRepresentations,2025. URL\nhttps://openreview.net/forum?id=FBkpCyujtS.\n16\nPreprint. Underreview.\nFengNiu,BenjaminRecht,ChristopherRe,andStephenJ.Wright. Hogwild! alock-free\napproachtoparallelizingstochasticgradientdescent.InProceedingsofthe25thInternational\nConferenceonNeuralInformationProcessingSystems,2011.\nLongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,\nChongZhang,SandhiniAgarwal,KatarinaSlama,AlexRay,JohnSchulman,JacobHilton,\nFraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F\nChristiano,JanLeike,andRyanLowe. Traininglanguagemodelstofollowinstructions\nwithhumanfeedback. InAdvancesinNeuralInformationProcessingSystems,2022.\nRobertPeharz,StevenLang,AntonioVergari,KarlStelzner,AlejandroMolina,MartinTrapp,\nGuy Van den Broeck, Kristian Kersting, and Zoubin Ghahramani. Einsum networks:\nFastandscalablelearningoftractableprobabilisticcircuits. InInternationalConferenceof\nMachineLearning,2020.\nGabrielPoesia, AlexPolozov, VuLe, AshishTiwari, GustavoSoares, ChristopherMeek,",
    "char_length": 1483
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 40,
    "text": "andSumitGulwani. Synchromesh: Reliablecodegenerationfrompre-trainedlanguage\nmodels. In International Conference on Learning Representations, 2022. URL https://\nopenreview.net/forum?id=KmtVD97J43e.\nMattPostandDavidVilar. Fastlexicallyconstraineddecodingwithdynamicbeamallo-\ncationforneuralmachinetranslation. InMarilynWalker,HengJi,andAmandaStent\n(eds.), Proceedings of the 2018 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp.\n1314–1324,NewOrleans,Louisiana,June2018.AssociationforComputationalLinguistics.\ndoi: 10.18653/v1/N18-1119. URLhttps://aclanthology.org/N18-1119/.\nLianhuiQin,SeanWelleck,DanielKhashabi,andYejinChoi.COLDdecoding:Energy-based\nconstrainedtextgenerationwithlangevindynamics. InAliceH.Oh, AlekhAgarwal,\nDanielleBelgrave,andKyunghyunCho(eds.),AdvancesinNeuralInformationProcessing\nSystems,2022. URLhttps://openreview.net/forum?id=TiZYrQ-mPup.\nRafaelRafailov,ArchitSharma,EricMitchell,ChristopherDManning,StefanoErmon,and\nChelseaFinn. Directpreferenceoptimization: Yourlanguagemodelissecretlyareward\nmodel. InThirty-seventhConferenceonNeuralInformationProcessingSystems,2023. URL\nhttps://openreview.net/forum?id=HPuSIXJaa9.\nDan Roth. On the hardness of approximate reasoning. In IJCAI, pp. 613–619. Morgan\nKaufmann,1993.\nChristopherDeSa,ChrisRe,andKunleOlukotun. Ensuringrapidmixingandlowbias\nforasynchronousgibbssampling. InProceedingsofThe33rdInternationalConferenceon",
    "char_length": 1489
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 41,
    "text": "MachineLearning.\nTimo Schick, Sahana Udupa, and Hinrich Schu¨tze. Self-diagnosis and self-debiasing:\nA proposal for reducing corpus-based bias in NLP. Transactions of the Association for\nComputational Linguistics, 9:1408–1424, 2021. doi: 10.1162/tacl a 00434. URL https:\n//aclanthology.org/2021.tacl-1.84/.\nWeijiaShi,AndyShih,AdnanDarwiche,andArthurChoi. Ontractablerepresentationsof\nbinaryneuralnetworks. InProceedingsofthe17thInternationalConferenceonPrinciplesof\nKnowledgeRepresentationandReasoning(KR),2020.\nAlexanderSmolaandShravanNarayanamurthy. Anarchitectureforparalleltopicmodels.\nProceedingsofVLDBEndow.,2010.\nNisanStiennon,LongOuyang,JeffWu,DanielM.Ziegler,RyanLowe,ChelseaVoss,Alec\nRadford, Dario Amodei, and Paul Christiano. Learning to summarize from human\nfeedback. In Proceedings of the 34th International Conference on Neural Information Pro-\ncessingSystems,NeurIPS’20,RedHook,NY,USA,2020a.CurranAssociatesInc. ISBN\n9781713829546.\n17\nPreprint. Underreview.\nNisanStiennon,LongOuyang,JeffreyWu,DanielZiegler,RyanLowe,ChelseaVoss,Alec\nRadford, Dario Amodei, and Paul F Christiano. Learning to summarize with human\nfeedback. InAdvancesinNeuralInformationProcessingSystems,2020b.\nHanshiSun,MominHaider,RuiqiZhang,HuitaoYang,JiahaoQiu,MingYin,MengdiWang,\nPeterBartlett,andAndreaZanette. Fastbest-of-ndecodingviaspeculativerejection,2024.\nURLhttps://arxiv.org/abs/2410.20290.\nJiaoSun,YufeiTian,WangchunshuZhou,NanXu,QianHu,RahulGupta,JohnFrederick",
    "char_length": 1458
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 42,
    "text": "Wieting,NanyunPeng,andXuezheMa. Evaluatinglargelanguagemodelsoncontrolled\ngenerationtasks,2023.\nFahimTajwar,AnikaitSingh,ArchitSharma,RafaelRafailov,JeffSchneider,TengyangXie,\nStefanoErmon,ChelseaFinn,andAviralKumar. Preferencefine-tuningofllmsshould\nleveragesuboptimal,on-policydata. InProceedingsofthe41stInternationalConferenceon\nMachineLearning,2024.\nAntonioVergari,NicolaDiMauro,andFlorianaEsposito. Simplifying,regularizingand\nstrengtheningsum-productnetworkstructurelearning. InJointEuropeanConferenceon\nMachineLearningandKnowledgeDiscoveryinDatabases,pp.343–358.Springer,2015.\nAntonio Vergari, YooJung Choi, Anji Liu, Stefano Teso, and Guy Van den Broeck. A\ncompositionalatlasoftractablecircuitoperationsforprobabilisticinference. InNeurIPS,\n2021.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. Universal\nadversarialtriggersforattackingandanalyzingNLP. InKentaroInui,JingJiang,Vin-\ncent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), pp. 2153–2162, Hong Kong, China, November\n2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1221. URL\nhttps://aclanthology.org/D19-1221/.\nBenjaminWarner,AntoineChaffin,BenjaminClavie´,OrionWeller,OskarHallstro¨m,Said\nTaghadouini,AlexisGallagher,RajaBiswas,FaisalLadhak,TomAarsen,NathanCooper,",
    "char_length": 1440
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 43,
    "text": "GriffinAdams,JeremyHoward,andIacopoPoli. Smarter,better,faster,longer:Amodern\nbidirectionalencoderforfast,memoryefficient,andlongcontextfinetuningandinference,\n2024. URLhttps://arxiv.org/abs/2412.13663.\nAlexander Wettig, Kyle Lo, Sewon Min, Hannaneh Hajishirzi, Danqi Chen, and Luca\nSoldaini. Organizetheweb: Constructingdomainsenhancespre-trainingdatacuration,\n2025. URLhttps://arxiv.org/abs/2502.10341.\nBrandonT.WillardandRe´miLouf. Efficientguidedgenerationforlargelanguagemodels.\nArXiv,abs/2307.09702,2023.\nWeiminXiong,YifanSong,XiutianZhao,WenhaoWu,XunWang,KeWang,ChengLi,Wei\nPeng,andSujianLi. Watcheverystep! LLMagentlearningviaiterativestep-levelprocess\nrefinement. InProceedingsofthe2024ConferenceonEmpiricalMethodsinNaturalLanguage\nProcessing.\nHonghua Zhang, Po-Nien Kung, , Masahiro Yoshida, Nanyun Peng, and Guy Van den\nBroeck. Adaptablelogicalcontrolforlargelanguagemodels. InNeurIPS,2024a.\nMaosenZhang,NanJiang,LeiLi,andYexiangXue. Languagegenerationviacombinatorial\nconstraintsatisfaction: AtreesearchenhancedMonte-Carloapproach. InTrevorCohn,\nYulan He, and Yang Liu (eds.), Findings of the Association for Computational Linguistics:\nEMNLP2020, pp.1286–1298, Online, November2020. Association forComputational\nLinguistics. doi: 10.18653/v1/2020.findings-emnlp.115. URL https://aclanthology.\norg/2020.findings-emnlp.115/.\n18\nPreprint. Underreview.\nYuansen Zhang, Xiao Wang, Tianze Chen, Jiayi Fu, Tao Gui, and Qi Zhang. P4: Plug-",
    "char_length": 1443
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 44,
    "text": "and-playdiscretepromptingforlargelanguagemodelspersonalization. InLun-WeiKu,\nAndreMartins,andVivekSrikumar(eds.),FindingsoftheAssociationforComputational\nLinguistics: ACL 2024, pp. 9129–9144, Bangkok, Thailand, August 2024b. Association\nfor Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.541. URL https:\n//aclanthology.org/2024.findings-acl.541/.\nStephenZhao,RobBrekelmans,AlirezaMakhzani,andRogerGrosse. Probabilisticinfer-\nenceinlanguagemodelsviatwistedsequentialmontecarlo. InProceedingsofthe41st\nInternationalConferenceonMachineLearning,ICML’24.JMLR.org,2024.\nRuiZheng,ShihanDou,SongyangGao,YuanHua,WeiShen,BinghaiWang,YanLiu,Senjie\nJin, Qin Liu, Yuhao Zhou, Limao Xiong, Lu Chen, Zhiheng Xi, Nuo Xu, Wenbin Lai,\nMinghaoZhu,ChengChang,ZhangyueYin,RongxiangWeng,WensenCheng,Haoran\nHuang,TianxiangSun,HangYan,TaoGui,QiZhang,XipengQiu,andXuanjingHuang.\nSecretsofrlhfinlargelanguagemodelsparti: Ppo,2023. URLhttps://arxiv.org/abs/\n2307.04964.\nWangchunshuZhou,YuchenEleanorJiang,EthanWilcox,RyanCotterell,andMrinmaya\nSachan.Controlledtextgenerationwithnaturallanguageinstructions.InAndreasKrause,\nEmma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan\nScarlett(eds.),Proceedingsofthe40thInternationalConferenceonMachineLearning,volume\n202ofProceedingsofMachineLearningResearch,pp.42602–42613.PMLR,23–29Jul2023.\nURLhttps://proceedings.mlr.press/v202/zhou23g.html.\nBanghuaZhu,MichaelJordan,andJiantaoJiao. Principledreinforcementlearningwithhu-",
    "char_length": 1480
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 45,
    "text": "manfeedbackfrompairwiseork-wisecomparisons. InProceedingsofthe40thInternational\nConferenceonMachineLearning,2023.\nDanielM.Ziegler,NisanStiennon,JeffreyWu,TomB.Brown,AlecRadford,DarioAmodei,\nPaulChristiano,andGeoffreyIrving. Fine-tuninglanguagemodelsfromhumanprefer-\nences,2020. URLhttps://arxiv.org/abs/1909.08593.\n19\nPreprint. Underreview.\nTable4: Breakdownoftheaverageϕtopic,TopicProb,andExp.Min.Topicfor6topics\nwhen steering Llama-3.2 (1B) generations to adhere to each given topic. Topics are\norderedleft-to-rightaccordingtotheirreportedfrequencyinWettigetal.(2025).\nMetric Method Politics Finance&Business Science&Tech Food&Dining History Industrial\nrandom 90.89 95.79 91.21 89.83 92.13 91.40\nbeamsearch 90.94 97.54 86.02 90.18 91.14 93.95\nϕtopic\nBoN 97.40 98.98 98.64 94.36 98.30 97.46\nSConE 98.99 99.70 99.42 97.14 99.60 99.56\nrandom 84.00 92.80 84.80 84.40 84.40 86.80\nbeamsearch 83.60 95.60 77.60 86.80 89.20 92.00\nTopicProb\nBoN 96.00 98.00 97.20 89.60 97.20 94.40\nSConE 98.40 100.00 99.20 93.60 99.60 99.60\nrandom 82.51 92.03 81.55 82.46 82.48 82.41\nbeamsearch 88.51 97.08 84.61 87.64 90.36 93.89\nExp.Min.Topic\nBoN 94.93 97.74 95.58 91.52 96.21 95.13\nSConE 96.42 99.08 95.60 93.37 97.47 98.23\nA ExperimentDetails\nSConE. Asatrade-offbetweenefficiencyandperformance,weperformexactinferenceover\nthe top-10 tokens of the base LM. For each prefix, we run 2 independent, non-blocking\nGibbsSamplingchainsfor20iterations,applyingathinningfactorof5. Eachchainstartsby",
    "char_length": 1469
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 46,
    "text": "sampling 25 tokens from the base LM using a combination of nucleus and min-p sam-\npling (top p=0.9, min p=0.1) (Holtzman et al., 2020; Minh et al., 2025). A BERT-based\nmodel(Warneretal.,2024)isusedtoefficientlyapproximatetheconditionals p˜cond.\nB HyperparametersConfigurations\nInthissection,wedescribethehyperparametersusedforeachofthedecodingalgorithms\nandbaselinesusedinthiswork. ExceptwhereexplicitlymentionedwerelyontheHug-\ngingFace’simplementation10andthedefaultconfigurations.\n• RandomSearch(random): do sample=True\n• BeamSearch(beamsearch): do sample=True,num beams=5andtemperature=0.3.\n• Best-of-N (BoN) (BoN): We implement a custom best-of-n rejection sampling ap-\nproach (Stiennon et al., 2020a), that independently generates N = 10 sequences\nusing HuggingFace’s generate method, parameterized with do sample=True,\ntop p=0.9, min p=0.1. A verifier ϕ is used to choose the final generation, pick-\na\ning the generation out of the N that maximizes the constraint verifier. For the\ndetoxification experiments where the goal is to minimize toxicity as measured\nby ϕtoxicity, we chose the generation that minimizes ϕtoxicity (in practice, we\nmaximize1 ϕtoxicity).\n−\nExperimentswererunonRTXA6000(48GBRAM)GPUsusingHuggingFaceandPyTorch.\nC AdditionalResults\nC.1 ControlledTopicGeneration\nLastly,weevaluatethemethodsontheirabilitytocontrolforthetopicofLMgenerations.\nWechoose6diversetopicsfromtherecentlytaxonomyconcerningthewebstructure(Wettig",
    "char_length": 1445
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 47,
    "text": "etal.,2025),includingfrequent(e.g.,Finance&BusinessandPolitics)andlessfrequenttopics\n(e.g., History, Industrial). For each topic, we randomly select 50 different examples from\ntheTopicAnnotations-Llama-3.1-405B-FP8(Wettigetal.,2025)testset,breakingtheminto\nprefixesof8to12words. Eachprefixisusedtosampleamaximumof60tokens.\n10https://huggingface.co/(version4.49.0)\n20\nPreprint. Underreview.\nTopic Generation Task. In general, we find that uncontrolled baselines achieve a fairly\nhigh average constraint score ( 91%), which may be explained by the use of longer\n≥\nprefixesduringgeneration. Wefindthistobethecaseformostexamples. Nonetheless,\nthediscrepancybetweenuncontrolledandcontrolledmethodsisstillvisiblewiththelatter\nachieving7%-8%higheraverageconstraintscores. Remarkably,wefindthatSConEisnot\nonlyabletoimproveuponBoN,achievinganaveragescoreof98.89%butalsoproduces\nhigherqualitygenerationsasemphasizedbythelowerperplexity.\nD EfficientLookaheadGenerationviaApproximateGibbsSampling\nOurapproachrequiresaccesstoplausible Algorithm4Hogwild! GibbsSampling\nfuture continuations, or lookahead sam-\nples, y , given a prefix y . However, 1: Input: ModernBert,prefixy 1:i ,lookahead∆,\ni+1:T 1:i blocksizeB,numworkersW,iterationsN\nwe would like to avoid expensive autore-\ngressivesampling,especiallysinceweare 2: Output: y˜ 1:T drawnapproximatelyfrom p\nhappy to trade off sample quality for effi- 3:\nciency. Intuitively,weareonlyinterestedin 4: ▷Randomlyinitializecontinuationy i+1:T",
    "char_length": 1477
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 48,
    "text": "acrudeprojectionofwherethecurrenttra- 5: s InitializeSequence(y 1:i ,∆)\n←\njectorymightleadus,asopposedtoaper- 6: ▷LaunchW workersforN/W updates\nfectlycoherentnaturallanguagesentence. 7: forallworkersw =1toW inparalleldo\n8: foriter =1to N/W do\nTaking cue from speculative decod- ⌈ ⌉\ning (Leviathan et al., 2023), given a 9: ▷Sampleblockstartjincontinuation\nprefix y we start with a guess for the 10: j (i+1,T B+1)\n1:i ∼ U −\ncontinuation y , either by padding 11: blk idx [j : j+B 1]\ni+1:T ← −\nwith [MASK] tokens or crudely sampling 12: ▷Read(potentiallystale)states local\np(y\nj |\ny\n1:i\n)for j = i+1to T. Wecanthen 13: slocal\n←\nReadSharedState(s)\nrefine these crude continuations using 14: ▷Getapproximateblockconditionals\nGibbs Sampling (Koller & Friedman, 2009), 15: pblk ModernBert(slocal,blk idx)\n←\na Markov chain Monte Carlo (MCMC) 16: ▷Samplenewtokensfortheblock\na to p k p e r n oac in hth th a e tst s o e c q h u a e s n ti c c e a , lly as s y a m m p p t l o es tic e a a l c l h y 1 1 7 8 : : y ▷ ′b U lk p ← da S te am sh pl a e re F d ro s m e B q l u o e c n k c D e is (H t( o p g b w lk i ) ld!)\nconvergingtothetruedistribution. There- 19: WriteSharedState(s,blk idx,y b′lk )\nfore, by setting a cutoff, or a maximum\n20: endfor\nnumber of iterations, we can control how\n21: endfor\ncrude of a lookahead sample we desire.\n22: WaitForAllWorkers()\nUnfortunately,thisintroducesamultitude\nof computational challenges. First, the 23: y˜ 1:T ← ReadSharedState(s)",
    "char_length": 1468
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 49,
    "text": "Gibbssamplerassumesefficientaccessthe 24: returny˜ 1:T\nthe full conditionals p(y y i) i, which\ni\nrequiresO( V )forwardpa | ss − eso ∀ ftheLMforasinglepositioni,whichisuntenablegiven\n| |\nthevocabularysizeofmodernLMs. Second,initsmostbasicform,Gibbssamplingrequires\nmanyiterationsthroughthesentence,computingtheconditionalandresamplingasingle\ntokenperiteration,whichisquiteslow.\nToovercomethesechallengesandenableefficientgeneration,weutilizeseveralstrategies:\nApproximateConditionalswithMaskedLanguageModels(MLMs) Inplaceofanalyti-\ncallycomputingtheconditionalscomputation,weleverageefficientpretrainedMLMsto\napproximatetheconditionalprobability p(y y ). Thesemodelsareinherentlydesignedto\npredictmaskedtokensgiventheirbidirectio\ni|\nna−l\ni\ncontext,providingafastapproximationof\ntherequiredconditionaldistributionswithoutexpensiveanalyticalmarginalization.\nParallel and Asynchronous Updates (Hogwild! Style) Standard Gibbs sampling up-\ndatestokenssequentially. Inabidtoacceleratesampling,weemployparallel,potentially\nasynchronousupdatesinspiredbyHogwild!(Smola&Narayanamurthy,2010;Niuetal.,\n2011)approaches. Multipletokenpositionsjcanbeupdatedsimultaneously,possiblyusing\nslightlystalecontextinformationy .ThistradesofftheunbiasednessofGibbssampling(Sa\nj\n−\netal.) forsubstantialgainsinwall-clocktimethtarecrucialforinference-timeapplications.\n21\nPreprint. Underreview.\nBlockedGibbsSampling Ratherthansamplingindividualtokensoneatatime,wecan",
    "char_length": 1439
  },
  {
    "paper_id": "prob_control_for_llms",
    "chunk_id": 50,
    "text": "updatecontiguousblocksoftokenssimultaneously. Thisreducesthenumberofsampling\niterationsrequiredforconvergenceofthechainwhileallowingustobetterleveragethe\nparallelprocessingcapabilitiesofmodernhardware,especiallywhencombinedwithMLM-\nbasedapproximateconditionalsthatexcelatprocessingmultiplepositionsefficiently.\nControllingtheEfficiency-AccuracyTrade-off Theuseofapproximateconditionalsin-\ntroducesanaturaldialtobalanceefficiencyandsamplequality. InverymuchaHogwild!\nfashion,thefrequencyatwhichwere-computeorsynchronizetheseapproximatecondi-\ntionalsusingthelatestcontextinfluencesthistrade-off. Lessfrequentupdatesleadtofaster\nsamplingusingpotentially moreoutdatedcontextualinformation, whilemorefrequent\nupdatesimprovefidelitytothetargetdistributionatthecostofincreasedcomputation.\nBycombiningGibbssamplingwiththeseefficiency-focusedtechniques—approximating\nconditionals via MLMs, parallelizing updates Hogwild! style, and employing blocked\nsampling—wecanrapidlygeneratediverseandplausiblelookaheadsamplesy suitable\ni+1:T\nforourinference-timealgorithm,effectivelytransformingthecomputationallydemanding\ntaskofsamplingfromthejointdistributionintoamanageableandefficientprocedure.\nThepseudocodefortheapproachelucidatedabovecanbeseeninAlgorithm4.Furthermore,\nanefficientPyTorchimplementationwillbemadeavailableinourGitHubRepository.\n22",
    "char_length": 1333
  }
]