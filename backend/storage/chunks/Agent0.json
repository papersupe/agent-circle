[
  {
    "paper_id": "Agent0",
    "chunk_id": 0,
    "text": "Agent0: Unleashing Self-Evolving Agents from Zero Data\nvia Tool-Integrated Reasoning\nPengXia1 KaideZeng1 JiaqiLiu1 CanQin2 FangWu3 YiyangZhou1 CaimingXiong2 HuaxiuYao1\nAbstract extensive interaction with an environment, such as deep\nresearch(OpenAI,2025;Google,2024;Teametal.,2025)\nLarge Language Model (LLM) Agents, often\nandagenticcoding(Jimenezetal.,2023;Anthropic,2025;\ntrained with Reinforcement Learning (RL), are\nWang et al., 2024a). To optimize these complex, multi-\nconstrainedbyadependencyonhuman-curated\nstepinteractionsandmovebeyondhard-codedworkflows,\ndata, limiting scalability and tethering AI to\nReinforcementLearning(RL)hasemergedasaprincipal\nhuman knowledge. Existing self-evolution trainingparadigm(Ouyangetal.,2022;Shaoetal.,2024;\nframeworksofferanalternativebutaretypically\nTu et al., 2025), achieving significant progress on com-\nrestrictedbythemodel’sinherentcapabilitiesand\nplexreasoningtasks. However,theefficacyofthesemeth-\nsingle-round interactions, hindering the devel-\nods,whetherReinforcementLearningfromHumanFeed-\nopmentofcomplexcurriculainvolvingtooluse\nback(RLHF)orReinforcementLearningfromVerifiable\nor dynamic reasoning. We introduce Agent0,\nRewards(RLVR),reliesheavilyonmassive,high-quality,\na fully autonomous framework that evolves\nhuman-curateddatasets(Zhangetal.,2025c). Thisdepen-\nhigh-performing agents without external data\ndencynotonlycreatesaseverescalabilitybottleneck(Yue\nthrough multi-step co-evolution and seamless",
    "char_length": 1464
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 1,
    "text": "etal.,2025),whichistime-consuming,labor-intensive,and\ntoolintegration. Agent0establishesasymbiotic\ncostly,butalsofundamentallytethersthepotentialofAIto\ncompetitionbetweentwoagentsinitializedfrom\nthelimitsofhumanknowledgeandannotationspeed.\nthesamebaseLLM:acurriculumagentthatpro-\nposesincreasinglychallengingfrontiertasks,and To break free from this reliance on human data, self-\nanexecutoragentthatlearnstosolvethem. We evolutionframeworkshaveemergedasapromisingalterna-\nintegrateexternaltoolstoenhancetheexecutor’s tive(Zhaoetal.,2025;Liuetal.,2025a;Huangetal.,2025;\nproblem-solvingcapacity;thisimprovement,in Wang et al., 2025d), offering a scalable pathway by en-\nturn,pressuresthecurriculumagenttoconstruct ablingmodelstoautonomouslygeneratetheirowntraining\nmore complex, tool-aware tasks. Through data. Yet,despitetheirpotential,existingself-playorself-\nthis iterative process, Agent0 establishes a challengingapproachesfacesevereconstraints. First,their\nself-reinforcingcyclethatcontinuouslyproduces capabilitiesarecappedbythemodel’sinherentknowledge\nhigh-quality curricula. Empirically, Agent0 andreasoningabilities(Fangetal.,2025;Chengetal.,2024;\nsubstantially boosts reasoning capabilities, Zhou et al., 2025a), causing the generated tasks to rarely\nimproving the Qwen3-8B-Base model by 18% surpassthemodel’scurrentcomplexity(Zhouetal.,2025b),\nonmathematicalreasoningand24%ongeneral leadingtolearningstagnation. Second,theseframeworks",
    "char_length": 1445
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 2,
    "text": "reasoning benchmarks. Code is available at typicallyoperateonlyinsingle-roundinteractions(Lietal.,\nhttps://github.com/aiming-lab/Agent0. 2025c),failingtocapturethedynamic,context-dependent\nnatureofreal-worldproblems. Thisduallimitationnotonly\nrestrictsthecomplexityoftheself-generatedcurriculumbut,\n1.Introduction\nmorecritically,hindersthemodelfrommasteringessential\nskillsthatrequirecomplextooluseormulti-stepreasoning.\nLarge Language Model (LLM) Agents have shown re-\nmarkable capabilities in tackling complex, long-horizon To address these challenges, as demostrated in Figure 1,\nproblems (Qiu et al., 2025b;a; Jin et al., 2025; Yu et al., weintroduceAgent0,afullyautonomousframeworkde-\n2025a; Tang et al., 2025; Zhai et al., 2025) that require signedtoguidetheevolutionofagentsentirelyfromscratch.\nAgent0completelyeliminatesthedependenceonanyex-\n1UNC-ChapelHill2SalesforceResearch3StanfordUniversity.\nternaldataorhumanannotations,pioneeringlycombining\nCorrespondenceto:PengXia<pxia@cs.unc.edu>,HuaxiuYao\n<huaxiu@cs.unc.edu>. toolintegrationwithmulti-roundco-evolution. Theframe-\nwork’simplementationbeginswithabaseLLMfromwhich\nPreliminarywork.\n1\n5202\nvoN\n02\n]GL.sc[\n1v34061.1152:viXra\nAgent0:UnleashingSelf-EvolvingAgentsfromZeroDataviaTool-IntegratedReasoning\nq\n…\nCurriculum Executor\nAgent Question Agent\nReasoning Process\nTool\nEnvironment\nModel Tool Tool\nResponse Calling Response\na\n̂\nCurriculum Executor\nReward Predicted Reward\nAnswer\nr r\nC E\n(a) (b)",
    "char_length": 1458
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 3,
    "text": "Figure1.TheAgent0autonomousco-evolutionframework.TheCurriculumAgent(left)usesRLtogeneratefrontiertasks,rewarded\nbytheExecutorAgent’suncertaintyandtool-usefrequency. TheExecutorAgent(right)learntosolvethembyRL.Thissharedtool\nintegrationdrivesavirtuouscycle,spiralinguptaskcomplexityandagentcapabilityentirelyfromscratch.\nwe initialize two functionally distinct agents: an execu- cycleoftheexecutor’scapabilityimprovement.\ntoragentandacurriculumagent. Theseagentsco-evolve\nthroughasymbioticcompetition: thecurriculumagentis\n2.Preliminaries\ntrained using RL (Shao et al., 2024) to propose frontier\ntasksthatpreciselychallengetheexecutor’scurrentcapabil- LLM as a Policy Agent. We formulate the LLM as an\nities,usingtheexecutor’suncertainty(i.e.,self-consistency agent,representedbyapolicyπ withparametersθ. Given\nθ\nacross multiple answers) and its frequency of tool use as apromptx,theagentautoregressivelygeneratesaresponse\nrewardsignals. Concurrently,theexecutoragentistrained y ∼ π (·|x). The general objective of reinforcement\nθ\nvia RL to successfully solve these tasks, optimizing on a learningistooptimizeθtomaximizetheexpectedreward\nfilteredsetofchallengingproblemsgeneratedbythefrozen J(θ)=E [R(y|x)].\nx∼D,y∼πθ(·|x)\ncurriculumagentandusingpseudo-labelsderivedfromits\nGroup Relative Policy Optimization (GRPO).\nown majority voting. Equipping the executor with a tool\nGRPO (Shao et al., 2024) is a reinforcement learn-\nenhancesitsproblem-solvingabilities,whichinturncom-",
    "char_length": 1474
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 4,
    "text": "ingmethodthatavoidstrainingacriticbyusingintra-group\npelsthetool-equippedcurriculumagenttogeneratemore\nrelativerewards. Foreachpromptx,themodelsamplesG\ncomplex,tool-basedcurricula. Thisestablishesavirtuous\nresponses {y ,...,y }, which are scored to get rewards\ncycle,drivingasynchronousspiralofimprovementinboth 1 G\n{r ,...,r }. GRPOcomputesnormalizedadvantagesAˆ\nagentcapabilityandcurriculumcomplexity. Furthermore, 1 G i\nweextendthisparadigmtosupportmulti-turninteractions, using a z-score: Aˆ i = s r td i ( − { m rj ea } n G j ( = {r 1 j )+ }G j ϵ = no 1 rm ), where ϵ norm is a\nenablingthegenerationofcontext-rich,conversationaltasks small constant for numerical stability. The policy isthen\nthatbetterreflectreal-worldproblem-solving. updated by minimizing the following PPO-style clipped\nlossfunction(Schulmanetal.,2017):\nTheprimarycontributionofthispaperisAgent0,anovel\nframework that autonomously evolves LLM agents from G (cid:32)\nscratchthroughtool-augmentedreasoningwithoutrelying L GRPO (θ)=− G 1 (cid:88) min π π θ ( ( x x i ) ) Aˆ i ,\non any external data. Across ten benchmarks spanning i=1\nθold i\n(1)\n(cid:33)\nmathematicalandgeneralreasoning,empiricalresultsshow (cid:16) π (x ) (cid:17)\nthatAgent0achievessubstantialmodelagnosticcapabil- clip π θ (x i ) ,1−ϵ,1+ϵ Aˆ i +βKL(π θ ∥π θold ),\nitygains,improvingmathematicalreasoningperformance\nθold i\nby 18% and general reasoning performance by 24%. In where πθ(xi) is the importance sampling ratio between",
    "char_length": 1470
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 5,
    "text": "addition, our analysis confirms this gain is driven by our\nπθold (xi)\nthecurrentpolicyπ andthereferencepolicyπ fromthe\nco-evolutionaryloop,wherethecurriculumagentlearnsto\nθ θold\npreviousiteration. Aˆ isthenormalizedadvantage, andϵ\ngenerateprogressivelycomplextasks,creatingavirtuous i\nandβ arehyperparameters. TheKL-divergencetermacts\n2\nAgent0:UnleashingSelf-EvolvingAgentsfromZeroDataviaTool-IntegratedReasoning\nPolicy Update by GRPO\nx {y 1,1 ,⋯,y 1,m} ŷ\nCurriculum … 1 Executor … Majority … 1\nAgent x i Agent {y i,1 ,⋯,y i,m} Voting y i ̂\nModel Tool\nEnvironment Response Calling\nTool Final\nResponse Answer\nSelf-Consistency Executor … … {y i,1 ,⋯,y i,m} Majority y … 1 ̂\nData p(x̂) Filtered Data Agent {y 1,1 ,⋯,y 1,m} Voting y i ̂\nAmbiguity Signal Policy Update by ADPO\nFigure2.TheAgent0co-evolutionaryloop.(1)CurriculumEvolution:TheCurriculumAgentπ istrainedviaRLtogeneratetasks,\nθ\nmaximizingarewardR basedonexecutorUncertaintyR ,ToolUseR andRepetitionPenaltyR .(2)ExecutorEvolution:Tasks\nC unc tool rep\narefilteredbyself-consistencyscorepˆtocreateachallengingdatasetD(t).TheExecutorAgentπ isthentrainedonD(t)viaADPO,an\nϕ\nambiguity-awareRLmethodusingmajority-votepseudo-labelsy˜.\nasaregularizationpenaltytostabilizetraining. multi-turninteractions,enablingtheCurriculumAgentto\ngeneratecontext-rich,conversationaltasksthatbetterreflect\n3.TheAgent0Framework real-worldproblem-solving.\n3.1.FrameworkOverview 3.2.CurriculumAgentTraining",
    "char_length": 1437
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 6,
    "text": "Agent0 is a fully autonomous, iterative co-evolutionary ThegoaloftheCurriculumAgentπ ,istogenerateaprompt\nθ\nframework designed to enhance the capabilities of LLM xthatmaximizesacompositerewardsignalR . Thisre-\nC\nagents without relying on any human-annotated data. At wardsignalisdesignedtoquantifythechallengeoftaskx\nthe core of this framework are two functionally distinct forthecurrentExecutorAgentπ . Weoptimizeπ using\nϕ θ\nagentsinitializedfromthesamebaseLLM,π base : (1)Cur- theGRPOalgorithmdescribedintheSection2.\nriculumAgent(π )aimstogeneratefrontiertasksthatare\nθ\nForeachtaskx generatedbyπ ,wecomputeitsrewardby\nappropriatelychallengingforthecurrentExecutorAgent; i θ\nsamplingkresponses{y }k fromthecurrentExecutorπ .\n(2) Executor Agent (π ) aims to solve the increasingly j j=1 ϕ\nϕ\nThecompositerewardR consistsoftwokeycomponents:\ncomplextasksproposedbytheCurriculumAgent. C\nUncertainty Reward. This reward incentivizes the Cur-\nThese two agents co-evolve iteratively through a process\nriculum Agent to generate tasks that the Executor finds\nofsymbioticcompetition,asillustratedinFigure2. Each\nconfusingoruncertain(Shietal.,2025;Baeetal.,2025).\niterationtofthisprocessisdividedintotwostages:\nWeusetheExecutor’sself-consistencypˆ(x;π )asaproxy\nϕ\nCurriculumEvolution. WetraintheCurriculumAgentπ θ foruncertainty. pˆisdefinedastheproportionofthek re-\nusingRLtospecializeingeneratingtasksthatchallengethe sponsesthatvoteforthemajorityanswer(y˜). Thereward",
    "char_length": 1460
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 7,
    "text": "currentExecutorAgentπ(t−1). functionisdesignedtobemaximizedwhenpˆ=0.5,where\nϕ\ntheExecutor’suncertaintyishighest:\nExecutorEvolution. WeusethefrozenCurriculumAgent\nπ(t) to generate a pool of tasks, from which we filter a R (x;π )=1−2|pˆ(x;π )−0.5| (2)\nθ unc ϕ ϕ\nchallengingdatasetD(t). WethentraintheExecutorAgent\nπ onthisdatasetusingRL,evolvingitintoπ(t).\nThisfunctionpenalizestasksthatareeithertooeasy(pˆ→1)\nϕ ϕ ortoohard(pˆ→0).\nTheintegrationofacodeinterpretertoolestablishesavirtu-\nToolUseReward. Todrivethevirtuouscycle,wemustex-\nouscycle: theExecutorAgent’sproblem-solvingcapabil-\nplicitlyrewardtasksthatprompttheExecutortouseitstool.\nitiesareenhancedbythetool, whichinturncompelsthe\nWe define R based on the number of tool invocations,\ntool-equippedCurriculumAgenttogeneratemorecomplex, tool\nidentifiedbythetoolresponsemarker, i.e., ‘‘‘output,\ntool-basedcurricula. Furthermore,theframeworksupports\nwithinacompleteprediction y = π (x). LetN (y)be\nϕ tool\n3\nAgent0:UnleashingSelf-EvolvingAgentsfromZeroDataviaTool-IntegratedReasoning\nAlgorithm1Self-EvolutionaryFrameworkAgent0 whereλ ,λ ,andλ arehyperparameters. Weusethis\nunc tool rep\nRequire: BaseLLMπ base ;IterationsT;Samplesk. R C astherewardr i intheGRPOloss.\n1: Initializeπ(0) ←π andπ(0) ←π .\nθ base ϕ base\n2: foreachiterationt=1,...,T do 3.3.ExecutorAgentTraining\n3: ▷CurriculumEvolution(Trainπ )\nθ\n4: Initializeπ θ ←π θ (t−1) TheExecutorAgentπ ϕ ’sobjectiveistomaximizeitssuc-",
    "char_length": 1438
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 8,
    "text": "5: GenerateabatchoftasksX ={x i }∼π θ cessrateinsolvingtasksgeneratedbytheCurriculumAgent\n6: fortaskx i ∈Xdo π . ThisstageoftrainingisalsobasedonGRPO.\n7: Samplekresponses{y }k ∼π(t−1)(x ) θ\nj j=1 ϕ i\n8: ComputeR (x )usingEq.5\nC i 3.3.1.DATASETCURATIONANDTRAJECTORY\n9: endfor\n10: Updateπ usingL with(X,R )→π(t) GENERATION\nθ GRPO C θ\n11: ▷ExecutorEvolution(Trainπ )\nϕ ChallengingDatasetConstruction. AftertheCurriculum\n12: Generate X ∼ π(t) and filter to D(t) = {(x,pˆ,y˜)}\npool θ Agent π(t) is trained, we freeze it. We use it to generate\nwhere|pˆ(x)−0.5|≤δ θ\n13: Initializeπ ←π(t−1) a large pool of candidate tasks X pool . For each task x in\n14: forbatchB ϕ D ={ ϕ (x,pˆ(x),y˜)}∼D(t)do this pool, we have the current Executor π ϕ (t−1) sample k\n15: InitializeT ,A˜ ,P responsesandcalculateitsself-consistencypˆ(x). Itiscal-\nbatch batch batch\n16: for(x,pˆ(x),y˜)∈B do culated as the proportion of responses that voted for this\nD\n17: Samplektrajectories{τ\ni\n}k\ni=1\n∼π\nϕ\n(x) majorityanswery˜:\n18: ComputerewardsR =I(o =y˜)\ni i\n19: ComputescaledadvantagesA˜ i ←A i ·f(pˆ(x)) 1(cid:88) k (cid:88) k\n20: Add{τ i }toT batch ,{A˜ i }toA˜ batch ,pˆ(x)toP batch pˆ(x)= k I(o i =y˜), y˜=argm y ax I(o i =y), (6)\n21: endfor i=1 i=1\n22: Updateπ ϕ usingL ADPO (Eq.8)oncollectedbatch where I is the indicator function. To build an efficient\n23: endfor\n24: π(t) ←π training curriculum, we filter for tasks that lie at the ca-\nϕ ϕ\n25: endfor pabilityfrontier. Soweretainonlythosetaskswhoseself-",
    "char_length": 1479
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 9,
    "text": "consistencyscoresfallwithinaninformativeband:\n(cid:110) (cid:12) (cid:12) (cid:111)\nthe total count of these markers in y. The reward is then D(t) = x∈X |(cid:12)pˆ(x;π(t−1))−0.5(cid:12)≤δ , (7)\npool (cid:12) ϕ (cid:12)\ncalculatedasaweighted,cappedvalue:\nwhereδisathresholdcontrollingthecurriculumdifficulty.\nR tool (x;π ϕ )=γ·min(N tool (y),C) (3) Thisfilteringstepensuresthatπ ϕ trainsonlyontasksthat\nareneithertooeasynortoohardforit.\nwhereγisascalinghyperparameterforrewardscoreandC\nisacaponthenumberofrewardedcallstopreventreward- Multi-TurnRollout. Wereplacethestandardsingle-turn\ningexcessiveorspurioustooluse. generation with a multi-step, tool-integrated rollout pro-\ncess. During this process, each of the k trajectories is\nRepetitionPenalty. Toencouragediversitywithinatrain-\ngenerated by having the policy π(t−1) first produce text\ning batch X, following (Huang et al., 2025), we intro- ϕ\nreasoningt . Whenthepolicyemitsatool-calltrigger(i.e.,\nduce a repetition penalty R . We first compute pair- 1\nrep ‘‘‘python...‘‘‘tags),generationispaused.Thecode\nwise distances between generated tasks using a similar-\nc isthenexecutedinasandbox,whichreturnsanexecu-\nity metric, such as BLEU score (Papineni et al., 2002): 1\ntionresultorerrorf . Thisfeedbackf ,prependedwitha\nd =1−BLEU(x ,x ). Tasksarethengroupedintoclus- 1 1\nij i j simpleprefixlike‘‘‘output...‘‘‘,isfedbacktothe\nters C = {C ,...,C } where d < τ . The penalty\n1 K ij BLEU policy. Thepolicythencontinuesgenerating,conditioning",
    "char_length": 1493
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 10,
    "text": "forataskx belongingtoclusterC isproportionaltoits\ni k on the history and the new feedback [t ⊕c ⊕f ⊕...].\nrelativeclustersize: 1 1 1\nThis iterative process repeats until the policy generates a\n|C | final answer o (i.e., in {boxed...} tags), resulting in a\nRrep(x\ni\n)=λrep\nB\nk , (4)\ncomplete,hybridreasoningtrajectory. Thisdynamic,inter-\nleavedfeedbackmechanismallowstheagenttoiteratively\nwhereBisthebatchsizeandλ isascalingfactor.\nrep refineitsreasoningandcorrecterrors,mimickingthe“aha\nCompositeReward. Thefinalrewardcombinesthesesig- moment”ofself-correction.\nnals, subtracting the repetition penalty, and is gated by a\nPseudo-LabelAdvantage. Aftergeneratingk fulltrajec-\nformatcheckR .\nformat toriesandidentifyingtheirkfinalanswers{o }k ,weuse\ni i=1\nR (x )=R (x )·max(0,(λ R thepreviouslydeterminedmajorityanswery˜asthepseudo-\nC i format i unc unc\n+λ tool R tool )−R rep (x i )) (5) label. WethenassignaterminalrewardR i =I(o i =y˜)to\n4\nAgent0:UnleashingSelf-EvolvingAgentsfromZeroDataviaTool-IntegratedReasoning\neachtrajectorybasedonwhetheritsanswero matchesthis confidentsamplestopreservestability.\ni\npseudo-label. ThisoutcomerewardR isusedtocompute\ni TheExecutorAgentisupdatedbyminimizingtheADPO\ntheadvantageA fortheentiremulti-steptrajectoryi.\ni objective:\n3.3.2.AMBIGUITY-DYNAMICPOLICYOPTIMIZATION (cid:34) G (cid:32)\nL (θ)=E − 1 (cid:88) min r (θ)A˜ (x),\nStandardGRPOtreatsalltrainingsamplesequally(Schul- ADPO x∼D(t) G i i\nmanetal.,2017;Shaoetal.,2024). However,inourself- i=1",
    "char_length": 1487
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 11,
    "text": "(cid:33)(cid:35)\nevolutionarysetting,werelyonmajorityvotingtoderive clip (cid:16) r (θ),1−ϵ ,1+ϵ (x) (cid:17) A˜ (x) ,\npseudo-labels,whichintroducestwocriticalissues: label i low high i\nnoise and restricted exploration on ambiguous tasks. To (8)\naddressthese,weproposeAmbiguity-DynamicPolicyOp- wherer (θ)istheimportancesamplingratio,A˜ (x)isthe\ni i\ntimization(ADPO),whichincorporatestwokeymodifica- ambiguity-scaled advantage, and ϵ (x) is the dynamic\nhigh\ntionsmotivatedbythedata’sambiguitysignalpˆ(x). upperboundinverselyrelatedtopˆ(x).\nAmbiguity-Aware Advantage Scaling. The first issue\nis that for high-ambiguity tasks (low pˆ(x)), the majority 4.Experiments\nanswer is prone to errors. Directly optimizing on these\nIn this section, we evaluate the performance of Agent0,\nnoisylabelsusingstandardGRPOrisksreinforcingincor-\naiming to answer the following questions: (1) How does\nrectreasoning. Topreventoverfittingtopotentiallyinaccu-\nratepseudo-labels,wescalethenormalizedadvantageAˆ . theperformanceofAgent0compareagainststate-of-the-art\ni\nself-evolvingbaselines? (2)Istheproposedco-evolutionary\nWedefineascalingfactors(x) = f(pˆ(x)),wheref isan\nloopeffectiveatprogressivelyimprovingtheagents’perfor-\nincreasingfunctionofself-consistency. Theadvantageis\nmodifiedasA˜ (x)=Aˆ ·s(x). Thisproportionallydown- manceovermultipleiterations? (3)Howeffectiveiseach\ni i\nkeycomponentofourframework? (4)Canthemathemat-\nweightsthetrainingsignalfromunreliable,low-consistency",
    "char_length": 1467
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 12,
    "text": "icalreasoningabilitiescultivatedbyAgent0generalizeto\nsamples.\nimproveperformanceongeneral-domainreasoningtasks?\nAmbiguity-\nModulated Trust\n4.1.ExperimentalSetup\nRegions. The sec-\nond issue pertains Implementation Details. Our framework Agent0, is\nto the rigid con- implemented based on the VeRL (Sheng et al., 2025).\nstraints imposed by We evaluate Agent0 on two base models: Qwen3-4B-\nstandard proximal BaseandQwen3-8B-Base(Yangetal.,2025a). Boththe\nalgorithms(Yuetal., two Agent are initialized from these base models. Dur-\n2025b). Whilestatic ing the co-evolutionary loop, for each task x i , we sam-\nclipping (e.g., ϵ) is Figure3.Up-clippedtokenprobabil- ple k = 10 responses from the Executor to compute un-\ndesigned to ensure ities. Most up-clipped tokens have certainty and generate pseudo-labels. The task filtering\nstability,itcreatesan lowprobabilities,implyingstandard threshold is set to δ = 0.25, retaining tasks with a self-\nasymmetric barrier clippinglimitsexploration. consistency pˆ(x) between 0.3 and 0.8. For the Curricu-\ntolearning. AsillustratedinFigure3, empiricalanalysis lum Agent, we set the tool reward scaling λ tool = 0.6\nreveals that the upper clipping bound is predominantly and cap C = 4. For the Executor Agent, we integrate a\ntriggeredbytokenswithlowprobabilities. Thisindicates sandboxedcodeinterpreter(Chengetal.,2025)basedon\nthat the standard mechanism disproportionately “clamps” VeRL-Tool(Jiangetal.,2025),allowingittoexecutecode",
    "char_length": 1468
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 13,
    "text": "the growth of unlikely tokens, effectively stifling the snippetsenclosedin‘‘‘python...‘‘‘tagsandreceive\nemergence of new reasoning paths. This restriction is the‘‘‘output...‘‘‘.\nparticularly detrimental for high-ambiguity tasks (low\nBaselineMethods. WecompareAgent0againstseveral\npˆ(x)),wherethecorrectreasoningoftenresidesinthetail\nstate-of-the-artself-improvementmethods. 1)BaseModel:\nof the current policy distribution and requires significant\nThepre-trainedbasemodelwithoutanyfine-tuning.2)Base\nupdates to surface. To address this bottleneck, ADPO\nModel w/ tool: The base model evaluated in a zero-shot\ndynamically modulates the trust region. We define the\nsetting, but given access to the code interpreter. 3) Self-\nupperclippingboundϵ (x)asadecreasingfunctionof\nhigh EvolvingMethods: R-Zero(Huangetal.,2025),Absolute\npˆ(x). Thiseffectivelyrelaxestheconstraintforambiguous\nZero(Zhaoetal.,2025), SPIRAL(Liuetal.,2025a)and\ninputs, permitting larger gradient steps to uplift potential\nSocratic-Zero(Wangetal.,2025d).\nlow-probabilitysolutions,whileretainingtightboundson\nEvaluationDatasetsandMetrics. Agent0requiresno\n5\nAgent0:UnleashingSelf-EvolvingAgentsfromZeroDataviaTool-IntegratedReasoning\nTable1.Comprehensiveresultsonmathematicalreasoningbenchmarks.Thepeakperformanceachievedduringeachmodel’straining\nprocessishighlightedinbold.\nModelName AVG AMC Minerva MATH GSM8K Olympiad AIME25 AIME24\nQwen3-4B-Base\nBaseModel ✗ ✗ 42.6 45.7 38.2 68.2 87.8 41.0 6.15 10.9",
    "char_length": 1466
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 14,
    "text": "BaseModelw/tool ✓ ✗ 44.2 46.3 39.6 71.0 88.6 43.7 7.71 12.3\n+AbsoluteZero ✓ ✗ 46.4 50.0 41.9 76.2 89.3 41.5 13.4 12.2\n+SPIRAL ✗ ✗ 47.0 57.5 42.4 76.4 91.0 38.4 10.0 13.3\n+R-Zero ✗ ✗ 49.1 57.3 52.9 79.6 92.1 44.6 4.27 12.7\n+Agent0 ✓ ✗ 52.5 60.6 55.6 80.5 92.6 46.7 14.1 17.4\nQwen3-8B-Base\nBaseModel ✗ ✗ 49.2 52.0 50.0 78.0 89.1 44.7 16.7 13.9\nBaseModelw/tool ✓ ✗ 53.2 60.3 54.9 79.2 90.7 47.9 18.7 20.9\n+AbsoluteZero ✓ ✗ 52.6 62.5 52.9 76.6 92.0 47.8 18.2 18.4\n+R-Zero ✗ ✗ 54.7 61.7 60.7 82.0 94.1 48.9 19.2 16.4\n+Socratic-Zero ✗ ✓ 56.1 63.7 52.4 81.2 87.3 55.1 24.5 28.4\n+Agent0 ✓ ✗ 58.2 62.4 61.3 82.4 94.5 54.0 24.8 28.0\nTable2. Resultsongeneral-domainreasoningbenchmarks.\nModelName OverallAVG MATHAVG SuperGPQA MMLU-Pro BBEH\nQwen3-4B-Base\nBaseModel ✗ ✗ 27.1 42.6 20.9 37.4 7.57\nBaseModelw/tool ✓ ✗ 30.3 44.2 25.8 42.9 8.32\n+AbsoluteZero ✓ ✗ 33.6 46.4 27.1 52.6 8.3\n+SPIRAL ✗ ✗ 34.2 47.0 27.1 53.2 9.57\n+R-Zero ✗ ✗ 34.6 49.1 27.6 51.5 10.4\n+Agent0 ✓ ✗ 37.6 52.5 29.9 55.9 12.0\nQwen3-8B-Base\nBaseModel ✗ ✗ 34.5 49.2 28.3 51.8 8.6\nBaseModelw/tool ✓ ✗ 36.7 53.2 29.5 54.8 9.37\n+AbsoluteZero ✓ ✗ 39.9 52.6 33.5 62.5 10.8\n+R-Zero ✗ ✗ 38.7 54.7 31.4 58.2 10.6\n+Socratic-Zero ✗ ✓ 39.2 56.1 30.1 60.9 9.5\n+Agent0 ✓ ✗ 42.1 58.2 33.0 63.4 13.7\nhuman-annotateddatafortraining. Weevaluateallmethods utilizesacodeexecutor,by10.6%. ItevenexceedsSocratic-\non two suites of benchmarks: 1) Mathematical Reason- Zeroby3.7%,whichreliesonexternalOpenAIAPIs. This",
    "char_length": 1445
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 15,
    "text": "ing: We use a comprehensive set including AMC, Min- demonstratesthesuperiorityofAgent0’sself-evolutionap-\nerva(Lewkowyczetal.,2022), MATH(Hendrycksetal., proach. By using tools to interact with the environment,\n2021),GSM8K(Cobbeetal.,2021),Olympiad-Bench(He theagenteffectivelyenhancesthequalityanddiversityof\netal.,2024),AIME25,andAIME24. 2)General-Domain questions generated by the curriculum agent. Similarly,\nReasoning: To measure generalization, we use SuperG- fortheexecutionagent,thismoreeffectivelyimprovesits\nPQA(Duetal.,2025),MMLU-Pro(Wangetal.,2024b), problem-solvingcapabilities.\nandBBEH(Kazemietal.,2025). Wereporttheaccuracy\nGeneralizationtoGeneral-DomainTasks. Furthermore,\n(pass@1)basedongreedydecodingacrossallbenchmarks,\nTable2showsstrongevidenceofgeneralization.OnQwen3-\nexceptAMCandAIMEbenchmarks(mean@32).\n8B, Agent0 achieves the highest overall average score\namong all approaches, significantly outperforming other\n4.2.MainResults\ndata-freemethods. Thisindicatesthatthecomplex,multi-\nWepresentthemainresultsformathematicalreasoningin stepreasoningabilitieswecultivatedintheexecutionagent\nTable1andforgeneral-domainreasoninginTable2. byusingthecurriculumagentwithtools,canbeeffectively\ntransferredtogeneral-domaintasks.\nComparisonwithBaselines. Itsignificantlyoutperforms\nall compared baseline methods in both mathematics and\n4.3.Analysis\ngeneral-domain reasoning. On Qwen3-8B-Base, Agent0",
    "char_length": 1411
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 16,
    "text": "surpassesthepowerfuldata-freemethodR-Zeroby6.4% Inthissection,weprovideadetailedanalysisofeachmod-\nandoutperformstheself-playmethodAbsoluteZero,which ule’sperformance,alongwithaseriesofanalyticalexperi-\n6\nAgent0:UnleashingSelf-EvolvingAgentsfromZeroDataviaTool-IntegratedReasoning\nTable5.Evolution of Task Difficulty and Tool Use. We report\nthepassrateofthefixedExecutionAgent(fromIteration1)on\ndatasetsgeneratedbytheCurriculumAgentatdifferentstages.\nDataset PassRate(Executor ) Avg.ToolCalls\nIter1\nD 64.0 1.65\nIter1\nD 58.5 2.10\nIter2\nD 51.0 2.60\nIter3\nFigure4.Performance on mathematical and general reasoning\nco-evolutionaryloop.Withtheinvolvementoftools,thecur-\nbenchmarks,showingconsistentimprovementforbothQwen3-4B\nriculumagentprogressivelygeneratesmoredifficulttasks,\nandQwen3-8Bacrossthreeco-evolutionaryiterations.\nments,tobetterunderstandtheperformancegains. whiletheexecutionagentlearnstosolvethesetasksmore\nefficiently. Thisalsoconfirmsthatagentself-evolutionisa\nTable3. AblationstudyofAgent0.\nreasonableandpromisingdirection(Huangetal.,2025).\nMethod GeneralAVG MathAVG\nAgent0 36.7 58.2 Strategic Tool Inte- Table4.Comparison on non-tool\nCurriculumAgent grationMatters. Our andothertool-integratedbaselines.\nw/oTraining 29.5 46.8 advantageliesnotjust\nModel MATH General\nw/oToolReward 31.8 48.7\nin having a tool, but\nw/oRepetitionPenalty 31.3 47.9 Qwen3-4B 42.6 22.0\ninlearninghowtouse\nExecutionAgent w/oTool\nw/oADPO 34.9 56.2 it. As shown in Ta- +SPIRAL 47.0 30.0",
    "char_length": 1475
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 17,
    "text": "w/oMulti-turn 35.3 55.9 ble 4, merely provid- +R-Zero 49.1 29.8\nw/Tool\ning a tool (i.e., Base\nAblation Study. As shown in Table 3, we conducted a +TIR 44.2 25.7\nModelw/Tool)yields +AbsoluteZero 46.4 29.3\nseriesofablationexperimentstoevaluatetheimpactofeach +Agent0 52.5 32.6\na slight performance\ncomponent in our method. Specifically, we evaluate the\nboost. Agent0signifi-\nimpactof: (1)thecurriculumagent’straining,(2)thetool\ncantlyoutperformsothertool-usingbaselines,suchasAb-\nreward, (3) the repetition penalty, (4) our ambiguity scal-\nsoluteZero. Agent0alsosignificantlysurpassesnon-tool\ningmechanism,and(5)themulti-turnreasoningcapability.\nmethodslikeR-ZeroandSPIRAL.Thisindicatesthatour\nForthecurriculumagent,withouttraining,theperformance\ncurriculum agent, by using the R reward to explicitly\nsignificantlydropsby9.3%. Thisreflectsthevalueofthe tool\nincentivizethegenerationofcomplextasksrequiringtool\nlearnedcurriculum. Next,whenthetoolrewardisnotin-\nuse,isfarmoreeffectivethanmethodsthatonlyusetools\ncluded,themodel’sperformancedropsby7.2%. Thistests\nforvalidation(e.g.,AbsoluteZero)ordonotusetoolsatall\nourcorehypothesisthatexplicitlyrewardingtool-usetasks\n(e.g.,R-Zero). Furthermore,theexecutionagentutilizesthe\nis necessary. It shows a severe performance degradation\ntoolinconjunctionwithmulti-stepreasoning,whichalso\nwhenweremovethediversitycomponent,indicatingthat\nleadstoperformancegains,resultinginco-evolution.\nR ishighlyeffectiveforcurriculumdiversity,particularly\nrep",
    "char_length": 1486
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 18,
    "text": "forgeneraltasks. Asfortheexecutionagent,trainingitus- EvolutionofTaskDifficultyandToolUse.Weanalyzethe\ningtheoriginalGRPOwithstandardadvantageandclipping tasksgeneratedbythecurriculumagentduringthetraining\nresulted in a performance drop of 1.9%. This is because iterations. We sample 200 questions from each iteration\ntheoriginalalgorithmdoesnotaccountforthereliabilityof forthisanalysis. AsshowninTable5,thepassrateofthe\npseudo-labels,demonstratingtheeffectivenessofourpro- executionagent(fromIteration1)progressivelydecreases\nposedambiguityscalingmechanism. Theintroductionof when evaluated on task sets generated by the curriculum\nmulti-turnreasoningplayedasignificantroleinboosting agentsfromIterations1,2,and3. Thisindicatesthatthe\nAgent0’sperformance,especiallyforcomplexmathematical taskdifficultyisgraduallyincreasing,confirmingthatthe\nreasoningthatrequiresmulti-turnreasoning. curriculumadaptstotheimprovementintheexecutor’scapa-\nbilities. Moreimportantly,theaveragenumberoftoolcalls\nConsistent Improvement through Co-Evolution. As\npergeneratedtasksteadilyincreasesacrossiterations. This\nshown in Figure 4, our method demonstrates stable and\ndirectlyprovesthatourR rewardsuccessfullyguidesthe\nprogressiveimprovementduringtheiterativeprocess. On tool\ncurriculumagenttogeneratemorecomplexandtool-reliant\nQwen3-8B-Base, the average math score improved from\nproblems,therebydrivingavirtuouscycle.\n55.1 (Iter 1) to 56.5 (Iter 2), peaking at 58.2 (Iter 3). In",
    "char_length": 1463
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 19,
    "text": "additiontomathematics,Agent0showedthesimilartrend QualitativeAnalysis. Figure5illustratestheco-evolution\nonothergeneral-domainreasoningtasks,withanaverage of task complexity and solving proficiency. The Curricu-\nimprovementof2%periterationcomparedtotheprevious lumAgenteffectivelygeneratesincreasinglydifficultprob-\none. This iterative gain validates the effectiveness of our lems,progressingfrombasicgeometry(Iter1)tocomplex\n7\nAgent0:UnleashingSelf-EvolvingAgentsfromZeroDataviaTool-IntegratedReasoning\nGenerated Questions from Curriculum Agent Test Question (MATH)\nCompute: 1−2+3−4+5−…+99−100\nIter 1 Let ABC be a triangle with ∠A=90∘. Points D and E\nlie on sides AB and AC, respectively, such that Model Output by Agent0\nAD+EC=BC and AD⋅EC=BD⋅AE. Find the To solve the problem 1−2+3−4+5−…+99−100, we can observe\nmeasure of ∠B. the pattern in the series. The series alternates between adding and\nsubtracting consecutive integers.\nA rectangle R has sides of length 10 and 5. Point P lies We can group the terms in pairs to simplify the calculation:\nIter 2 inside R such that the distances from P to the four (1−2)+(3−4)+(5−6)+…+(99−100)\ns p i o d s e i s ti v o e f r R e a a l r e n u x m , y b ,1 e 0 rs − . F x i , n a d n t d h e 5 m − a y x , i w m h u e m re p x o a s n s d ib l y e a re t E o a t c a h l, t p h a e ir r e ( 2 a k re − 5 1 0 ) − pa 2 ir k s s . i T m h p e l r i e fi f e o s r e to , t − he 1 . s S u i m nc o e f t t h h e e r e s e a r r i e e s 1 i 0 s 0 : terms in",
    "char_length": 1495
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 20,
    "text": "value ofx2+y2. 50×(−1)=−50\nLet's confirm this with Python code.\nIn a particular city, each neighborhood is assigned a\nIter 3 distinct integer address from 1 to N. To ensure the ```python\nsafety of the city, the city council decides to organize # Calculate the sum of the series 1 - 2 + 3 - 4 + 5 - ... + 99 - 100\na unique parade route where no two adjacent\nresult = sum(i if i % 2 != 0 else -i for i in range(1, 101))\nneighborhoods should share a common digit in their\naddress. For example, if the parade starts at address print(result)\n15, the next possible stop cannot be any address ```\ncontaining the digits 1 or 5. What is the maximum ```output\nvalue of N such that the parade can visit all -50\nneighborhoods without breaking this rule? Express ```\nyour answer as a three-digit integer. The sum of the series 1−2+3−4+5−…+99−100 is \\boxed{-50}.\nFigure5.QualitativeCaseAnalysis.Left:ExamplesofgeneratedquestionsshowingaclearincreaseincomplexityanddiversityfromIter\n1toIter3.Right:AdemonstrationofAgent0’ssolvingprocess,utilizingahybridapproachofmathematicalreasoningandPythoncode\nexecutiontosolveastandardMATHproblem.\nconstraint satisfaction tasks (Iter 3). Simultaneously, the Tool-Integrated Reasoning (TIR). Applying Reinforce-\nExecutorAgent0demonstratesreliableproblem-solvingca- mentLearning(RL)(Jaechetal.,2024;Wangetal.,2025c;b;\npabilities. In the provided example, the agent effectively Zhouetal.,2025c;Wuetal.,2025a;Yangetal.) toenhance",
    "char_length": 1449
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 21,
    "text": "combines natural language reasoning to identify patterns LLMtool-useisagrowingfield. Manyapproachesrelyon\nwiththePythoncodeinterpretertoverifycalculations,vali- domain-specific data or supervised fine-tuning (Jin et al.,\ndatingthemodel’sabilitytohandlehybridreasoningtasks. 2025;Fengetal.,2025;Lietal.,2025b;Gengetal.,2025;\nHanetal.,2025b;Suetal.,2025). ThemoregeneralZero\nRLsetting,however,isnotoriouslyunstableinmulti-turn\n5.RelatedWork\nscenarios. RecentadvancesinTIRaddressthesechallenges\nSelf-Evolving from Zero Data. The paradigm of self- throughthreekeydimensions: stability,generalization,and\nevolution, where LLMs generate their own training data, complexity. Tostabilizelearningdynamics,methodslike\nhasgainedsignificanttraction(Liuetal.,2024;Dongetal., ASPO(Lin&Xu,2025)andSimpleTIR(Xueetal.,2025)\n2024; Fang et al., 2025; Yang et al., 2025b; Kuba et al., introducetheoreticalguaranteesandgradientfilteringfor\n2025).Thisapproachrangesfromdual-agent“Coder-Tester” voidturns.Beyondstability,(Chenetal.,2025)demonstrate\nsetupsinverifiabledomains(Linetal.,2025;Wangetal., thecross-domaintransferabilityoftool-useskills.Finally,to\n2025e)tofullyautonomousframeworks(Zhaoetal.,2025; handlecomplexmulti-turnscenarios,advancedtechniques\nHuangetal.,2025;Wangetal.,2025d;Liuetal.,2025b;Tao optimizeforlong-horizonplanning(Gaoetal.,2025;Erdo-\netal.,2024;Zhangetal.,2025a;Wuetal.,2025b;Luetal., ganetal.,2025),memorymanagement(Yanetal.,2025),",
    "char_length": 1439
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 22,
    "text": "2025)thatlearntogeneratenovelproblemsfromscratch. andinteractionefficiency(Wangetal.,2025a).\nToguidethislearning, manymethodsuselabel-freerein-\nforcementlearning,relyingonheuristicrewardsignalssuch 6.Conclusion\nasoutputconfidence(Lietal.,2025a)orconsistency(Zhang\netal.,2025b;Prabhudesaietal.,2025;Zuoetal.,2025;Yu We introduce Agent0, a fully autonomous framework\netal.,2025c). However,thesesystemsarecriticallylimited whereacurriculumagentandanexecutoragentco-evolve\nbythemodel’sinherentknowledge(Hanetal.,2025a;Xia withoutanyhuman-curateddata. Weintegratedacodein-\netal.,2025),causingcurriculumstagnationastasksrarely terpreterintotheloop,whichcreatesavirtuouscycle: the\nsurpass the model’s current complexity. Agent0 breaks tool-equippedexecutor’simprovingcapabilitiesdrivethe\nthiscapbyintegratinganexternaltool,providingexternal curriculumagenttogenerateprogressivelyhardertasks.Our\nproblem-solvingpower. However, withoutexternaltools, experimentsshowthatAgent0significantlyenhancesthe\nsuchclosed-loopsystemsriskmodecollapseandcurriculum reasoningabilitiesofbaseLLMs. Itdemonstratesascalable\nstagnation,astheyremainboundedbythemodel’sinherent andeffectivepathwayforevolvinghighlycapableagents,\nknowledge. Agent0breaksthisceilingbyintegratingan breakingthedependencyonhuman-annotateddatasets.\nexternaltooltointroduceobjectiveproblem-solvingpower.\n8\nAgent0:UnleashingSelf-EvolvingAgentsfromZeroDataviaTool-IntegratedReasoning\nAcknowledgement llm evaluation across 285 graduate disciplines. arXiv",
    "char_length": 1499
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 23,
    "text": "preprintarXiv:2502.14739,2025.\nWethankChengsongHuangforhelpfuldiscussions. This\nworkispartiallysupportedbytheAIforMathFundfromRe- Erdogan, L. E., Lee, N., Kim, S., Moon, S., Furuta, H.,\nnaissancePhilanthropy. TheAuthorsalsoacknowledgethe Anumanchipalli,G.,Keutzer,K.,andGholami,A. Plan-\nNationalArtificialIntelligenceResearchResource(NAIRR) and-act: Improvingplanningofagentsforlong-horizon\nPilot,PurdueAnvilAIforcontributingtothisresearchre- tasks. arXivpreprintarXiv:2503.09572,2025.\nsult.\nFang,W.,Liu,S.,Zhou,Y.,Zhang,K.,Zheng,T.,Chen,K.,\nSong,M.,andTao,D.Serl:Self-playreinforcementlearn-\nReferences\ningforlargelanguagemodelswithlimiteddata. arXiv\nAchiam,J.,Adler,S.,Agarwal,S.,Ahmad,L.,Akkaya,I., preprintarXiv:2505.20347,2025.\nAleman,F.L.,Almeida,D.,Altenschmidt,J.,Altman,S.,\nFeng,J.,Huang,S.,Qu,X.,Zhang,G.,Qin,Y.,Zhong,B.,\nAnadkat,S.,etal. Gpt-4technicalreport. arXivpreprint\nJiang,C.,Chi,J.,andZhong,W. Retool: Reinforcement\narXiv:2303.08774,2023.\nlearning for strategic tool use in llms. arXiv preprint\nAnthropic. Claude code, 2025. URL https://www. arXiv:2504.11536,2025.\nclaude.com/product/claude-code.\nGao,J.,Fu,W.,Xie,M.,Xu,S.,He,C.,Mei,Z.,Zhu,B.,\nBae,S.,Hong,J.,Lee,M.Y.,Kim,H.,Nam,J.,andKwak, andWu,Y. Beyondtenturns: Unlockinglong-horizon\nD. Onlinedifficultyfilteringforreasoningorientedre- agenticsearchwithlarge-scaleasynchronousrl. arXiv\ninforcementlearning. arXivpreprintarXiv:2504.03380, preprintarXiv:2508.07976,2025.\n2025.\nGeng,X.,Xia,P.,Zhang,Z.,Wang,X.,Wang,Q.,Ding,R.,",
    "char_length": 1497
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 24,
    "text": "Chen, Z., Yang, J., Xiao, T., Zhou, R., Zhang, L., Xi, X., Wang, C., Wu, J., Zhao, Y., Li, K., etal. Webwatcher:\nShi,X.,Wang,W.,andWang,J. Cantool-integratedre- Breakingnewfrontierofvision-languagedeepresearch\ninforcementlearninggeneralizeacrossdiversedomains? agent. arXivpreprintarXiv:2508.05748,2025.\narXivpreprintarXiv:2510.11184,2025.\nGoogle. Try deep research and our new ex-\nCheng,P.,Dai,Y.,Hu,T.,Xu,H.,Zhang,Z.,Han,L.,Du, perimental model in gemini, your ai assistant,\nN.,andLi,X. Self-playingadversariallanguagegame 2024. URL https://blog.google/products/\nenhancesllmreasoning. AdvancesinNeuralInformation gemini/google-gemini-deep-research/.\nProcessingSystems,37:126515–126543,2024.\nHan,S.,Liu,J.,Su,Y.,Duan,W.,Liu,X.,Xie,C.,Bansal,\nCheng,Y.,Chen,J.,Chen,J.,Chen,L.,Chen,L.,Chen,W., M.,Ding,M.,Zhang,L.,andYao,H. Alignmenttipping\nChen,Z.,Geng,S.,Li,A.,Li,B.,Li,B.,Li,L.,Liu,B., process: How self-evolution pushes llm agents off the\nLiu,J.,Liu,K.,Liu,Q.,Liu,S.,Liu,S.,Liu,T.,Liu,T., rails. arXivpreprintarXiv:2510.04860,2025a.\nLiu,Y.,Long,R.,Mai,J.,Ning,G.,Peng,Z.Y.,Shen,\nK., Su, J., Su, J., Sun, T., Sun, Y., Tao, Y., Wang, G., Han, S., Xia, P., Zhang, R., Sun, T., Li, Y., Zhu, H.,\nWang,S.,Wang,X.,Wang,Y.,Wang,Z.,Xia,J.,Xiang, and Yao, H. Mdocagent: A multi-modal multi-agent\nL., Xiao, X., Xiao, Y., Xi, C., Xin, S., Xu, J., Xu, S., frameworkfordocumentunderstanding. arXivpreprint\nYang,H.,Yang,J.,Yang,Y.,Yuan,J.,Zhang,J.,Zhang, arXiv:2503.13964,2025b.",
    "char_length": 1467
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 25,
    "text": "Y.,Zhang,Y.,Zheng,S.,Zhu,H.,andZhu,M. Fullstack\nHe,C.,Luo,R.,Bai,Y.,Hu,S.,Thai,Z.L.,Shen,J.,Hu,J.,\nbench: Evaluatingllmsasfullstackcoders,2025. URL\nHan,X.,Huang,Y.,Zhang,Y.,etal. Olympiadbench: A\nhttps://arxiv.org/abs/2412.00535.\nchallengingbenchmarkforpromotingagiwitholympiad-\nCobbe,K.,Kosaraju,V.,Bavarian,M.,Chen,M.,Jun,H., level bilingual multimodal scientific problems. arXiv\nKaiser,L.,Plappert,M.,Tworek,J.,Hilton,J.,Nakano, preprintarXiv:2402.14008,2024.\nR.,etal. Trainingverifierstosolvemathwordproblems.\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,\narXivpreprintarXiv:2110.14168,2021.\nM., Song, D., and Steinhardt, J. Measuring mas-\nDong, G., Lu, K., Li, C., Xia, T., Yu, B., Zhou, C., and sive multitask language understanding. arXiv preprint\nZhou, J. Self-play with execution feedback: Improv- arXiv:2009.03300,2020.\ninginstruction-followingcapabilitiesoflargelanguage\nHendrycks,D.,Burns,C.,Kadavath,S.,Arora,A.,Basart,\nmodels. arXivpreprintarXiv:2406.13542,2024.\nS.,Tang,E.,Song,D.,andSteinhardt,J.Measuringmath-\nDu,X.,Yao,Y.,Ma,K.,Wang,B.,Zheng,T.,Zhu,K.,Liu, ematicalproblemsolvingwiththemathdataset. arXiv\nM.,Liang,Y.,Jin,X.,Wei,Z.,etal. Supergpqa: Scaling preprintarXiv:2103.03874,2021.\n9\nAgent0:UnleashingSelf-EvolvingAgentsfromZeroDataviaTool-IntegratedReasoning\nHuang, C., Yu, W., Wang, X., Zhang, H., Li, Z., Li, R., Lin,H.andXu,Z. Understandingtool-integratedreasoning.\nHuang, J., Mi, H., and Yu, D. R-zero: Self-evolving arXivpreprintarXiv:2508.19201,2025.",
    "char_length": 1491
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 26,
    "text": "reasoningllmfromzerodata. 2025. URLhttps://\nLin,Z.,Shen,S.,Shang,J.,Weston,J.,andNie,Y.Learning\narxiv.org/abs/2508.05004.\ntosolveandverify: Aself-playframeworkforcodeand\nJaech,A.,Kalai,A.,Lerer,A.,Richardson,A.,El-Kishky, testgeneration. arXivpreprintarXiv:2502.14948,2025.\nA., Low, A., Helyar, A., Madry, A., Beutel, A., Car-\nney, A., et al. Openai o1 system card. arXiv preprint Liu, B., Guertler, L., Yu, S., Liu, Z., Qi, P., Balcells, D.,\narXiv:2412.16720,2024. Liu,M.,Tan,C.,Shi,W.,Lin,M.,Lee,W.S.,andJaques,\nN. Spiral: Self-playonzero-sumgamesincentivizesrea-\nJiang,D.,Lu,Y.,Li,Z.,Lyu,Z.,Nie,P.,Wang,H.,Su,A., soningviamulti-agentmulti-turnreinforcementlearning.\nChen,H.,Zou,K.,Du,C.,etal. Verltool: Towardsholis- arXivpreprintarXiv:2506.24119,2025a.\nticagenticreinforcementlearningwithtooluse. arXiv\npreprintarXiv:2509.01055,2025. Liu,B.,Jin,C.,Kim,S.,Yuan,W.,Zhao,W.,Kulikov,I.,Li,\nX.,Sukhbaatar,S.,Lanchantin,J.,andWeston,J. Spice:\nJimenez,C.E.,Yang,J.,Wettig,A.,Yao,S.,Pei,K.,Press,\nSelf-play in corpus environments improves reasoning.\nO.,andNarasimhan,K. Swe-bench: Canlanguagemod-\narXivpreprintarXiv:2510.24684,2025b.\nels resolve real-world github issues? arXiv preprint\narXiv:2310.06770,2023. Liu,Y.,Sun,P.,andLi,H. Largelanguagemodelsasagents\nintwo-playergames. arXivpreprintarXiv:2402.08078,\nJin, B., Zeng, H., Yue, Z., Yoon, J., Arik, S., Wang, D.,\n2024.\nZamani, H., and Han, J. Search-r1: Training llms to",
    "char_length": 1423
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 27,
    "text": "reasonandleveragesearchengineswithreinforcement Lu, H., Wen, Y., Cheng, P., Ding, R., Xu, H., Guo, J.,\nlearning. arXivpreprintarXiv:2503.09516,2025. Wang, C., Chen, H., Jiang, X., and Jiang, G. Search\nself-play:Pushingthefrontierofagentcapabilitywithout\nKazemi, M., Fatemi, B., Bansal, H., Palowitch, J., Anas-\nsupervision. arXivpreprintarXiv:2510.18821,2025.\ntasiou,C.,Mehta,S.V.,Jain,L.K.,Aglietti,V.,Jindal,\nD.,Chen,P.,etal. Big-benchextrahard. arXivpreprint OpenAI. Openai deep research system card,\narXiv:2502.19187,2025. 2025. URL https://openai.com/index/\nintroducing-deep-research/.\nKuba,J.G.,Gu,M.,Ma,Q.,Tian,Y.,andMohan,V. Lan-\nguage self-play for data-free training. arXiv preprint Ouyang,L.,Wu,J.,Jiang,X.,Almeida,D.,Wainwright,C.,\narXiv:2509.07414,2025. Mishkin,P.,Zhang,C.,Agarwal,S.,Slama,K.,Ray,A.,\net al. Training language models to follow instructions\nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,\nwithhumanfeedback. Advancesinneuralinformation\nC.H.,Gonzalez,J.E.,Zhang,H.,andStoica,I. Efficient\nprocessingsystems,35:27730–27744,2022.\nmemorymanagementforlargelanguagemodelserving\nwithpagedattention. InProceedingsoftheACMSIGOPS\nPapineni, K., Roukos, S., Ward, T., andZhu, W.-J. Bleu:\n29thSymposiumonOperatingSystemsPrinciples,2023.\na method for automatic evaluation of machine transla-\ntion. InProceedingsofthe40thannualmeetingofthe\nLewkowycz, A., Andreassen, A., Dohan, D., Dyer, E.,\nAssociationforComputationalLinguistics,pp.311–318,",
    "char_length": 1464
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 28,
    "text": "Michalewski, H., Ramasesh, V., Slone, A., Anil, C.,\n2002.\nSchlag,I.,Gutman-Solo,T.,etal. Solvingquantitative\nreasoningproblemswithlanguagemodels. Advancesin\nPrabhudesai, M., Chen, L., Ippoliti, A., Fragkiadaki, K.,\nneural information processing systems, 35:3843–3857,\nLiu, H., and Pathak, D. Maximizing confidence alone\n2022.\nimprovesreasoning. arXivpreprintarXiv:2505.22660,\nLi, P., Skripkin, M., Zubrey, A., Kuznetsov, A., and 2025.\nOseledets, I. Confidence is all you need: Few-shot\nQiu, J., Qi, X., Wang, H., Juan, X., Wang, Y., Zhao, Z.,\nrl fine-tuning of language models. arXiv preprint\nGeng, J., Guo, J., Li, P., Shi, J., et al. Alita-g: Self-\narXiv:2506.06395,2025a.\nevolving generative agent for agent generation. arXiv\nLi,X.,Zou,H.,andLiu,P. Torl: Scalingtool-integratedrl. preprintarXiv:2510.23601,2025a.\narXivpreprintarXiv:2503.23383,2025b.\nQiu,J.,Qi,X.,Zhang,T.,Juan,X.,Guo,J.,Lu,Y.,Wang,\nLi, Y., Shen, X., Yao, X., Ding, X., Miao, Y., Krishnan, Y., Yao, Z., Ren, Q., Jiang, X., etal. Alita: Generalist\nR., and Padman, R. Beyond single-turn: A survey on agentenablingscalableagenticreasoningwithminimal\nmulti-turninteractionswithlargelanguagemodels. arXiv predefinitionandmaximalself-evolution. arXivpreprint\npreprintarXiv:2504.04717,2025c. arXiv:2505.20286,2025b.\n10\nAgent0:UnleashingSelf-EvolvingAgentsfromZeroDataviaTool-IntegratedReasoning\nRein,D.,Hou,B.L.,Stickland,A.C.,Petty,J.,Pang,R.Y., Wang,H.,Qian,C.,Zhong,W.,Chen,X.,Qiu,J.,Huang,S.,",
    "char_length": 1459
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 29,
    "text": "Dirani, J., Michael, J., and Bowman, S. R. Gpqa: A Jin,B.,Wang,M.,Wong,K.-F.,andJi,H. Otc: Optimal\ngraduate-level google-proof q&a benchmark. In First toolcallsviareinforcementlearning. arXive-prints,pp.\nConferenceonLanguageModeling,2024. arXiv–2504,2025a.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Wang, H., Que, H., Xu, Q., Liu, M., Zhou, W., Feng, J.,\nKlimov, O. Proximal policy optimization algorithms. Zhong,W.,Ye,W.,Yang,T.,Huang,W.,etal. Reverse-\nArXiv preprint, abs/1707.06347, 2017. URL https: engineeredreasoningforopen-endedgeneration. arXiv\n//arxiv.org/abs/1707.06347. preprintarXiv:2509.06160,2025b.\nShao,Z.,Wang,P.,Zhu,Q.,Xu,R.,Song,J.,Bi,X.,Zhang, Wang, H., Xu, Q., Liu, C., Wu, J., Lin, F., and Chen, W.\nH.,Zhang,M.,Li,Y.,Wu,Y.,etal. Deepseekmath: Push- Emergent hierarchical reasoning in llms through rein-\ningthelimitsofmathematicalreasoninginopenlanguage forcement learning. arXiv preprint arXiv:2509.03646,\nmodels. arXivpreprintarXiv:2402.03300,2024. 2025c.\nSheng,G.,Zhang,C.,Ye,Z.,Wu,X.,Zhang,W.,Zhang,R., Wang, S., Jiao, Z., Zhang, Z., Peng, Y., Ze, X., Yang,\nPeng,Y.,Lin,H.,andWu,C. Hybridflow: Aflexibleand B., Wang, W., Wei, H., and Zhang, L. Socratic-zero:\nefficientrlhfframework. InProceedingsoftheTwentieth Bootstrappingreasoningviadata-freeagentco-evolution.\nEuropeanConferenceonComputerSystems,pp.1279– arXivpreprintarXiv:2509.24726,2025d.\n1297,2025.\nWang,X.,Li,B.,Song,Y.,Xu,F.F.,Tang,X.,Zhuge,M.,\nPan,J.,Song,Y.,Li,B.,Singh,J.,etal. Openhands: An",
    "char_length": 1494
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 30,
    "text": "Shi,T.,Wu,Y.,Song,L.,Zhou,T.,andZhao,J. Efficient\nopen platform for ai software developers as generalist\nreinforcementfinetuningviaadaptivecurriculumlearning.\narXivpreprintarXiv:2504.05520,2025. agents. arXivpreprintarXiv:2407.16741,2024a.\nWang,Y.,Ma,X.,Zhang,G.,Ni,Y.,Chandra,A.,Guo,S.,\nSrivastava,A.,Rastogi,A.,Rao,A.,Shoeb,A.A.M.,Abid,\nRen,W.,Arulraj,A.,He,X.,Jiang,Z.,etal. Mmlu-pro:\nA., Fisch, A., Brown, A. R., Santoro, A., Gupta, A.,\nAmorerobustandchallengingmulti-tasklanguageun-\nGarriga-Alonso, A., et al. Beyond the imitation game:\nderstandingbenchmark. AdvancesinNeuralInformation\nQuantifyingandextrapolatingthecapabilitiesoflanguage\nProcessingSystems,37:95266–95290,2024b.\nmodels. Transactions on machine learning research,\n2023.\nWang,Y.,Yang,L.,Tian,Y.,Shen,K.,andWang,M. Co-\nevolvingllmcoderandunittesterviareinforcementlearn-\nSu,Z.,Xia,P.,Guo,H.,Liu,Z.,Ma,Y.,Qu,X.,Liu,J.,Li,\ning. arXivpreprintarXiv:2506.03136,2025e.\nY.,Zeng,K.,Yang,Z.,etal. Thinkingwithimagesfor\nmultimodalreasoning: Foundations,methods,andfuture\nWu, F., Huang, X., Xuan, W., Zhang, Z., Xiao, Y., Wan,\nfrontiers. arXivpreprintarXiv:2506.23918,2025.\nG., Li, X., Hu, B., Xia, P., Leskovec, J., et al. Mul-\ntiplayer nash preference optimization. arXiv preprint\nTang,X.,Qin,T.,Peng,T.,Zhou,Z.,Shao,D.,Du,T.,Wei,\narXiv:2509.23102,2025a.\nX.,Xia,P.,Wu,F.,Zhu,H.,etal. Agentkb: Leveraging\ncross-domain experience for agentic problem solving. Wu,R.,Wang,X.,Mei,J.,Cai,P.,Fu,D.,Yang,C.,Wen,L.,",
    "char_length": 1468
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 31,
    "text": "arXivpreprintarXiv:2507.06229,2025. Yang,X.,Shen,Y.,Wang,Y.,etal.Evolver:Self-evolving\nllmagentsthroughanexperience-drivenlifecycle. arXiv\nTao, Z., Lin, T.-E., Chen, X., Li, H., Wu, Y., Li, Y., Jin,\npreprintarXiv:2510.16079,2025b.\nZ., Huang, F., Tao, D., and Zhou, J. A survey on\nself-evolutionoflargelanguagemodels. arXivpreprint Xia,P.,Wang,J.,Peng,Y.,Zeng,K.,Wu,X.,Tang,X.,Zhu,\narXiv:2404.14387,2024. H., Li, Y., Liu, S., Lu, Y., et al. Mmedagent-rl: Opti-\nmizingmulti-agentcollaborationformultimodalmedical\nTeam,T.D.,Li,B.,Zhang,B.,Zhang,D.,Huang,F.,Li,G.,\nreasoning. arXivpreprintarXiv:2506.00555,2025.\nChen,G.,Yin,H.,Wu,J.,Zhou,J.,etal. Tongyideepre-\nsearchtechnicalreport. arXivpreprintarXiv:2510.24701, Xue,Z.,Zheng,L.,Liu,Q.,Li,Y.,Zheng,X.,Ma,Z.,and\n2025. An, B. Simpletir: End-to-end reinforcement learning\nformulti-turntool-integratedreasoning. arXivpreprint\nTu,A.,Xuan,W.,Qi,H.,Huang,X.,Zeng,Q.,Talaei,S.,\narXiv:2509.02479,2025.\nXiao, Y., Xia, P., Tang, X., Zhuang, Y., etal. Position:\nThe hidden costs and measurement gaps of reinforce- Yan,S.,Yang,X.,Huang,Z.,Nie,E.,Ding,Z.,Li,Z.,Ma,\nment learning with verifiable rewards. arXiv preprint X.,Kersting,K.,Pan,J.Z.,Schu¨tze,H.,etal. Memory-\narXiv:2509.21882,2025. r1: Enhancinglargelanguagemodelagentstomanage\n11\nAgent0:UnleashingSelf-EvolvingAgentsfromZeroDataviaTool-IntegratedReasoning\nandutilizememoriesviareinforcementlearning. arXiv Zhao,A.,Wu,Y.,Yue,Y.,Wu,T.,Xu,Q.,Yue,Y.,Lin,M.,",
    "char_length": 1449
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 32,
    "text": "preprintarXiv:2508.19828,2025. Wang,S.,Wu,Q.,Zheng,Z.,andHuang,G. Absolute\nzero:Reinforcedself-playreasoningwithzerodata,2025.\nYang,A.,Li,A.,Yang,B.,Zhang,B.,Hui,B.,Zheng,B., URLhttps://arxiv.org/abs/2505.03335.\nYu,B.,Gao,C.,Huang,C.,Lv,C.,etal. Qwen3technical\nreport. arXivpreprintarXiv:2505.09388,2025a. Zhou,Y.,Levine,S.,Weston,J.,Li,X.,andSukhbaatar,S.\nSelf-challenginglanguagemodelagents. arXivpreprint\nYang,X.,Han,J.,Bommasani,R.,Luo,J.,Qu,W.,Zhou, arXiv:2506.01716,2025a.\nW.,Bibi,A.,Wang,X.,Yoon,J.,Stengel-Eskin,E.,etal.\nReliable and responsible foundation models. Transac- Zhou,Y.,Liang,Z.,Liu,H.,Yu,W.,Panaganti,K.,Song,L.,\ntionsonMachineLearningResearch. Yu,D.,Zhang,X.,Mi,H.,andYu,D. Evolvinglanguage\nmodelswithoutlabels: Majoritydrivesselection,novelty\nYang,Z.,Shen,W.,Chen,R.,Li,C.,Wan,F.,Yan,M.,Quan, promotes variation. arXiv preprint arXiv:2509.15194,\nX.,andHuang,F. Spell: Self-playreinforcementlearn- 2025b.\ning for evolving long-context language models. arXiv\nZhou, Y., Wang, Z., Wang, T., Xing, S., Xia, P., Li, B.,\npreprintarXiv:2509.23863,2025b.\nZheng,K.,Zhang,Z.,Chen,Z.,Zheng,W.,etal. Anypre-\nYu,H.,Chen,T.,Feng,J.,Chen,J.,Dai,W.,Yu,Q.,Zhang, fer: Anagenticframeworkforpreferencedatasynthesis.\nY.-Q.,Ma,W.-Y.,Liu,J.,Wang,M.,etal. Memagent:Re- arXivpreprintarXiv:2504.19276,2025c.\nshapinglong-contextllmwithmulti-convrl-basedmem-\nZuo,Y.,Zhang,K.,Sheng,L.,Qu,S.,Cui,G.,Zhu,X.,Li,\noryagent. arXivpreprintarXiv:2507.02259,2025a.\nH.,Zhang,Y.,Long,X.,Hua,E.,etal. Ttrl: Test-time",
    "char_length": 1499
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 33,
    "text": "Yu,Q.,Zhang,Z.,Zhu,R.,Yuan,Y.,Zuo,X.,Yue,Y.,Dai, reinforcementlearning.arXivpreprintarXiv:2504.16084,\nW.,Fan,T.,Liu,G.,Liu,L.,etal. Dapo: Anopen-source 2025.\nllmreinforcementlearningsystematscale.arXivpreprint\narXiv:2503.14476,2025b.\nYu, Z., Su, W., Tao, L., Wang, H., Singh, A., Yu, H.,\nWang,J.,Gao,H.,Yuan,W.,Weston,J.,etal. Restrain:\nFromspuriousvotestosignals–self-drivenrlwithself-\npenalization. arXivpreprintarXiv:2510.02172,2025c.\nYue,Y.,Chen,Z.,Lu,R.,Zhao,A.,Wang,Z.,Song,S.,and\nHuang,G. Doesreinforcementlearningreallyincentivize\nreasoningcapacityinllmsbeyondthebasemodel? arXiv\npreprintarXiv:2504.13837,2025.\nZhai,Y.,Tao,S.,Chen,C.,Zou,A.,Chen,Z.,Fu,Q.,Mai,\nS., Yu, L., Deng, J., Cao, Z., et al. Agentevolver: To-\nwardsefficientself-evolvingagentsystem. arXivpreprint\narXiv:2511.10395,2025.\nZhang,D.-C.,Zhao,Y.,Wu,J.,Zhang,L.,Li,B.,Yin,W.,\nJiang,Y.,Li,Y.-F.,Tu,K.,Xie,P.,etal. Evolvesearch:\nAniterativeself-evolvingsearchagent. InProceedings\nofthe2025ConferenceonEmpiricalMethodsinNatural\nLanguageProcessing,pp.13134–13147,2025a.\nZhang, K., Yao, Q., Liu, S., Wang, Y., Lai, B., Ye, J.,\nSong, M., and Tao, D. Consistent paths lead to truth:\nSelf-rewardingreinforcementlearningforllmreasoning.\narXivpreprintarXiv:2506.08745,2025b.\nZhang, K., Zuo, Y., He, B., Sun, Y., Liu, R., Jiang, C.,\nFan, Y., Tian, K., Jia, G., Li, P., et al. A survey of\nreinforcementlearningforlargereasoningmodels. arXiv\npreprintarXiv:2509.08827,2025c.\n12",
    "char_length": 1438
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 34,
    "text": "Agent0:UnleashingSelf-EvolvingAgentsfromZeroDataviaTool-IntegratedReasoning\nAppendix\nA.ExperimentalDetails\nA.1.HyperparameterSettings\nExecutorAgentTraining\n• GlobalBatchSize: 128\n• LearningRate: 1×10−6\n• WeightDecay: 1×10−2\n• KLPenaltyCoefficient(λ ): 1×10−2\nKL\n• MaxSteps: 40\n• NumberofRollouts: 16\n• RolloutTemperature: 1.0\n• RolloutTop-p: 0.99\nCurriculumAgentTraining\n• GlobalBatchSize: 128\n• LearningRate: 1×10−6\n• WeightDecay: 1×10−2\n• KLPenaltyCoefficient(λ ): 1×10−2\nKL\n• MaxSteps: 5\n• NumberofRollouts: 4\n• RolloutTemperature: 1.0\n• RolloutTop-p: 0.99\nA.2.Prompt\nThissectionpresentstheprompttemplatesusedfortheexecutorandcurriculumagent,andjudgepromptinTable6,Table7\nandTable8.\nTable6. Prompttemplateusedforexecutoragent.\nSystemPrompt:\nAconversationbetweenUserandAssistant. Theuserasksaquestion,andtheAssistantsolvesit. Theassistantfirst\nthinksaboutthereasoningprocessinthemindandthenprovidestheuserwiththeanswer. User: Pleaseintegrate\nnaturallanguagereasoningwithprogramstosolvetheproblemabove.Ifyouwanttorunanypythoncode,writecode\ninthepythonmarkdowncodeblockandtheexecutionwillbeappendedinanoutputcodeblocklike‘‘‘python\nyou code here‘‘‘‘‘‘output result here‘‘‘. Pleaseputyourfinalanswerwithinboxed{}.\nUserPrompt:\n{problem}\n13\nAgent0:UnleashingSelf-EvolvingAgentsfromZeroDataviaTool-IntegratedReasoning\nTable7. Prompttemplateusedforcurriculumagent.\nSystemPrompt:\nYouareanexpertcompetition-mathproblemsetter. FIRST,inyourprivatescratch-pad,thinkstep-by-steptodesign",
    "char_length": 1475
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 35,
    "text": "abrand-new,non-trivialproblem. Theproblemcouldcomefromanyfieldofmathematics,includingbutnotlimited\ntoalgebra, geometry, numbertheory, combinatorics, prealgebra, probability, statistics, andcalculus. Aimfora\ndifficultysuchthatfewerthan30%ofadvancedhigh-schoolstudentscouldsolveit. Avoidre-usingtextbookcliches\norfamouscontestproblems. THEN,withoutrevealinganyofyourprivatethoughts,outputexactlythefollowing\ntwoblocks:\n<question>\nThefullproblemstatementononeormorelines\n</question>\nboxed{final answer}\nDoNOToutputanythingelse—noexplanations,noextramarkup.\nUserPrompt:\nGenerateonenew,challengingreasoningquestionnow. Remembertoformattheoutputexactlyasinstructed.\nTable8.Prompttemplateusedforjudging.WeusetheGPT-4o(Achiametal.,2023)asthejudgemodel(temperature:0.1).\nSystemPrompt:\nYouareamathanswerchecker.\nUserPrompt:\nHi,thereisananswer: {answer},andthegroundtruthansweris: {response},pleasecheckwhethertheanswer\niscorrectornot,andreturnthe**only**YesorNo.\nA.3.SandboxConfiguration\nWeintegratedaPython-basedcodeexecutionsandbox(Chengetal.,2025)toenableverificationandalgorithmicreasoning.\nThesystemcomprisestwocorecomponents: aMulti-TurnInteractionProtocolandaDistributedExecutionOrchestrator,\nsupportedbyarobustinfrastructure.\nMulti-TurnInteractionProtocol. Weemploya“stop-and-go”strategytofacilitatemulti-stepreasoning: Themodelis\ninstructedtogeneratereasoningfollowedbyexecutablePythoncodewithinmarkdowndelimiters. Upondetectingacode",
    "char_length": 1433
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 36,
    "text": "blockviaregex,generationhalts. Thecodeisextractedandruninanisolatedsandbox,capturingstdoutorstderr. Execution\nresultsareappendedtotheconversationhistory. Themodeltheninterpretstheseresultstoderiveafinalanswerformattedin\nLaTeXboxednotation(boxed{···}).\nDistributed Execution Orchestrator. To manage parallel candidate generation (e.g., N = 10), we implemented a\nload-balancing mechanism: Execution is decoupled into isolated worker nodes to prevent interference with the main\ninferenceserver. Athread-safeRound-Robinschedulerdistributesrequestsacrossnodes. AThreadPoolExecutormanages\nasynchronouscalls,preventingmainloopblocking. Thesystemrobustlyhandlesnetworktimeoutsandfailures,feeding\nerrormessagesbacktothemodelforpotentialself-correction.\nInfrastructure. BuiltonFlaskandvLLM(Kwonetal.,2023),thesystemensureshighthroughput. Tomaintainstability,a\nbackgroundthreadperformslow-prioritytensoroperationsduringidleperiods,preventingGPUdeepsleepandensuring\nconsistentlatency.\nB.OverviewoftheBaselines\n• BaseModel: Thepre-trainedbasemodelwithoutanyfine-tuning.\n14\nAgent0:UnleashingSelf-EvolvingAgentsfromZeroDataviaTool-IntegratedReasoning\n• BaseModelw/tool: Thebasemodelevaluatedinazero-shotsetting,butgivenaccesstothecodeinterpreter.\n• R-Zero(Huangetal.,2025): Aself-evolvingframeworkthatoperatesfromzerodata,butdoesnotutilizeexternaltools.\n• AbsoluteZero(Zhaoetal.,2025): Aself-playmethodthatdoesuseacodeexecutorforverification,representingastrong\ntool-awarebaseline.",
    "char_length": 1467
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 37,
    "text": "• SPIRAL(Liuetal.,2025a): Aself-playmethodbasedonzero-sumgamesandmulti-turninteractions.\n• Socratic-Zero(Wangetal.,2025d): Astrongbaselinerepresentingmethodsthatleverageexternalproprietarymodelsfor\nreasoningassistance.\nC.EvaluationBenchmarks\n• AMC: A collection of problems from standard American middle and high school math competitions, serving as a\nfoundationalbenchmarkforpre-collegiatemathematicalreasoning.\n• Minerva(Lewkowyczetal.,2022): Thedatasetevaluatesthemodel’sabilitytohandleformalscientificnotationandsolve\ncomplexSTEM-relatedquestions.\n• MATH(Hendrycksetal.,2021):Acomprehensivedatasetofchallenginghighschoolcompetitionproblemsacrossvarious\nsubfields(e.g.,algebra,geometry),requiringcomplexheuristicsearchandmulti-stepderivation.\n• GSM8K(Cobbeetal.,2021): Aclassicbenchmarkconsistingofhigh-qualitygradeschoolmathwordproblemsthattest\nthemodel’sabilitytoperformmulti-steplogicusingbasicarithmeticoperations.\n• Olympiad-Bench(Heetal.,2024): AnadvancedbenchmarkaggregatingextremelydifficultproblemsfromChineseand\nInternationalMathematicalOlympiads,designedtoprobetheupperlimitsofLLMreasoningcapabilities.\n• AIME24&AIME25: Thesedatasetscompriseproblemsfromthe2024and2025AmericanInvitationalMathematics\nExaminations,servingasarigoroustestofadvancedproblem-solvingonrecent,likelyuncontaminateddata.\n• SuperGPQA(Duetal.,2025): AnevolutionoftheGPQAdataset(Reinetal.,2024),thisbenchmarkfeaturesdifficult",
    "char_length": 1410
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 38,
    "text": "graduate-levelquestionsacrossscientificdomainsthatarechallengingevenforexperts,specificallydesignedtominimize\ndatacontaminationandretrievalshortcuts.\n• MMLU-Pro(Wangetal.,2024b): AnenhancedversionoftheMMLUbenchmark(Hendrycksetal.,2020)thatintroduces\nharderquestions,increasedoptions,andmorecomplexreasoningrequirementstobetterdifferentiatebetweentop-tier\nlanguagemodels.\n• BBEH(Kazemietal.,2025): AselectedsubsetofchallengingtasksfromtheBig-Benchsuite(Srivastavaetal.,2023),\nfocusingonareaswherelanguagemodelstraditionallystruggle,suchassymbolicmanipulation,logicaldeduction,and\nalgorithmictracking.\nD.AdditionalResultsandAnalysis\nD.1.TheImpactoftheNumberofTurnsonPerformance\nWeinvestigatetheimpactoftheconversationlengthonmodelperformancebyincreasingtheinteractionturnsfrom1to4\nduringthecurriculumgenerationphase. AsshowninTable9,extendingthenumberofturnsyieldssignificantbenefits.\nComparedtothesingle-turnbaseline,the4-turnsettingimprovestheexecutor’soverallperformanceby3.4%,withspecific\ngainsof3%onmathematicalbenchmarksand2.6%ongeneraldomaintasks. Thisperformanceboostcanbeattributedto\ntheincreasedcomplexityofthecurriculum. Multi-turninteractionsencouragethecurriculumagenttogeneratetaskswith\nlongercontextdependenciesandprogressivedifficulty. Consequently,theexecutorisforcedtoenhanceitscapabilityto\nmaintainlogicalconsistencyandreasoningoverextendedhorizons,ratherthanrelyingonsimplepatternmatching.\nD.2.DetailedResults",
    "char_length": 1428
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 39,
    "text": "Foramoredetailedanalysis,wereportthecompleteresultsofthe3-iterationexperimentsinTable10andTable11. These\ntablesprovideacomprehensivebreakdownoftheagent’sperformanceacrossallindividualbenchmarks,furthervalidating\ntheeffectivenessandrobustnessofAgent0.\n15\nAgent0:UnleashingSelf-EvolvingAgentsfromZeroDataviaTool-IntegratedReasoning\nTable9.Ablationstudyonthenumberofinteractionturns.Increasingturnsfrom1to4leadstoconsistentperformancegainsacrossall\ndomains.\nNumberofTurns OverallAVG MathAVG GeneralAVG\n1 35.5 50.4 30.8\n2 35.8 50.7 31.1\n3 36.1 51.2 31.3\n4 36.7 51.9 31.6\nTable10.Comprehensiveresultsonmathematicalreasoningbenchmarks.Thepeakperformanceachievedduringeachmodel’straining\nprocessishighlightedinbold.\nModelName AVG AMC Minerva MATH GSM8K Olympiad AIME25 AIME24\nQwen3-4B-Base\nBaseModel ✗ ✗ 42.6 45.7 38.2 68.2 87.8 41.0 6.15 10.9\n+Agent0(Iter1) ✓ ✗ 51.9 59.8 55.0 79.9 92.6 46.1 13.0 16.8\n+Agent0(Iter2) ✓ ✗ 52.2 60.0 55.1 80.2 92.5 46.5 13.8 17.1\n+Agent0(Iter3) ✓ ✗ 52.5 60.6 55.6 80.5 92.6 46.7 14.1 17.4\nQwen3-8B-Base\nBaseModel ✗ ✗ 49.2 52.0 50.0 78.0 89.1 44.7 16.7 13.9\n+Agent0(Iter1) ✓ ✗ 55.1 57.3 59.0 81.6 93.9 48.4 20.9 24.9\n+Agent0(Iter2) ✓ ✗ 56.5 59.2 60.1 81.9 94.0 51.2 22.9 26.1\n+Agent0(Iter3) ✓ ✗ 58.2 62.4 61.3 82.4 94.5 54.0 24.8 28.0\nE.CaseAnalysis\nToprovidequalitativeevidenceofthemodel’sevolution,Table12,Table13,Table14,Table15,Table16,Table17,Table18,\nTable19,andTable20through9presentthreerepresentativequestionsgeneratedateachstagefromIteration1toIteration",
    "char_length": 1488
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 40,
    "text": "3. Weobserveaclearprogressionindifficulty: whiletheinitialiterationfeaturesrelativelystraightforwardqueries,the\ntasksinIteration3evolveintohighlycomplex,multi-stepproblemsrequiringdeepreasoning. Thisescalationisdrivenby\ntheco-evolutionarydynamic,wheretheCurriculumAgent,incentivizedtomaximizetheExecutor’slearningsignal,must\ncontinuouslypushthedifficultyfrontiertochallengetheExecutor’sexpandingproficiency,therebyeffectivelypreventing\nlearningstagnation.\n16\nAgent0:UnleashingSelf-EvolvingAgentsfromZeroDataviaTool-IntegratedReasoning\nTable11. Resultsongeneral-domainreasoningbenchmarks.\nModelName OverallAVG MATHAVG SuperGPQA MMLU-Pro BBEH\nQwen3-4B-Base\nBaseModel ✗ ✗ 27.1 42.6 20.9 37.4 7.57\n+Agent0(Iter1) ✓ ✗ 36.7 51.9 28.9 55.1 10.7\n+Agent0(Iter2) ✓ ✗ 36.9 52.2 29.3 55.3 11.0\n+Agent0(Iter3) ✓ ✗ 37.6 52.5 29.9 55.9 12.0\nQwen3-8B-Base\nBaseModel ✗ ✗ 34.5 49.2 28.3 51.8 8.6\n+Agent0(Iter1) ✓ ✗ 40.7 55.1 32.5 62.2 13.0\n+Agent0(Iter2) ✓ ✗ 41.3 56.5 32.5 62.4 13.6\n+Agent0(Iter3) ✓ ✗ 42.1 58.2 33.0 63.4 13.7\nTable12. SampledquestionsgeneratedbyCurriculumAgent(Iter1).\nQuestionsfromCurriculumAgent\nLetS bethesetofallpositiveintegersnforwhichthepolynomial\nP(x)=x3−2023x2+nx−1\nhasthreedistinctpositiveintegerroots. FindthesumofallelementsinS.\nTable13. SampledquestionsgeneratedbyCurriculumAgent(Iter1).\nQuestionsfromCurriculumAgent\nInatriangleABC withsidelengthsa,b,andc(wherea=BC,b=CA,andc=AB),lettheareabeK. Ifthe",
    "char_length": 1415
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 41,
    "text": "incircleofthetriangletouchesBC,CA,andAB atD,E,andF respectively,andthelengthsofBD,CE,and\nAF arex,y,andzrespectively,provethatx2+y2+z2 ≥ 3K.\n2\nTable14. SampledquestionsgeneratedbyCurriculumAgent(Iter1).\nQuestionsfromCurriculumAgent\nWhatistheminimumnumberofpointsinsideasquarewithsidelength1thatareneededtoensurethatatleasttwo\nofthepointsareatmost0.25unitsapartfromeachother?\nTable15. SampledquestionsgeneratedbyCurriculumAgent(Iter2).\nQuestionsfromCurriculumAgent\nOna9×9chessboard,initiallyonecellisblack. Ineachmove,youcanchooseawhitecellthathasatleastone\nblackcellinthesameroworcolumnandinvertthecolorofthatchosencellfromwhitetoblack. Determinethe\nminimumnumberofmovesrequiredtoturntheentirechessboardintoablackboard.\nTable16. SampledquestionsgeneratedbyCurriculumAgent(Iter2).\nQuestionsfromCurriculumAgent\nLetS ={1,2,3,...,100}. AsubsetAofSiscalled*good*ifforanyx,y ∈A(withx̸=y),thesumx+yisnot\naperfectsquare. Findthemaximumpossiblesizeofa*good*subsetofS.\n17\nAgent0:UnleashingSelf-EvolvingAgentsfromZeroDataviaTool-IntegratedReasoning\nTable17. SampledquestionsgeneratedbyCurriculumAgent(Iter2).\nQuestionsfromCurriculumAgent\nInthelandofPolytopia,eachcityisrepresentedbyauniquepointonalargesphericalmap. Thekingdecidesto\ncreateanewcityataspecialpointonthissphere. Todeterminethelocation,heusesasequenceofoperationsonthe\ncoordinatesofexistingcities.\nThemapisrepresentedbyaspherewheretheequationx2+y2+z2 =1holdsforanypoint(x,y,z)representing",
    "char_length": 1440
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 42,
    "text": "acity. Thekingchoosestwoexistingcities,AandB,withcoordinates(a ,a ,a )and(b ,b ,b ),respectively. He\n1 2 3 1 2 3\ndefinesanewcityCwithcoordinatescalculatedbytheformula:\na2+b2+a b ·(1+a b +a b +a b )\nc = i i i i 1 1 2 2 3 3\ni 1+a2+a2+a2+b2+b2+b2+(a b +a b +a b )2\n1 2 3 1 2 3 1 1 2 2 3 3\nfori=1,2,3.\n(cid:16) √ (cid:17) (cid:16) √ (cid:17)\nGiven that the coordinates of city A are 1,1, 2 and the coordinates of city B are −1,1, 2 , find the\n2 2 2 2 2 2\ncoordinatesofthenewcityC.\nTable18. SampledquestionsgeneratedbyCurriculumAgent(Iter3).\nQuestionsfromCurriculumAgent\nA sequence of positive integers a ,a ,a ,...,a is defined such that for each n ≥ 1, the number a is\n1 2√3 2024 n+1\ndeterminedbytherulea = a +⌊ a ⌋,startingwitha = 1. Findtheremainderwhena isdividedby\nn+1 n n 1 2024\n1000.\nTable19. SampledquestionsgeneratedbyCurriculumAgent(Iter3).\nQuestionsfromCurriculumAgent\nInaknockouttournamentwith2n players,wherenisapositiveinteger,eachmatcheliminatesoneplayer. The\ntournament is structured such that each round halves the number of players. What is the minimum number of\nmatches that must be played to determine the champion if, for each round, the number of matches played is a\nFibonaccinumber?\nTable20. SampledquestionsgeneratedbyCurriculumAgent(Iter3).\nQuestionsfromCurriculumAgent\nAcircleisdividedinto2023congruentarcs. Theendpointsofonearcarecoloredredandblue. Iftwopointsthat\narediametricallyoppositearebothcoloredred,whatistheprobabilitythatthetwoendpointsofthearcdirectly",
    "char_length": 1486
  },
  {
    "paper_id": "Agent0",
    "chunk_id": 43,
    "text": "adjacenttotheredendpointsarebothblue? Expressyouranswerasacommonfraction.\n18",
    "char_length": 77
  }
]