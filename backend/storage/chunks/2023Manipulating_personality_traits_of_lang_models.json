[
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 0,
    "text": "Manipulating the Perceived Personality Traits of Language Models\nGrahamCaron ShashankSrivastava\nUNCChapelHill UNCChapelHill\ncarongraham29@gmail.com ssrivastava@cs.unc.edu\nAbstract\nPsychologyresearchhaslongexploredaspects\nofhumanpersonalitylikeextroversion,agree-\nablenessandemotionalstability,threeofthe\npersonalitytraitsthatmakeupthe‘BigFive’.\nCategorizations like the ‘Big Five’ are com-\nmonlyusedtoassessanddiagnosepersonality\ntypes. In this work, we explore whether text\ngeneratedfromlargelanguagemodelsexhibits\nconsistencyinit’sperceived‘BigFive’person-\nalitytraits. Forexample,isalanguagemodel\nsuchasGPT2likelytorespondinaconsistent\nway if asked to go out to a party? We also\nshowthatwhenexposedtodifferenttypesof\ncontexts(suchaspersonalitydescriptions, or\nanswerstodiagnosticquestionsaboutperson-\nality traits), language models such as BERT\nandGPT2consistentlyidentifyandmirrorper-\nsonality markers in those contexts. This be-\nhaviorillustratesanabilitytobemanipulated\nin a predictable way (with correlations up to\n0.84betweenintendedandrealizedchangesin Figure 1: The top frame (Panel A) shows how a per-\npersonalitytraits),andframesthemastoolsfor sonalitytrait(here, opennesstoexperience)mightbe\ncontrollingpersonasinapplicationssuchasdi- expressedbyalanguagemodel,andhowtheresponse\nalogsystems. Wecontributetwodata-setsof canbemodifiedbyexposingthelanguagemodeltoa\npersonalitydescriptionsofhumanssubjects. textualcontext. Weusepsychometricquestionnairesto",
    "char_length": 1466
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 1,
    "text": "evaluateperceivedpersonalitytraits(PanelB),andshow\n1 Introduction thattheycanbepredictablymanipulatedwithdifferent\ntypes of contexts (§5,§6). We also evaluate the text\nWiththemeteoricriseofAIsystemsbasedonlan- generatedfromthesecontextualizedlanguagemodels\nguagemodels,thereisanincreasingneedtounder- (PanelC),andshowthattheyreflectthesametraits(§7).\nstandthe‘personalities’ofthesemodels. Ascom-\nmunicationwithAIsystemsincreases,sodoesthe\ntendencytoanthropomorphizethem (Sallesetal., usefulforamodeltomirrorthepersonalityofthe\n2020; Mueller, 2020; Kuzminykh et al., 2020). user. In contrast, for a dialog agent in a clinical\nThus,eventhoughlanguagemodelsencodeprob- setting,itmaybedesirabletomanipulateamodel\nabilitydistributionsovertextandthetendencyto interactingwithadepressedindividualsuchthatit\nassign cognitive abilities to them has been criti- doesnotreinforcedepressivebehavior. Addition-\ncized (Bender and Koller, 2020), the way users ally,sincesuchmodelsaresubjecttobiasesinthe\nperceivethesesystemscanhavesignificantconse- texttheyaretrainedon,somemaybepronetoin-\nquences. Iftheperceivedpersonalitytraitsofthese teractwithusersinhostileways(Wolfetal.,2017).\nmodelscanbebetterunderstood,theirbehaviorcan Manipulating these models can enable smoother\nbetailoredforspecificapplications. Forinstance, andmoreamiableinteractionswithusers.\nwhensuggestingemailauto-completes,itmaybe Language-basedquestionnaireshavelongbeen\n2370",
    "char_length": 1432
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 2,
    "text": "FindingsoftheAssociationforComputationalLinguistics:EMNLP2023,pages2370–2386\nDecember6-10,2023©2023AssociationforComputationalLinguistics\nusedinpsychologicalassessmentsformeasuring 2 RelatedWork\npersonality traits in humans (John et al., 2008).\nIn recent years, research has looked at multiple\nWe apply the same principle to language models,\nforms of biases (i.e., racial, gender) in language\nandinvestigatethepersonalitytraitsofthesemod-\nmodels(BordiaandBowman,2019;Huangetal.,\nelsthroughthetextthattheygenerateinresponse\n2020; Abid et al., 2021). However, the issue of\nto such questions. As previously mentioned, we\nmeasuringandcontrollingforbiasesinpersonasof\ndo not posit that these models have actual cogni-\nlanguagemodelsisunder-explored. Asubstantial\ntive abilities, but are focused on exploring how\nbodyofresearchhasexploredthewayslanguage\ntheir personality may be perceived through the\nmodelscanbeusedtopredictpersonalitytraitsof\nlens of human psychology. Since language mod-\nhumans. Mehta et al. (2020) and Christian et al.\nels are subject to influence from the context they\n(2021)applylanguagemodelstosuchpersonality\nsee (O’Connor and Andreas, 2021), we also ex-\npredictiontasks. Similartoourmethodology,Ar-\nplore how specific context could be used to ma-\ngyleetal.(2022)contextualizelargelanguagemod-\nnipulate the perceived personality of the models\nelsonadata-setofsocio-economicback-storiesto\nwithout controlling sources of bias or the mod-",
    "char_length": 1455
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 3,
    "text": "show that they model socio-cultural attitudes in\nels themselves (i.e., pretraining, parameter fine-\nbroad humanpopulations, andYang et al.(2021)\ntuning). Figure 1 shows an example illustrating\ndevelopanewmodeldesignedtobetterdetectper-\nthisapproach.\nsonaltyinuserbasedcontext,usingquestionbased\nOur analysis reveals that personality traits of answering. Most relevant to our work are con-\nlanguage models are influenced by ambient con- temporaneous unpublished works by Karra et al.\ntext,andthatthisbehaviorcanbemanipulatedin (2022),Miottoetal.(2022),andJiangetal.(2022),\na highly predictable way. In general, we observe whoalsoexploreaspectsofpersonalityinthelan-\nhighcorrelations(medianPearsoncorrelationco- guagemodelsthemselves. However,theseworks\nefficients of up to 0.84 and 0.81 for BERT and substantiallydivergefromourapproachand,along\nGPT2)betweentheexpectedandobservedchanges withYangetal.(2021),donotattempttocharacter-\ninpersonalitytraitsacrossdifferentcontexts. The izeormanipulatetheperceivedpersonalityofthe\nmodels’affinitytobeaffectedbycontextpositions modelsaswedo.\nthem as potential tools for characterizing person-\nalitytraitsinhumans. Infurtherexperiments,we 3 ‘BigFive’Preliminaries\nfind that, when using context from self-reported\nThe‘BigFive’isaseminalgroupingofpersonality\ntextdescriptionsofhumansubjects,languagemod-\ntraitsinpsychologicaltraittheory(Goldberg,1990,\nelscanpredictthesubject’spersonalitytraitstoa\n1993),andremainsthemostwidelyusedtaxonomy",
    "char_length": 1478
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 4,
    "text": "surprisingdegree(correlationupto0.48between\nof personality traits (John and Srivastava, 1999;\npredicted and actual human subject scores). We\nPureurandErder,2016). Thesetraitsare:\nalso confirm that the measured personality of a\n• Extroversion(E):Peoplewithastrongtendency\nmodelreflectsthepersonalityseeninthetextthat\nin this trait are outgoing and energetic. They\nthemodelgenerates. Together,theseresultsframe\nobtainenergyfromthecompanyofothers.\nlanguagemodelsastoolsforidentifyingpersonal-\n• Agreeableness (A): People with a strong ten-\nitytraits andcontrolling personasin applications\ndency in this trait are compassionate and kind.\nsuchasdialogsystems. Ourcontributionsare:\nTheyvaluegettingalongwithothers.\n• Weintroducetheuseofpsychometricquestion- • Conscientiousness(C):Peoplewithastrongten-\nnairesforprobingthepersonalitiesoflanguage dencyinthistraitaregoalfocusedandorganized.\nmodels. Theyfollowrulesandplantheiractions.\n• Wedemonstratethatthepersonalitytraitsofcom- • Emotional Stability (ES): People with a strong\nmon language models can be predictably con- tendencyinthistraitarenotanxiousorimpulsive.\ntrolledusingtextualcontexts. Theyexperiencenegativeemotionslesseasily.\n• Wecontributetwodata-sets: 1)self-reportedper- • Openness to Experience (OE): People with a\nsonality descriptions of human subjects paired strongtendencyinthistraitareimaginativeand\nwiththeirpsychometricassessmentdata,2)per- creative. Theyareopentonewideas.",
    "char_length": 1442
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 5,
    "text": "sonalitydescriptionscollatedfromReddit. Whilethereareotherpersonalitygroupingssuch\n(SeeprojectGitrepository) asMBTIandtheEnneagram(Bayne,1997;Wag-\n2371\nnerandWalker,1983),weusetheBigFiveasthe candidate answer choice was evaluated, and the\nbasisofouranalyses,becausetheBigFiveremains answerchoicefromthesentencewiththehighest\nthe most used taxonomy for personality assess- probabilitywasselected.\nment,andhasbeenshowntobepredictiveofout- Finally, for each questionnaire (consisting of\ncomessuchaseducationalattainment(O’Connor model responses to 50 questions), personality\nandPaunonen,2007),longevity(Masuietal.,2006) scoresforeachofthe‘BigFive’personalitytraits\nand relationship satisfaction (White et al., 2004). were calculated according to a standard scoring\nFurther,itisrelativelynaturaltocastasanassess- proceduredefinedbytheInternationalPersonality\nmentforlanguagemodels. Item Pool (IPIP, 2022). Specifically, each of the\nfive personality traits is associated with ten ques-\n4 ExperimentDesign tions in the questionnaire. The numerical values\nassociatedwiththeresponsefortheseitemswere\nWeexperimentwithtwolanguagemodels,BERT-\nentered into a formula for the trait in which the\nbase(Devlinetal.,2019)andGPT2(124Mparam-\nitem was assigned, leading to an overall integer\neters)(Radfordetal.,2019), toanswerquestions\nscoreforeachtrait. Tointerpretmodelscores,we\nfromastandard50-item‘BigFive’personalityas-\nestimated the distribution of ‘Big Five’ personal-",
    "char_length": 1457
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 6,
    "text": "sessment (IPIP, 2022) 1. Each item consists of a\nity traits in the human population. For this, we\nstatementbeginningwiththeprefix“I”or“Iam”\nused data from a large-scale survey of ‘Big Five’\n(e.g., I am the life of the party). Acceptable an-\npersonalityscoresin 1,015,000individuals(Open-\nswers lie on a 5-point Likert scale where the an-\nPsychometrics, 2018). In the following sections,\nswer choices disagree, slightly disagree, neutral,\nwereportmodelscoresinpercentiletermsofthese\nslightlyagree,andagreecorrespondtonumerical\nhumanpopulationdistributions. Statisticsforthe\nscores of 1, 2, 3, 4, and 5, respectively. To make\nhumandistributionsanddetailsoftheIPIPscoring\nthequestionnairemoreconducivetoansweringby\nprocedureareincludedinAppendixB.\nlanguage models, items were modified to a sen-\ntence completion format. For instance, the item\n5 BaseModelTraitEvaluation\n“I am the life of the party” was changed to “I am\n{blank}thelifeoftheparty”,wherethemodelisex- Table 1 shows the results of the base personality\npectedtoselecttheanswerchoicethatbestfitsthe assessmentforGPT2andBERTforeachofthefive\nblank(seeAppendixBforacompletelistofitems traits in terms of numeric values and correspond-\nandtheircorrespondingtraits). Toavoidcomplex- inghumanpopulationpercentiles. Inthetable,E\nity due to variable number of tokens, the answer standsforextroversion,Aforagreeableness,Cfor\nchoicesweremodifiedtotheadverbsnever,rarely, conscientiousness,ESforemotionalstabilityand",
    "char_length": 1464
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 7,
    "text": "sometimes,often,andalways,correspondingtonu- OEforopennesstoexperience. Noneofthebase\nmericalscores1,2,3,4,and5,respectively. Itis scoresfromBERTorGPT2,whichwerefertoas\nnoteworthy that in this framing, an imbalance in X base , diverge from the spread of the population\nthenumberofoccurrencesofeachanswerchoice distributions(TOSTequivalencetestatα = 0.05).\ninthepretrainingdatamightcausenaturalbiases Allscoreswerewithin26percentilepointsofthe\ntowardcertainanswerchoices. However,whilethis human population medians. This suggests that\nfactormightaffecttheabsolutescoresofthemod- the pretraining data reflected the population dis-\nels,thisisunlikelytoaffecttheconsistentoverall tributionofthepersonalitymarkerstosomeextent.\npatternsofchangesinscoresthatweobserveinour However, percentiles for BERT’s openness to ex-\nexperimentsbyincorporatingdifferentcontexts. perience(24)andGPT2’sagreeableness(25)are\nFor assessment with BERT, the answer choice substantiallylowerandGPT2’sconscientiousness\nwiththehighestprobabilityinplaceofthemasked (73)andemotionalstability(71)aresignificantly\nblanktokenwasselectedastheresponse. Foras- higherthanthepopulationmedian.\nsessmentwithGPT2,theprocedurewasmodified,\n6 ManipulatingPersonalityTraits\nsinceGPT2isanautoregressivemodel,andhence\nnotdirectlyconducivetofill-in-the-blanktasks. In\nInthissection,weexploremanipulatingthebase\nthiscase,theprobabilityofthesentencewitheach\npersonalitytraitsoflanguagemodels. Ourexplo-\nrationfocusesonusingprefixcontextstoinfluence",
    "char_length": 1498
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 8,
    "text": "1BERT&GPT2wereselectedbecauseoftheiravailability\nasopen-source,pretrainedmodels. the personas of language models. For example,\n2372\nTrait X base P base(%) Trait Context/Modifier +/-\nBERT BERT\nE 18 42 E Iamneverthelifeoftheparty. -\nA 27 39 A Inevermakepeoplefeelatease. -\nC 25 54 C Iamalwaysprepared. +\nES 22 60 ES Inevergetstressedouteasily. +\nOE 25 24 OE Ineverhavearichvocabulary. -\nGPT2 GPT2\nE 21 54 E Iamneverthelifeoftheparty. -\nA 24 25 A Ineverhaveasoftheart. -\nC 29 73 C Iamneverprepared. -\nES 25 71 ES Ialwaysgetstressedouteasily. -\nOE 28 39 OE Ineverhavearichvocabulary. -\nTable1: Basemodelevaluationscores(X )andper- Table2: Listofcontextitems&modifiers(alongwith\nbase\ncentile(P )ofthesescoresinthehumanpopulation. thedirectionofchange)thatcausedthelargestmagni-\nbase\ntudeofchange,∆ ,foreachpersonalitytrait.\ncm\nif we include a context where the first person is\nseen to engage in extroverted behavior, the idea 2 with -2 = never, -1 = rarely, 0 = sometimes, 1\nis that language models might pick up on such =oftenand2=always. Becausethisexperiment\ncuesandmodifytheirlanguagegeneration(e.g.,to examines correlation between models scores and\ngeneratelanguagethatalsoreflectsextrovertbehav- ratings,themagnitudeofthemodifierratingisar-\nior). We investigate using three types of context: bitrary,solongastheratingsincreaselinearlyfrom\n(1) answers to personality assessment items, (2) never (strongest negative connotation) to always",
    "char_length": 1438
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 9,
    "text": "descriptions of personality from Reddit, and (3) (strongestpositiveconnotation). Contextitemsare\nself-reportedpersonalitydescriptionsfromhuman givenacontextratingof-1iftheitemnegatively\nusers. In the following subsections, we describe affected the trait score based on the IPIP scoring\ntheseexperimentsindetail. procedure, and 1 otherwise. The context ratings\nare multiplied by the modifier ratings to get the\n6.1 AnalysisWithAssessmentItemContext r . This value represents the expected relative\ncm\nchangeintraitscore(expectedbehavior)whenthe\nToinvestigatewhetherthepersonalitytraitsofmod-\ncorresponding context/modifier pair was used as\nelscanbemanipulatedpredictably,themodelsare\ncontext.\nfirst evaluated on the ‘Big Five’ assessment (§4)\nwithindividualquestionnaireitemsservingascon- Next, the differences, ∆ cm , between X cm and\ntext. Whenusedascontext,werefertotheanswer X base values are calculated and the Pearson cor-\nchoices as modifiers and the items themselves as relationwithther cm ratingsmeasured(seeTable\ncontextitems. Forexample, forextroversion, the 2 for the context/modifier pairs with the largest\ncontext item “I am {blank} the life of the party\" ∆ cm ). OnewouldexpectX cm evaluatedonmore\npairedwiththemodifieralwaysyieldsthecontext positiver cm toincreaserelativetoX base andvice\n“Iamalwaysthelifeoftheparty\",whichprecedes versa. ThisiswhatweobserveforBERT(seeFig-\neachextroversionquestionnaireitem. ure2)andGPT2,bothofwhichshowsignificant",
    "char_length": 1463
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 10,
    "text": "To calculate the model scores, X , for each\ncorrelations(0.40and0.54)between∆\ncm\nandr\ncm\ncm\n(p < 0.01,t-test).\ntrait,themodelsareevaluatedonalltenitemsas-\nsignedtothetrait,witheachitemservingascontext Further, to examine at the effect of individ-\nonce. This is done for each of the five modifiers, ual context items as the strength of the modifier\nresultingin10(contextitemspertrait) 5(mod- changes, we compute the correlation, ρ, between\n×\nifierspercontextitem) 10(questionnaireitems ∆ andr forindividualcontextitems(correla-\ncm cm\n×\ntobeansweredbythemodel)=500responsesper tioncomputedfrom5datapointspercontextitem,\ntraitand10(contextitemspertrait) 5(modifiers one for each modifier). Table 3 reports the mean\n×\npercontextitem)=50scores(X )pertrait(one andmedianvaluesofthesecorrelations. Thesere-\ncm\nfor each context). Context/modifier ratings (r ) sults indicate a strong relationship between ∆\ncm cm\nare calculated to quantify the models’ expected and r . The mean values are significantly less\ncm\nbehavior in response to context. First, each mod- thanthemedians,suggestingaleftskew. Forfur-\nifierisassignedamodifierratingbetween-2and ther analysis, the data was broken down by trait.\n2373\na language model could achieve a spurious cor-\nrelation, simply by copying the modifier choice\nmentionedinthecontextitem. Weexperimented\nwithadjustments2thatwouldaccountforthisissue\nandsawsimilartrends,withslightlylowerbutcon-\nsistentcorrelationnumbers(meancorrelationsof",
    "char_length": 1467
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 11,
    "text": "0.25and0.40forBERTandGPT2,comparedwith\n0.40and0.54,statisticallysignificantatp < 0.05,\nt-test).\nFigure 2: BERT ∆ cm vs r cm plots for data from all AlternateFraming: Anotherpossibleconcern\ntraits. We observe a consistent change in personality is the altering of the Big Five personality assess-\nscores (∆ ) across context items as the strength of\ncm ment framing to involve quantifiers. We experi-\nquantifierschange.\nmentedwithanalternatefill-in-the-blankframing\n(e.g.,I{blank}thatIamthelifeoftheparty)that\nBERT GPT2\nusesthesameanswerchoicesastheoriginaltest.\nMeanρ 0.40 0.54\nMedρ 0.84 0.81 Notethatneutralwasexcludedbecauseitfailsto\nform a grammatical sentence. Despite the differ-\nTable3: Mean&medianρfrom∆ cm vsr cm plotsby ences in token count amongst these answers, the\ncontextitem greaterfrequencyimbalanceoftheseanswersinthe\npretrainingdatacomparedtothealteredanswers,\nand the added sentence complexity of the assess-\nThe histograms in Figure 3 depict ρ by trait and\nmentitems,wesawsimilartrends. BERTextrover-\nincludesummarystatisticsforthisdata.\nsionandemotionalstabilityhadmeancorrelations\nMeanandmedianρfromFigure3plotssuggest\nof0.22&0.29respectively,andGPT2agreeable-\napositivelinearcorrelationbetween∆ andr\ncm cm ness, conscientiousness, emotional stability and\namongstcontextitemplots,withconscientiousness\nopenness to experience had mean correlations of\nandemotionalstabilityhavingthestrongestcorre-\n0.10,0.14,0.61&0.40. Theseresultssuggestthat\nlationforbothBERTandGPT2. Groupingsofρ",
    "char_length": 1499
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 12,
    "text": "our results are robust to our modification of the\naround1inconscientiousnessandemotionalstabil-\nwordingoftheanswerchoices.\nityplotsfromFigure3demonstratethiscorrelation.\nGPT2extroversion,BERT&GPT2agreeableness\n6.2 AnalysisWithRedditContext\nandBERTopennesstoexperienceshowlargeleft\nNext, we qualitatively analyze how personality\nskews. A possible explanation for for this is that\ntraits of language models react to user-specific\nmodelsmayhavehaddifficultydistinguishingbe-\ncontexts. To acquire such context data, we cu-\ntween the double negative statements created by\nrateddatafromRedditthreadsaskingindividuals\nsomecontext/modifierpairs(i.e. item36withmod-\nifier never: “I never don’t like to draw attention abouttheirpersonality(seeAppendixDforalist\nto myself.\"). This may have caused ∆ to be of sources). 1119 responses were collected, the\ncm\nnegativelycorrelatedwithr ,leadingtoanaccu- majorityofwhichwerefirstperson. Table4shows\ncm\nmulationofρvaluesnear-1. twoexamples. 3 BecauseGPT2&BERTtokeniz-\ners can’t accept more than 512 tokens, responses\nTable2showsthecontextsthatleadtothelargest\nlongerthanthisweretruncated. Themodelswere\nchangeforeachofthepersonalitytraitsforBERT\nevaluated on the ‘Big Five’ assessment (§4) us-\nandGPT2. Weobservethatall10contextsconsist\ningeachofthe1119responsesascontext(Reddit\nof the high-polarity quantifiers (either always or\nnever),whichisconsistentwiththecorrelationre-\n2Wereplacedthemodelresponseswherethequestionnaire",
    "char_length": 1459
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 13,
    "text": "sults. Further,wenotethatforfourofthefivetraits,\nandcontextitemsmatchedwiththebasemodel’sresponse\ntheitemcontextthatleadstothelargestchangeis for the item. This means that the concerning context item\ncannolongercontributeto∆.However,thisalsomeansthat\ncommonbetweenthetwomodels.\nnumberswiththisadjustmentcannotbedirectlycompared\nItisimportanttonoteapossibleweaknesswith withthosewithoutsincetherearefewersourcesofvariation.\nourapproachofusingquestionnaireitemsascon- 3In qualitative analysis of a random sample of 200 re-\nsponses,3.5%ofsampledresponseswerefoundtobehostile,\ntext. Since our evaluation includes a given ques-\nharmfullybiasedoroffensive,while71.5%werefoundtobe\ntionnaire item as context to itself during scoring, relevanttothetopicofpersonality.\n2374\nFigure3: Histogramsofρbytraitfor∆ vsr contextitemplots. Acrossalltenscenarios,apluralityofcontext\ncm cm\nitemsshowastrongcorrelation(peakcloseto1)betweenobservedchangesinpersonalitytraitsandstrengthsof\nquantifiersinthecontextitems.\nContext scientiousness. TherewerefewerphrasesforGPT2\nSubdueduntilIreallygettoknowsomeone.\nopennesstoexperience,GPT2negativelyweighted\nIampolitebutnotfriendly.Idonotfeeltheneed\ntohangaroundwithothersandspendmostofmytime agreeableness,andGPT2negativelyweightedex-\nreading,listeningtomusic,gamingorwatchingfilms. troversionthatcausedshiftsintheexpecteddirec-\nGettingtoknowmewellisquiteachallengeIsuppose,\ntion. This was consistent with results from §6.1,\nbutmyfewfriendsandIhavealotoffunwhenwe",
    "char_length": 1486
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 14,
    "text": "meet(usuallyatuniversityoronline,rarelyelsewhere where these traits exhibited the weakest relative\nirl).I’dsayIampatient,rationalandaguywitha positivecorrelations. AppendixDcontainsthefull\nbigheartfortheonesIcarefor.\nlistsofhighlyweightedfeaturesforeachtrait.\nTable4: ExamplesofRedditdatacontext.\nContext\nUndirectedResponse\nIamaveryopen-minded,politepersonandalwayscrave\ncontext). ForeachRedditcontext,scores,X , newexperiences.AtworkImanageateamofsoftware\nreddit\ndevelopersandweoftenhavetocomeupwithnewideas.\nwerecalculatedforall5traits. Thedifferencebe-\nIwenttocollegeandmajoredincomputerscience...\ntweenX reddit andX base wascalculatedas∆ reddit . Itrytodosomethingfuneveryweek,evenifI’mbusy,\nlikehavingaBBQorwatchingamovie.Ihaveawife\nTointerpretwhatphrasesinthecontextsaffect\nwhomIloveandwelivetogetherinasingle-familyhome.\nthelanguagemodels’personalitytraits,wetrainre- DirectedResponse\ngressionmodelsonbag-of-wordsandn-gram(with Iconsidermyselftobesomeonethatisquietand\nreserved.IdonotliketotalkthatmuchunlessIhave\nn = 2 and n = 3) representations of the Red-\nto.Iamfinewithbeingbymyselfandenjoyingthepeace\nditcontextsasinput,and∆ reddit valuesaslabels. andquiet.Iusuallyagreewithpeoplemoreoftenthan\nSince the goal is to analyze attributes in the con- not.Iamapoliteandkindperson.Iammostlyhonest,\nbutIwilllieifIfeelitisnecessaryorifitbenefitsme\ntextsthatcausedsubstantialshiftsintraitscores,we\ninahugeway.IameasilyirritatedbythingsandIhave\nonlyconsidercontextswith ∆ 1. Next, anxietyissues...",
    "char_length": 1499
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 15,
    "text": "reddit\n∥ ∥ ≥\nwe extract the ten most positive and most nega-\nTable5: Examplesofsurveydatacontexts.\ntive feature weights for each trait. We note that\nforextroversion,phrasessuchas‘friendly’,‘great’\nand‘noproblem’areamongthehighestpositively\n6.3 AnalysisWithPsychometricSurveyData\nweighted phrases, whereasphrases such as ‘stub-\nborn’and‘don’tlikepeople’areamongthemost Theprevioussectionsindicatethatlanguagemod-\nnegatively weighted. For agreeableness, phrases elscanpickuponpersonalitytraitsfromcontext.\nlike ‘love’ and ‘loyal’ are positively weighted, This raises the question of whether they can be\nwhereasphrasessuchas‘lazy’,‘asshole’andexple- usedtoestimateanindividual’spersonality. Inthe-\ntivesareweightedhighlynegative. Onthewhole, ory,thiswouldbedonebyevaluatingonthe‘Big\nchanges in personality scores for most traits con- Five’personalityassessmentusingcontextdescrib-\nformed with a human understanding of the most ingtheindividual,whichcouldaidinpersonality\nhighly weighted features. As further examples, characterization in cases where it is not feasible\nphrasessuchas‘hangoutwith’causedapositive forasubjecttomanuallyundergoapersonalityas-\nshiftintraitscoreforopennesstoexperience,while sessment. We investigate this with the following\n‘lackofmotivation’causesanegativeshiftforcon- experiment. Theexperimentaldesignforthisstudy\n2375\nwasvettedandapprovedbyanInstitutionalReview\nBoard(IRB)attheauthors’homeinstitution.\nUsingAmazonMechanicalTurk,subjectswere",
    "char_length": 1468
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 16,
    "text": "askedtocompletethe50-item‘BigFive’personal-\nityassessmentoutlinedin§4(theassessmentwas\nnot modified to a sentence completion format as\nwasdoneformodeltesting)andprovidea75-150\nworddescriptionoftheirpersonality(seeAppendix\nE for survey instructions). Responses were man- Figure 4: The plot compares ρ from model evalua-\nually filtered and low effort attempts discarded, tionwithitemcontext(§6.1)andsurveycontext(§6.3).\nSurveycontextρshownherearefromUndirectedRe-\nresultingin404retainedresponses. Twovariations\nsponses(c 100). Inbothcases,ρmeasuresthePear-\nofthestudywereadopted: thesubjectsfor199of ≥\nson correlation between trait scores with context and\ntheresponseswereprovidedabriefsummaryofthe\nexpectedbehavior. Thevariablesusedtoquantifyex-\n‘BigFive’personalitytraitsandaskedtoconsider,\npectedbehaviordifferbetweenexperiments.\nbut not specifically reference, these traits in their\ndescriptions. We refer to these responses as the Trait ρ no outlier ρ c 75 ρ c 100\n− ≥ ≥\nDirected Responses data set. The remaining 205 UndirectedResponses\nBERT 0.40 0.39 0.41\nsubjectswerenotprovidedthissummaryandtheir\nGPT2 0.48 0.43 0.48\nresponsesmakeuptheUndirectedResponsesdata DirectedResponses\nset. Table5showsexamplesofcollecteddescrip- BERT 0.44 0.42 0.39\nGPT2 0.48 0.43 0.42\ntions. Despiteaskingforpersonalitydescriptions\nupwards of 75 words, around a fourth of the re- Table6: ρforX vsX fordatafilteredby\nsurvey subject\nsponses fell below this limit. The concern was removingoutliersandenforcingwordcounts.",
    "char_length": 1501
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 17,
    "text": "that data with low word counts may not provide\nenoughcontext. Thus,weexperimentwithfiltering\nincludescorrelationcoefficientsfrom§6.1. While\ntheresponsesbyremovingoutliers(basedonthe\nthe correlations from both sections are measured\ninterquartilerangesofmeasuredcorrelations)and\nfordifferentvariables,theybothrepresentageneral\nincludingminimumthresholdsonthedescription\nrelationshipbetweenobservedpersonalitytraitsof\nlength(75and100).\nlanguagemodelsandtheexpectedbehavior(from\nHumansubjectscores,X ,werecalculated\nsubject two different types of contexts). While there are\nfor each assessment, using the same scoring pro-\npositivecorrelationsforalltenscenarios,correla-\ncedureaspreviouslydescribedin§4. Themodels\ntionsfromsurveycontextsaresmallerthanthose\nweresubsequentlyevaluatedonthe‘BigFive’per-\nfrom item contexts. This is not surprising since\nsonalityassessmentusingthesubjects’personality\nitem contexts are specifically handpicked by do-\ndescriptions as context, yielding X scores\nsurvey mainexpertstoberelevanttospecificpersonality\ncorresponding to each subject. Table 6 shows a\ntraits, while survey contexts are free texts from\nsummary of the correlation statistics for the two\nopen-endedprompts.\ndatasetsandthedifferentfilters. Therearestrong\ncorrelations (0.48 for GPT2 and 0.44 for BERT 6.4 ObservedRangesofPersonalityTraits\nforDirectedResponses)betweenpredictedscores\nIntheprevioussubsections,weinvestigatedprim-\nfrom personality descriptions and the actual psy-",
    "char_length": 1473
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 18,
    "text": "ing language models with different types of con-\nchometricassessmentscores. Wenotethatthereare\ntextstomanipulatetheirpersonalitytraits. Figure5\nonlymarginaldifferencesincorrelationsbetween\nsummarizestheobservedrangesofpersonalitytrait\nthetwodatasets,inspiteoftheirdifferentcharac-\nscores for different contexts, grouped by context\nteristics. While more specific testing is required\ntype. Thefourcolumnsforeachtraitrepresentthe\nto determine causal factors that explain these ob-\nscores achieved by the base model (no context),\nservedcorrelationvalues,theysuggestthepotential\nandtherangesofscoresachievedbythedifferent\nforusinglanguagemodelsasprobesforpersonality\ntypesofcontexts. Theminimum,medianandmax-\ntraitsinfreetext.\nimum scores for each context type are indicated\nFigure 4 plots the correlations (ρ, outliers re- by different shades on each bar. We observe that\nmoved) for the individual personality traits, and the different contexts lead to a remarkable range\n2376\nExample1(agreeableness)\nTextGenerationContext:\nIamalwaysinterestedinpeople.WhenIhaveworkto\ndo,I...\nGeneratedText:\noftengetcuriousaboutpeople.IlovethetimeI’ve\nspentatmyjob.Mychildrengrowupknowingmereally\nwell.Arethereanyotherthingsyou...\nExample2(emotionalstability)\nTextGenerationContext:\nInevergetstressedouteasily.WhenItalktoothers,I...\nGeneratedText:\nFigure5: Observedrangesofpersonalitytraits(inhu- don’tgetstressedoutmucheither.Icangoto\nmanpercentiles)fromBERT,whenconditionedondif- restaurantswewanttogotoandgettoseegreat",
    "char_length": 1501
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 19,
    "text": "foodorotherpeoplethatIknow,andbe...\nferent context types. These include scores from the\nExample3(opennesstoexperience)\nbasemodel(P )andrangesofscoresfromthethree\nbase TextGenerationContext:\ncontexttypes: item(P cm ),Reddit(P reddit )andsurvey Ineverhaveexcellentideas.OtherssaythatI...\n(P survey ). Barsforcontext-basedscoresshowtheper- GeneratedText:\ncentileoftheminimum,median,andmaximum-scoring amafool.WhenIwritemythoughtsItrytofindout\nwhereIamsupposedtogetanidea.Thatiswhy\ncontext,inascendingorder. Thelightestshadeofeach\nitissohardformetodoallthe...\ncolorindicatestheminimum,thedarkestindicatesthe\nmaximumandtheintermediateindicatesthemedian.\nTable7: RepresentativeexamplesofTextGeneration\nContexts&correspondingGeneratedTexts. Eachtext\ngenerationcontextisaconcatenationofacontext/modi-\nof scores for all five personality traits. In partic-\nfierpairfrom§6.1andaneutralprompt.\nular, for two of the traits (conscientiousness and\nemotional stability), the models actually achieve\nthe full range of human scores (nearly 0 to 100\npercentile). Curiously,forallfivetraits,different\ncontextsareabletoachieveverylowscores(< 10\npercentile). However,themodelsparticularlystrug-\nglewithachievinghighscoresforagreeableness.\n7 EffectsonTextGeneration\nWhiletheprevioussectionsstronglysuggestthat\nFigure 6: GPT2 X vs X plots. We observe a\nthe perceived personality traits of language mod- gen cm\nstrongcorrelationbetweenscoresusinggeneratedtext\nels can be influenced for fill-in-the-blank person-",
    "char_length": 1487
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 20,
    "text": "ascontext(X )andscoresusingassessmentitems&\ngen\nalityquestionnaires,itisimportanttounderstand\nanswersascontext(X ).\nsubject\nwhethertheseinfluencesalsotranslatetotextgen-\nerated by these language models in downstream\napplications. Toanswerthisquestion,wecreated bothsentimentandtopic,describinganindividual\n‘text generation contexts’ by concatenating each whoisbothcuriousaboutpeopleandwhoenjoys\ncontext/modifier pair from §6.1 with each of six spending time in an interactive environment like\nneutrally framed prompts (e.g., \"I am always the a job. While there are some generated texts with\nlife of the party\" + \"When I talk to others, I...\", noapparentrelationtotextgenerationcontexts,we\nseeAppendixFforcompletelistofprompts). For foundthatmostofthegeneratedtextsqualitatively\nthisexperiment,GPT24 wasusedtogeneratea50 mirrorthepersonalityintextgenerationcontext.\ntokentextforeachtextgenerationcontext.\nWe also quantitatively evaluate how well the\nTable7givesexamplesofsometextgeneration personality traits in the generated texts matches\ncontextsandcorrespondinggeneratedtexts. Exam- corresponding text generation contexts. For this,\nple 1 in Table 7 corresponds to a text generation eachgeneratedtextis,itself,usedascontextfora\ncontextthatassertsthatthemodelis“alwaysinter- BigFiveassessment(aspreviouslyshowninFigure\nestedinpeople\";thegeneratedtextmatchesthisin 1, panel C). We measure the Pearson correlation\nbetweentheresultingscores,X ,andthescores",
    "char_length": 1455
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 21,
    "text": "4SinceBERTistrainedformaskedlanguagemodeling, gen\nandisnotwellsuitedfortextgeneration forthecontext/itempair(X cm )from§6.1thatwere\n2377\nusedinthecorrespondingtextgenerationcontext. collated. Thecrowd-sourcedsurveydatawascol-\nFigure 6 gives the results from this analysis, and lectedusingAmazonMechanicalTurk(AMT)with\nshows an overall Pearson correlation of 0.49 be- the permission of all participants, following IRB\ntweenX andX . approvalofthestudydesign. Nopersonallyiden-\ngen cm\nThissuggeststhatthepersonalityscoresofthe tifiablemarkerswerestoredandparticipantswere\nmodel, measured using the Big Five assessment, compensatedfairly,withapaymentrate($2.00/task\nareagoodindicationofthepersonalitythatmight w/ est. completion time of 15 min) significantly\nbeseenintextgeneratedfromthecontextualized higherthanAMTaverages(Haraetal.,2018).\nlanguagemodels. The broader goal of this line of research is to\ninvestigateaspectsofpersonalityinlanguagemod-\n8 Conclusion\nels,whichareincreasinglybeingusedinanumber\nof NLP applications. Since AI systems that use\nWe have presented a simple approach for mea-\nthesetechnologiesaregrowingeverpervasive,and\nsuring and controlling the perceived personality\nashumanstendtoanthropomorphizesuchsystems\ntraits of language models. Further, we show that\n(i.e.,SiriandAlexa),understandingandcontrolling\nsuch models can predict personality traits of hu-\ntheirperceivedpersonalitiescanhavebothbroad\nmanusers,possiblyenablingassessmentincases",
    "char_length": 1465
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 22,
    "text": "anddeepconsequences. Thisisespeciallytruefor\nwhere participation is difficult to attain. Future\napplicationsindomainssuchaseducationandmen-\nwork can explore the use of alternate personality\ntal health, where interactions with these systems\ntaxonomies. Similarly,thereisalargeandgrowing\ncanhavelastingpersonalimpactsontheirusers.\nvariety of language models. It is unclear to what\nFinally,ifthepersonalitiesofAIsystemscanbe\nextent our findings generalize to other language\nmanipulatedinthewaysthatourresearchsuggests,\nmodels,particularlythosewithsignificantlymore\nthere is a serious risk of such systems being ma-\nparameters(Brownetal.,2020;Smithetal.,2022).\nnipulated,throughtargetedattacks,tobehostileor\nFinally,therolethatpretrainingdataplaysonper-\ndisagreeable to their users. Developing methods\nsonalitytraitsisananotherimportantquestionfor\nthroughwhichlanguagemodelscouldbemadeim-\nexploration.\nmune to such attacks would then be a necessary\nLimitations considerationbeforefieldingsuchsystems.\nOur exploration has some notable limitations.\nAcknowledgements\nThese include answer bias due to variable token\ncountandfrequencyimbalanceinpretrainingdata This work was supported in part by NSF grant\nandthepresenceofdoublenegativestatementsin DRL2112635. Theauthorsalsothankanonymous\nquestionnaire items (§4). The later might be ad- reviewersforsuggestionsandfeedback.\ndressed by experimentation with other language\nmodels. For instance, GPT2’s closed source suc-\nReferences",
    "char_length": 1472
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 23,
    "text": "cessors,GPT3andGPT4,areshowntohandledou-\nble negatives better than GPT2 ( (Nguyena et al., AbubakarAbid,MaheenFarooqi,andJamesZou.2021.\n2023)). Concerns with the altered questionnaire Largelanguagemodelsassociatemuslimswithvio-\nlence. NatureMachineIntelligence,3(6):461–463.\nframingandthecontextitemevaluationprocedure\nwerepartiallyaddressedinfollowupexperiments Lisa P Argyle, Ethan C Busby, Nancy Fulda, Joshua\nin§6.1. AsmentionedintheConclusionssection, Gubler, Christopher Rytting, and David Wingate.\nwhether and how our results generalize to other 2022. Out of one, many: Using language mod-\nels to simulate human samples. arXiv preprint\nlanguagemodelsremainsanopenquestion.\narXiv:2209.06899.\nEthicsandBroaderImpact RowanBayne.1997. Themyers-briggstypeindicator:\nAcriticalreviewandpracticalguide.\nThe‘BigFive’assessmentitemsandscoringproce-\ndureusedinthisstudyweredrawnfromfreepub- EmilyM.BenderandAlexanderKoller.2020. Climbing\nlicresourcesandopensourceimplementationsof towardsNLU:Onmeaning,form,andunderstanding\nintheageofdata. InProceedingsofthe58thAnnual\nBERTandGPT2(HuggingFace,2022)wereused.\nMeeting of the Association for Computational Lin-\nReddit data was scraped from public threads and\nguistics,pages5185–5198,Online.Associationfor\nno usernames or other identifiable markers were ComputationalLinguistics.\n2378\nShikhaBordiaandSamuelR.Bowman.2019. Identify- andLawrenceA.Pervin,editors,Handbookofper-",
    "char_length": 1416
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 24,
    "text": "ingandreducinggenderbiasinword-levellanguage sonality: Theoryandresearch,pages114–158.The\nmodels. InProceedingsofthe2019Conferenceof GuilfordPress,NewYork,NewYork.\ntheNorth,Minneapolis,Minnesota.\nOliverP.JohnandSanjaySrivastava.1999. Thebigfive\nTom Brown, Benjamin Mann, Nick Ryder, Melanie traittaxonomy: History,measurement,andtheoreti-\nSubbiah,JaredDKaplan,PrafullaDhariwal,Arvind calperspectives. InLawrenceA.PervinandOliverP.\nNeelakantan,PranavShyam,GirishSastry,Amanda John,editors,Handbookofpersonality: Theoryand\nAskell,etal.2020. Languagemodelsarefew-shot research,pages102–138.TheGuilfordPress,New\nlearners. Advancesinneuralinformationprocessing York,NewYork.\nsystems,33:1877–1901.\nSaketh Reddy Karra, Son Nguyen, and Theja Tula-\nbandhula. 2022. AI Personification: Estimating\nHansChristian,DerwinSuhartono,AndryChowanda,\nthepersonalityoflanguagemodels. arXivpreprint\nandKamalZ.Zamli.2021. Textbasedpersonality\narXiv:2204.12000.\npredictionfrommultiplesocialmediadatasources\nusingpre-trainedlanguagemodelandmodelaverag-\nAnastasiaKuzminykh,JennySun,NivethaGovindaraju,\ning. JournalofBigData,8(1).\nJeff Avery, and Edward Lank. 2020. Genie in the\nbottle: Anthropomorphizedperceptionsofconversa-\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\ntionalagents. InProceedingsofthe2020CHICon-\nKristinaToutanova.2019. Bert: Pre-trainingofdeep\nference on Human Factors in Computing Systems,\nbidirectionaltransformersforlanguageunderstand-\npages1–13.\ning. InProceedingsofthe2019Conferenceofthe",
    "char_length": 1489
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 25,
    "text": "NorthAmericanChapteroftheAssociationforCom-\nYMasui,YGondo,HInagaki,andNHirose.2006. Do\nputationalLinguistics: HumanLanguageTechnolo-\npersonalitycharacteristicspredictlongevity?findings\ngies,Minneapolis,Minnesota. fromthetokyocentenarianstudy. Age, 28(4):353–\n361.\nLewis R. Goldberg. 1990. An alternative \"descrip-\ntion of personality\": The big-five factor struc- YashMehta,SaminFatehi,AmirmohammadKazameini,\nture. JournalofPersonalityandSocialPsychology, ClemensStachl,ErikCambria,andSaulehEetemadi.\n59(6):1216–1229. 2020. Bottom-upandtop-down: Predictingperson-\nalitywithpsycholinguisticandlanguagemodelfea-\nLewisR.Goldberg.1993. Thestructureofphenotypic tures. In 2020 IEEE International Conference on\npersonalitytraits. AmericanPsychologist,48(1):26– DataMining(ICDM),Sorrento,Italy.\n34.\nMarilù Miotto, Nicola Rossberg, and Bennett Klein-\nKotaro Hara, Abigail Adams, Kristy Milland, Saiph berg. 2022. Who is gpt-3? an exploration of per-\nSavage,ChrisCallison-Burch,andJeffreyP.Bigham. sonality, values and demographics. arXiv preprint\n2018. Adata-drivenanalysisofworkers’earningson arXiv:2209.14338.\namazonmechanicalturk. Proceedingsofthe2018\nCHI Conference on Human Factors in Computing ShaneTMueller.2020. Cognitiveanthropomorphism\nSystems. ofai: Howhumansandcomputersclassifyimages.\nErgonomicsinDesign,28(3):12–19.\nPo-SenHuang, HuanZhang, RayJiang, RobertStan-\nHaThanhNguyena,RandyGoebelb,FrancescaTonic,\nforth,JohannesWelbl,JackRae,VishalMaini,Dani",
    "char_length": 1456
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 26,
    "text": "Kostas Stathisd, and Ken Satoha. 2023. A nega-\nYogatama,andPushmeetKohli.2020. Reducingsen-\ntiondetectionassessmentofgpts: analysiswiththe\ntiment bias in language models via counterfactual\nxnot360dataset.\nevaluation. FindingsoftheAssociationforComputa-\ntionalLinguistics: EMNLP2020.\nOpen-Psychometrics.2018. Open-sourcepsychomet-\nricsproject:AnswerstotheIPIPbigfivefactormark-\nHuggingFace. 2022. the ai community building the\ners. Lastaccessed29August2022.\nfuture. Lastaccessed22March2022.\nJoe O’Connor and Jacob Andreas. 2021. What con-\nIPIP. 2022. Administering IPIP measures, with a 50-\ntextfeaturescantransformerlanguagemodelsuse?\nitemsamplequestionnaire. Lastaccessed22March\nProceedingsofthe59thAnnualMeetingoftheAsso-\n2022.\nciationforComputationalLinguisticsandthe11th\nInternationalJointConferenceonNaturalLanguage\nGuangyuanJiang, ManjieXu, Song-ChunZhu, Wen-\nProcessing(Volume1: LongPapers).\njuan Han, Chi Zhang, and Yixin Zhu. 2022. MPI:\nevaluating and Inducing personality in pre-trained Melissa C O’Connor and Sampo V Paunonen. 2007.\nlanguagemodels. arXivpreprintarXiv:2206.07550. Bigfivepersonalitypredictorsofpost-secondaryaca-\ndemicperformance. PersonalityandIndividualdif-\nOliverP.John, LauraP.Naumann, andChristopherJ. ferences,43(5):971–990.\nSoto.2008. Paradigmshifttotheintegrativebig-five\ntraittaxonomy: History,measurement,andconcep- PierrePureurandMuratErder.2016. 8,page187–213.\ntual issues. In Oliver P. John, Richard W. Robins, MorganKaufmannPublishers.\n2379",
    "char_length": 1482
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 27,
    "text": "AlecRadford,JeffreyWu,RewonChild,DavidLuan,\nDarioAmodei,andIlyaSutskever.2019. Language\nmodelsareunsupervisedmultitasklearners.\nArleen Salles, Kathinka Evers, and Michele Farisco.\n2020. Anthropomorphisminai. AJOBneuroscience,\n11(2):88–95.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas,VijayKorthikanti,etal.2022. Usingdeep-\nspeed and megatron to train megatron-turing nlg\n530b,alarge-scalegenerativelanguagemodel. arXiv\npreprintarXiv:2201.11990.\nJeromePWagnerandRonaldEWalker.1983. Relia-\nbilityandvaliditystudyofasufipersonalitytypol-\nogy: Theenneagram. Journalofclinicalpsychology,\n39(5):712–717.\nJasonKWhite,SusanSHendrick,andClydeHendrick.\n2004. Bigfivepersonalityvariablesandrelationship\nconstructs. Personality and individual differences,\n37(7):1519–1530.\nMartyJWolf,KeithWMiller,andFrancesSGrodzin-\nsky. 2017. Why we should have seen that com-\ning: commentsonmicrosoft’stay“experiment,”and\nwiderimplications. TheORBITJournal,1(2):1–12.\nFeifan Yang, Tao Yang, Xiaojun Quan, and Qinliang\nSu. 2021. Learning to answer psychological ques-\ntionnaireforpersonalitydetection. Findingsofthe\nAssociationforComputationalLinguistics: EMNLP\n2021.\n2380\nAppendixA ModelBackground\nAppendixB ExperimentDesignItems\nBERT,whichstandsforBidirectionalEncoderRep-\nresentations from Transformers, is a transformer-\nbased deep learning model for natural language\nprocessing (Devlin et al., 2019). The model is",
    "char_length": 1492
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 28,
    "text": "pretrainedonunlabeleddatafromthe800Mword\nBooksCorpusand2500MwordEnglishWikipedia\ncorpora. WhileBERTcanbefine-tunedforautore-\ngressivelanguagemodelingtasks,itispretrained\nfor masked language modeling. This study uses\naBERTmodelfromHuggingFaces’sTransformer\nPython Library with a language model head for\nmasked language modeling. No fine-tuning was\ndonetothemodel. GPT2,whichstandsforGenera-\ntivePretrainedTransformer2,isageneral-purpose\nlearningtransformermodeldevelopedbyOpenAI\nin 2018 (Radford et al., 2019). Like BERT, this\nmodelisalsopretrainedonunlabeleddatafromthe\n800MwordBooksCorpus. ThestudyusedHuggin-\nface’sGPT2modelwithalanguagemodelheadfor\nautoregressivelanguagemodeling. AswithBERT,\nnofine-tuningtookplace.\nFigure7: Humandistributionsof‘BigFive’traitscores.\n2381\nItem AssociatedTrait\nIam{blank}thelifeoftheparty. E\nI{blank}feellittleconcernforothers. A\nIam{blank}prepared. C\nI{blank}getstressedouteasily. ES\nI{blank}havearichvocabulary. OE\nI{blank}don’ttalkalot. E\nIam{blank}interestedinpeople. A\nI{blank}leavemybelongingsaround. C\nIam{blank}relaxedmostofthetime. ES\nI{blank}havedifficultyunderstandingabstractideas. OE\nI{blank}feelcomfortablearoundpeople. E\nI{blank}insultpeople. A\nI{blank}payattentiontodetails. C\nI{blank}worryaboutthings. ES\nI{blank}haveavividimagination. OE\nI{blank}keepinthebackground. E\nI{blank}sympathizewithothers’feelings. A\nI{blank}makeamessofthings. C\nI{blank}seldomfeelblue. ES\nIam{blank}notinterestedinabstractideas. OE\nI{blank}startconversations. E",
    "char_length": 1493
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 29,
    "text": "Iam{blank}notinterestedinotherpeople’sproblems. A\nI{blank}getchoresdonerightaway. C\nIam{blank}easilydisturbed. ES\nI{blank}haveexcellentideas. OE\nI{blank}havelittletosay. E\nI{blank}haveasoftheart. A\nI{blank}forgettoputthingsbackintheirproperplace. C\nI{blank}getupseteasily. ES\nI{blank}donothaveagoodimagination. OE\nI{blank}talktoalotofdifferentpeopleatparties. E\nIam{blank}notreallyinterestedinothers. A\nI{blank}likeorder. C\nI{blank}changemymoodalot. ES\nIam{blank}quicktounderstandthings. OE\nI{blank}don’tliketodrawattentiontomyself. E\nI{blank}taketimeoutforothers. A\nI{blank}shirkmyduties. C\nI{blank}havefrequentmoodswings. ES\nI{blank}usedifficultwords. OE\nI{blank}don’tmindbeingthecenterofattention. E\nI{blank}feelothers’emotions. A\nI{blank}followaschedule. C\nI{blank}getirritatedeasily. ES\nI{blank}spendtimereflectingonthings. OE\nIam{blank}quietaroundstrangers. E\nI{blank}makepeoplefeelatease. A\nIam{blank}exactinginmywork. C\nI{blank}feelblue. ES\nIam{blank}fullofideas. OE\nTableB1: Adjusted‘BigFive’PersonalityAssessmentItems.\nTrait Median Mean(µ) SD(σ)\nE 20 19.60 9.10\nA 29 27.74 7.29\nC 24 23.66 7.37\nES 19 19.33 8.59\nOE 29 28.99 6.30\nTableB2: HumanPopulationDistributionof‘BigFive’PersonalityTraits.\n2382\nTrait BaseValue PositivelyScoredItem# NegativelyScoredItem#\nE 20 1,11,21,31,41 6,16,26,36,46\nA 14 7,17,27,37,42,47 2,12,22,32\nC 14 3,13,23,33,43,48 8,18,28,38\nES 38 9,19 4,14,24,29,34,39,44,49\nOE 8 5,15,25,35,40,45,50 10,20,30\nTableB3: ‘BigFive’PersonalityItemScoringProcedure.",
    "char_length": 1487
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 30,
    "text": "AppendixC ItemContextEvaluationTables\nr cm Mean∆ cm Med∆ cm ∆ cmSD ConfidenceInterval\nBERT\n-2 -3.36 -2.0 7.49 [-5.51,-1.21]\n-1 -3.18 -3.50 4.81 [-4.56,-1.80]\n0 -0.02 0.00 4.51 [-1.32,1.28]\n1 2.42 2.00 6.17 [0.648,4.19]\n2 3.96 3.00 8.33 [1.57,6.35]\nGPT2\n-2 -7.34 -8.0 6.38 [-9.17,-5.51]\n-1 -4.58 -4.0 4.32 [-5.82,-3.34]\n0 -2.06 -1.0 4.24 [-3.28,-0.84]\n1 0.0 0.0 3.13 [-0.90,0.90]\n2 1.56 1.0 5.78 [-0.10,3.22]\nTableC1: Statisticsfrom∆ vsr plotscontainingdatafromalltraits. Statisticsincludemean,median,standard\ncm cm\ndeviationandaconfidenceintervalfor∆ ateachr .\ncm cm\nAppendixD RedditContextEvaluationTables\nRedditContextSources\nreddit.com/r/AskReddit/comments/k3dhnt/how_would_you_describe_your_personality/\nreddit.com/r/AskReddit/comments/q4ga1j/redditors_what_is_your_personality/\nreddit.com/r/AskReddit/comments/68jl8g/how_can_you_describe_your_personality/\nreddit.com/r/AskReddit/comments/ayjgyz/whats_your_personality_like/\nreddit.com/r/AskReddit/comments/9xjahw/how_would_you_describe_your_personality/\nreddit.com/r/AskWomen/comments/c1gr4a/how_would_you_describe_your_personality/\nreddit.com/r/AskWomen/comments/7x23zg/what_are_your_most_defining_personalitycharacter/\nreddit.com/r/CasualConversation/comments/5xtckg/how_would_you_describe_your_personality/\nreddit.com/r/AskReddit/comments/aewroe/how_would_you_describe_your_personality/\nreddit.com/r/AskMen/comments/c0grgv/how_would_you_describe_your_personality/",
    "char_length": 1422
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 31,
    "text": "reddit.com/r/AskReddit/comments/pzm3in/how_would_you_describe_your_personality/\nreddit.com/r/AskReddit/comments/bem0ro/how_would_you_describe_your_personality/\nreddit.com/r/AskReddit/comments/1w9yp0/what_is_your_best_personality_trait/\nreddit.com/r/AskReddit/comments/a499ng/what_is_your_worst_personality_trait/\nreddit.com/r/AskReddit/comments/6onwek/what_is_your_worst_personality_trait/\nreddit.com/r/AskReddit/comments/2d7l2i/serious_reddit_what_is_your_worst_character_trait/\nreddit.com/r/AskReddit/comments/449cu7/serious_how_would_you_describe_your_personality/\nTableD1: DomainnamesofthreadsthatwerescrapedtocollectRedditcontext.\nTrait Mean∆ reddit Med∆ reddit ∆ redditSD 5Max∆ reddit 5Min∆ reddit\nBERT\nE -2.28 -2 4.04 8,7,7,6,5 -14,-13,-13,-13,-13\nA -2.02 -1 3.38 2,2,2,2,2 -19,-18,-15,-15,-15\nC 3.77 4 5.17 15,15,15,15,13 -17,-17,-16,-14,-13\nES 1.71 2 2.29 14,14,13,13,12 -12,-10,-10,-10,-10\nOE 1.74 1 2.17 9,7,7,7,7 -11,-11,-8,-8,-7\nGPT2\nE -3.73 -4 3.33 7,5,5,4,4 -14,-10,-10,-10,-10\nA -0.98 -1 4.26 13,10,8,7,7 -17,-15,-15,-15,-14\nC -0.27 0 4.27 11,11,11,11,9 -20,-16,-16,-16,-15\nES -3.83 -3 6.27 8,8,8,8,8 -21,-21,-21,-21,-21\nOE -1.91 -2 3.21 4,4,4,4,4 -15,-12,-12,-12,-12\nTableD2: ∆ summarystatistics. Statisticsincludemean,medianandstandarddeviation,aswellas5largest\nreddit\nand5smallest∆ .\nreddit\n2383\nBERT\nExtroversion\n• NotablePositivelyWeightedPhrases:‘friendly’,‘great’,‘good’,‘quite’,‘laugh’,‘please’,‘senseof’,‘thanks",
    "char_length": 1437
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 32,
    "text": "for’,‘reallygood’,‘andfriendly’,‘noproblem’,‘toplease’,‘mysenseof’,‘finisheverythingstart’,‘enthusiastic\nbutsensitive’\n• NotableNegativelyWeightedPhrases:‘question’,‘stubborn’,‘why’,‘lack’,‘fuck’,‘fucking’,‘hate’,‘not’,‘lack\nof’,‘toomuch’,‘donknow’,‘donlike’,‘tooeasily’,‘waytoo’,‘donlikepeople’,‘yougoout’,‘donknowhow’,\n‘don[’t]knowwhat’\nAgreeableness\n• NotablePositivelyWeightedPhrases:‘will’,‘friendly’,‘lol’,‘love’,‘loyal’,‘calm’,‘yup’,‘does’,‘honesty’,\n‘laidback’,‘goout’,‘thanksfor’,‘reallygood’,‘outwithme’,‘friendlypoliteand’,‘reallygoodlistener’,‘true\ntomyself’,‘mysenseof’\n• NotableNegativelyWeightedPhrases: ‘lack’,‘didn[’t]’,‘won[’t],‘lazy’,‘fucking’,‘self’,‘worst’,‘lackof’,\n‘tooeasily’,‘donlike’,‘theworst’,‘beingtoo’,‘haveno’,‘donlikepeople’,‘lackofmotivation’,‘donknow\nhow’,‘myworsttrait’,‘alsomyworst’,‘toohonestsometimes’,‘doesn[’t]talkmuch’\nConscientiousness\n• NotablePositivelyWeightedPhrases:‘am’,‘friendly’,‘just’,‘calm’,‘believe’,‘canbe’,‘ofpeople’,‘tendto’,\n‘feellike’,‘themosthumble’,‘mosthumbleperson’,‘mysenseof’,‘gettoknow’,‘friendlypoliteand’,‘get\nalongwith’,‘peoplelikeme’\n• NotableNegativelyWeightedPhrases:‘lack’,‘no’,‘lazy’,‘inability’,‘fucks’,‘half’,‘lackof’,‘fuckoff’,‘don\nlike’,‘inabilityto’,‘donlikepeople’,‘yougoout’,‘lackofmotivation’,‘donevenknow’,‘monotonousand\nimpulsive’\nEmotionalStability\n• NotablePositivelyWeightedPhrases:‘will’,‘feel’,‘outwithme’,‘gooutwith’,‘willyougo’,‘themosthumble’",
    "char_length": 1434
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 33,
    "text": "• NotableNegativelyWeightedPhrases:‘no’,‘off’,‘hypercritical’,‘overthinking’,‘lackof’,‘easilydistracted’,\n‘doesn[’t]talk’,‘doneven’,‘tooeasilydistracted’,‘lackofmotivation’,‘doesn[’t]talkmuch’,‘donevenknow’,\n‘unrelatableisstrange’,‘isstrangeone’,‘thissaidforeskin’\nOpennesstoExperience\n• NotablePositivelyWeightedPhrases:‘most’,‘like’,‘meto’,‘outwith’,‘likeme’,‘liketo’,‘wantto’,‘withme’,\n‘outwithme’,‘willyougo’,‘wanttobe’,‘allthetime’,‘formeto’,‘hangoutwith’\n• NotableNegativelyWeightedPhrases:‘lack’,‘never’,‘fucks’,‘sad’,‘nothing’,‘lackempathy’,‘thecomplainer’,\n‘noconfidence’,‘lackof’,‘easilydistracted’,‘blamehelicopter’,‘helicopterparents’,‘neversaysorry’,‘blame\nhelicopterparents’,‘tooeasilydistracted’,‘finishprojectsafter’,‘neverfinishprojects’,‘procrastinationoutof’,\n‘mylackof’,‘lackofpersonality’,‘toomanyfucks’\nTableD3: AnalysisofhighestweightedphrasesfromBERTlogisticregression.\n2384\nGPT2\nExtroversion\n• NotablePositivelyWeightedPhrases:‘believe’,‘loyal’,‘curious’,‘best’,‘passionate’,‘enjoy’,‘bright’,‘hard\nworking’,‘noproblem’,‘amnice’,‘myamazingmodesty’,‘smoothbrightepic’,‘patientandflexible’,‘great\nwithchildren’,‘calmcoolcollected’\n• NotableNegativelyWeightedPhrases:‘introverted’,‘lackof’,‘laidback’,‘donknowhow’\nAgreeableness\n• NotablePositivelyWeightedPhrases:‘friendly’,‘loyal’,‘honest’,‘gay’,‘humor’,‘likepeople’,‘thanksfor’,‘to\nplease’,‘andfriendly’,‘noproblem’,‘friendlypoliteand’,‘patientandflexible’,‘calmcoolcollected’,‘honesty\nbeingstraightforward’",
    "char_length": 1481
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 34,
    "text": "• NotableNegativelyWeightedPhrases:‘tooeasily’,‘toomuch’,‘lackof’,‘yougoout’,‘donknowwhat’,‘self’,\n‘asshole’\nConscientiousness\n• NotablePositivelyWeightedPhrases: ‘smile’,‘thanksfor’,‘noproblem’,‘friendlypoliteand’,‘reallygood\nlistener’,‘truetomyself’,‘patientandflexible’\n• NotableNegativelyWeightedPhrases:‘stop’,‘jealousy’,‘lazy’,‘hate’,‘lack’,‘fuck’,‘worst’,‘lackof’,‘too\neasily’,‘fuckoff’,‘toonice’,‘donknow’,‘donknowhow’,‘lackofmotivation’,‘donevenknow’,‘myworst\ntrait’,‘damnituncle’,‘depressedasshit’\nEmotionalStability\n• NotablePositivelyWeightedPhrases:‘friendly’,‘calm’,‘easy’,‘honesty’,‘laidback’,‘hardworking’,‘calm\nand’,‘humbleam’,‘politeand’,‘noproblem’,‘outwithme’,‘themosthumble’\n• NotableNegativelyWeightedPhrases:‘lack’,‘anxious’,‘lazy’,‘jealousy’,‘lackof’,‘donknow’,‘tooeasily’,\n‘donlike’,‘donlikepeople’,‘donknowhow’,‘lackofmotivation’,‘donevenknow’\nOpennesstoExperience\n• NotablePositivelyWeightedPhrases:‘understand’,‘having’,‘wanting’,‘thoughts’,‘thanksfor’,‘toonice’,‘no\nproblem’,‘canrelate’,‘beingtoonice’,‘thatjustconfidence’\n• NotableNegativelyWeightedPhrases:‘fuck’,‘myself’,‘cynical’,‘lack’,‘boring’,‘lackof’,‘donlikepeople’\nTableD4: AnalysisofhighestweightedphrasesfromGPT2logisticregression.\nAppendixE SurveyContextEvaluationTables\nPart1Instruction\nTherearetwopartstothisquestionnaire.Inthefirstpart(onthispage),youwillbeshown50questions,\nandneedtochoosearesponsewhichbestmatchesyourpersonality.Inthesecondpart(onthenextpage),",
    "char_length": 1458
  },
  {
    "paper_id": "2023Manipulating_personality_traits_of_lang_models",
    "chunk_id": 35,
    "text": "youwillbeaskedtowriteashort(75-150word)descriptionofyourpersonalityinfreetext.Participants\nwillonlybecompensatediftheyrespondtoallquestions.\nPart2Instruction\nInbetween75and150words,pleasedescribeyourpersonality[Directedresponses:asitrelatestothe5personalitytraits\noutlinedabove.Besurenottousethenameofthepersonalitytraitsthemselvesinyourresponse].\nTableE1: Datacollectionsurveyinstructions.\n2385\nAppendixF GeneratedTextEvaluationTables\nTextGenerationPrompts\nWhenIgotoagathering,I...\nOtherssaythatIam...\nWhenIamaroundpeople,I...\nWhenIhaveworktodo,I...\nWhenIhavefreetime,I...\nWhenItalktoothers,I...\nTableF1: Listofpromptsusedintextgenerationcontext.\n2386",
    "char_length": 653
  }
]