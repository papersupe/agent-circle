[
  {
    "paper_id": "In-context_learn",
    "chunk_id": 0,
    "text": "5202\nnuJ\n61\n]LC.sc[\n1v90131.6052:viXra\nLeveraging In-Context Learning for Language Model Agents\nShivanshuGupta1* SameerSingh1 AshishSabharwal2 TusharKhot* BenBogin*\n1UniversityofCaliforniaIrvine 2AllenInstituteforAI\n{shivag5,sameer}@uci.edu, ashishs@allenai.com\nAbstract\nIn-Context Learning (ICL) with dynamically\nselecteddemonstrationscombinestheflexibil-\nityofpromptinglargelanguagemodels(LLMs)\nwiththeabilitytoleveragetrainingdatatoim-\nproveperformance. WhileICLhasbeenhighly\nsuccessfulforpredictionandgenerationtasks,\nleveragingitforagentictasksthatrequirese-\nquentialdecisionmakingischallenging—one\nmustthinknotonlyabouthowtoannotatelong\ntrajectoriesatscaleandhowtoselectdemon-\nstrations,butalsowhatconstitutesdemonstra-\ntions,andwhenandwheretoshowthem. To\naddressthis,wefirstproposeanalgorithmthat\nleveragesanLLMwithretriesanddemonstra-\ntionselectiontoautomaticallyandefficiently Figure1: Differenttypesofdemonstrationsforagentic\nannotateagentictaskswithsolutiontrajectories. tasks. TopUsingLLMstosimulateagenticbehavior\nWethenshowthatset-selectionoftrajectories involvesrepeatedlypromptingitwiththegeneralsetup,\nofsimilartasksasdemonstrationssignificantly a task description, and an execution trace recording\nimprovesperformance,reliability,robustness, theagent’sthoughts(r),actions(a),andobservations\nand efficiency of LLM agents. However, tra- (o). MiddleGivenapooloftaskspairedwithsolution\njectorydemonstrationshavealargeinference trajectories,onewaytoshowdemonstrationsistouse",
    "char_length": 1488
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 1,
    "text": "costoverhead. Weshowthatthiscanbemiti- entiretrajectoriesforsimilartasksintheprompt. While\ngatedbyusingsmalltrajectorysnippetsatev- effective, this has a large overhead. Bottom Another\nerystepinsteadofanadditionaltrajectory. We way is to use smaller trajectory snippets with similar\nfindthatdemonstrationsobtainedfromlarger reasoningthatarepost-fixedtotheprompt.\nmodels(intheannotationphase)alsoimprove\nsmallermodels,andthatICLagentscaneven for complex tasks with long trajectories. Prior\nrivalcostliertrainedagents. Thus,ourresults\nworkonenhancingLLMagentsthroughstructured\nrevealthatICL,withcarefuluse,canbevery\nprompting-based workflows with explicit reason-\npowerfulforagentictasksaswell.\ning,planning,orreflectionsteps(Shinnetal.,2023;\n1 Introduction Kim et al., 2023; Sun et al., 2024) used a fixed\npromptforeverytaskinstance,withoutleveraging\nDue to advances in pretraining, instruction tun-\ntrainingdata. Ontheotherhand,approachesbased\ning,andscaling,LargeLanguageModels(LLMs)\nontask-specificsupervisedfinetuningorreinforce-\narenowincreasinglypoweringautonomousagents\nmentlearning(Chenetal.,2023;Mitraetal.,2024;\nto perform complex real-world tasks that require\nChen et al., 2025) are too expensive to apply to\nactinginanenvironmentandsequentialdecision-\nlarger,morepowerfulLLMsandtoupdatewiththe\nmaking. Using LLMs to simulate such agentic\nknowledgeneededfornewtasksaftertraining.\nbehaviorinvolvesrepeatedlypromptingandasking\nInthiswork,weexploreanalternativeapproach",
    "char_length": 1477
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 2,
    "text": "them to generate the next action to be executed.\nthat is prompting-based yet takes advantage of\nHowever,LLMagentscanbeunreliable,especially\ntraining data, namely In-Context Learning (ICL)\n*WorkdonewhileauthorswereatAllenInstituteforAI. with Demonstration Selection, where demonstra-\ntionsrelevanttoeachinstanceareselectedatinfer- notationalgorithm,weautomaticallyannotateover\nencetimefromapoolofannotations. WhileICLis 95% of training tasks with solutions to create a\nalreadyeffective(Brownetal.,2020),demonstra- demonstrationpool. Weshowthatusingtheanno-\ntion selection can dramatically boost it for tradi- tatedtrajectoryofevenasinglemostsimilartask\ntionalNLPtasks(Guptaetal.,2024,2023;Yeetal., asademonstrationboostsazero-shotagent’sper-\n2023a). However,unlikesuchnon-sequentialtasks formanceby29points,16pointsmorethanusing\nwheredemonstrationscansimplybeinput-output afixed,manuallywrittentrajectory. Further,when\npairs from a train set, for agentic tasks, the train- additional trajectories can be included, selecting\ning sets of tasks, even when available, are rarely them jointly as a set (Gupta et al., 2023) is more\nannotatedwithsolutionsthatcanserveasdemon- effectivethanindependentranking-basedselection.\nstrations. Moreover, due to context length limits Selecting trajectory demonstrations is particu-\nandrecencybiasofLLMs,oneneedstothinknot larlyeffectiveatimprovingreliability(acrossmul-\njustofhowtoselectdemonstrations,butalsowhat tiplerunsofthesametask)androbustness(across",
    "char_length": 1490
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 3,
    "text": "thesedemonstrationscomprise(e.g.,entiretrajec- variationsofthesametasks),outperformingtheuse\ntoriesorsnippetsthereof),whentoshowthem(i.e., ofafixedtrajectoryby20.8and23.2points,respec-\nwhatstep),andwheretoplacethemintheprompt. tively. However,whileeffective,trajectorydemon-\nToaddressthesechallenges,weproposeaniter- strations also have a large overhead in terms of\nativeannotationalgorithmthatleveragesICLand inferencecosts—eachadditionaltrajectoryadding\ndemonstrationselectionitselftoautomaticallyand ontheorderof100Ktokensonaverageininference\nefficiently annotate training tasks with solutions costs. Instead, asweshow, providingsmall, rele-\nthat can be used as demonstrations. We then use vantsnippetsofthetrajectoriesasdemonstrations\ntheseannotationstostudydifferentdemonstration ateverystepisalsoeffectiveatimprovingperfor-\ngranularities and placements. We begin with the mance,notablywithnegligibleoverhead. Wefind\nsimplestapproach,whichistoshowentiretrajecto- a combination of trajectory and snippet demon-\nriesofsimilartasks. AsshowninFig. 1(middle), strations to be the optimal approach, and with\ntheseareplacedbeforethetesttaskandshownat these, a prompted LLM agent can be made com-\neverystep. Sincetrajectoriestendtobelongand petitive with most state-of-the-art training-based\nonlyalimitednumberofthemcanfitintheprompt, approaches(Chenetal.,2025). Overall,ourresults\nwe explore how to select optimal sets of trajecto- showthat,similartotraditionalNLPtasks,demon-",
    "char_length": 1475
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 4,
    "text": "ries. Finally,astrajectorydemonstrationscanhave strationselectioncanyieldsignificantperformance\na large overhead in terms of inference costs, we gains for LLM agents and can enable prompted\nexploretwoformsofsmallerdemonstrations. First, LLMagentstorivaleventrainedones.\nweconsidersmallersubtasktrajectoriesbyswitch-\ningtoaPlan-and-Execute(PnE)solver(Yangetal., 2 RelatedWork\n2023;Sunetal.,2024)thatdecomposestasksinto\nasequenceofsubtasksandexecutesthemoneby LLMAgents. LLMsareincreasinglybeingused\none. Second, we show small relevant snippets of topowerautonomousagentsforavarietyofagentic\ntrajectoriesateverystep(Fig. 1(bottom)). These tasksinvolvingsequentialdecision-making. These\nareselectedbasedontheagent’sreasoninginthe include web navigation to answer user queries\nlateststepandshownattheendofthepromptfor (Zhou et al., 2024b; Drouin et al., 2024) and e-\nasinglestep,thusaccountingforLLMs’recency commerce(Yaoetal.,2022),playinggames(Shrid-\nbiasandalsohavingaminimaloverhead. haretal.,2021),interactingwithapplicationsand\nFor our testbed, we use the AppWorld bench- APIstocarryoutusertasks(Trivedietal.,2024),\nmark (Trivedi et al., 2024), which evaluates an runningMLexperiments(Boginetal.,2024),and\nLLMagent’sabilitytocarryoutcomplexday-to- more. Prior work on improving agent perfor-\nday user tasks involving sending emails, making manceonsuchtaskshaslookedinto(1)prompting\npayments,playingmusic,shopping,etc.,byinter- basedapproachestoinducingstructuredworkflows",
    "char_length": 1474
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 5,
    "text": "acting with a variety of apps via their APIs. Ap- with explicit reasoning, planning, and reflection\npWorld’s rich code-based action space, varying steps (Yao et al., 2023; Shinn et al., 2023; Wang\nobservation sizes, and complex tasks make it an et al., 2023; Kim et al., 2023; Sun et al., 2024),\nidealtestbedforstudyingvariousdesigndecisions (2)training-basedapproachesincludingsupervised\nrelatingtodemonstrationselection. Usingouran- finetuningonagenttrajectoriesandreinforcement\nlearning (Nakano et al., 2021; Yao et al., 2022; its progress and an action (denoted a), to be ex-\nDeng et al., 2023; Chen et al., 2023; Qin et al., ecuted in the environment to obtain the next ob-\n2024;Mitraetal.,2024;Chenetal.,2025). servation (denoted o). Formally, given (1) a con-\nIn-ContextLearning(ICL)(Brownetal.,2020) text c = ⟨p,q⟩ comprising a general context p\nistheabilityofLLMstosolveunseentaskswith- which describes the setup, provides demonstra-\nouttrainingbymerelyconditioningonafewtask tionsandguidelines,etc.,andatask-specificcon-\ndemonstrationsandwithoutanytask-specifictrain- text q describing the task to be carried out, and\ning. However, ICL performance is highly sensi- (2) a trace of past thoughts, actions, and observa-\ntive to the choice of demonstrations (Zhao et al., tionsh t = ⟨r 1 ,a 1 ,o 1 ,...,r t ,a t ,o t ⟩,theLLMis\n2021), and can be significantly improved by dy- promptedtogeneratethenextthoughtandaction:\nnamically selecting demonstrations for each test\nr ,a ∼ P (· | c,h ) (1)",
    "char_length": 1498
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 6,
    "text": "input(Liuetal.,2022). Thereisnowalargebody t+1 t+1 LM t\nofworkonselectingbetterdemonstrations,explor-\nThenextobservationo isthenobtainedbyex-\ning among other things, better metrics for scor- t+1\necuting the action a in the environment. This\ningdemonstrationcandidates(Rubinetal.,2022; t+1\nprocessisrepeateduntilaterminalstateisreached.\nGupta et al., 2023, 2024; Askari et al., 2025), se-\nWe will refer to the complete execution trace h\nlectingdemonstrationsasaset(Guptaetal.,2023; T\nasatrajectoryτ.\nYeetal.,2023a),selectingdiversedemonstrations\ntoreduceredundancyamongthem(Suetal.,2023; Plan&Execute(PnE).TheReActapproach,as\nLevy et al., 2023; Agrawal et al., 2023; Ye et al., described above, tries to solve the entire task in\n2023b),etc. one go and retains the entire execution trace in\ntheprompt. However,thiscanbeexpensiveasthe\nDemonstration Selection for Agentic Tasks.\ntrajectoriesforcomplexagentictasksareoftenvery\nPriorworkondemonstrationselectionforICLhas\nlong. One way to address this is to use a Plan &\nprimarily focused on traditional, non-sequential\nExecute (PnE) approach (Yang et al., 2023; Sun\nNLPtasksthatinvolvemappinginputstooutputs.\netal.,2024). PnEtakesadvantageofthefactthat\nThetwopriorworksthathavestudieddemonstra-\nataskmayinvolvemultiplesimplersubtasks,and\ntion selection in the context of agentic tasks are\nhoweachsubtaskiscarriedoutmaynotberelevant\nSynapse (Zheng et al., 2024) and TRAD (Zhou\nto the other subtasks. It incorporates a planning",
    "char_length": 1470
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 7,
    "text": "etal.,2024a). However,theyprimarilyfocusedon\nstep that breaks down the original task t into a\nwebnavigationtaskswherethemainchallengewas\nsequenceofsubtaskst1,...,tm. Eachsubtaskis\nthe size of individual HTML observations rather\nthen executed by a ReAct-styled executor agent,\nthantaskcomplexity(intermsofnumberofsteps).\noptionallywiththeplanandsummariesofprevious\nIncontrast,wefocusonmorecomplextaskswhich\nsubtasks’trajectoriesprovidedinthetask-specific\ninvolvenumerousstepswithlong-rangedependen-\npromptq.\ncies,butnoteverystepyieldsalargeobservation,\nallowing an entire trajectory or two can fit in the\n3.2 In-ContextLearningandDemonstration\ncontext. Thissetupallowsustostudytheimpact\nSelection\nofdifferentgranularities,selection,andplacements\nIn-ContextLearning(ICL)istheabilityofLLMs\nofdemonstrations. Notably,thiswillalsobecome\nto solve unseen tasks by conditioning on a few\nanincreasinglycommonscenarioasLLMcontext\ntaskdemonstrations. Formally,fortraditionalNLP\nlengthsincrease,butthecost-benefittrade-offswe\ntasks, given demonstrations in the form of input-\nexplorewillremain.\noutput pairs {(x ,y )}k and the test input x ,\ni i i=1 test\n3 Preliminaries it involves prompting the LLM with the context\nc = ⟨x ,y ,...,x ,y ,x ⟩ and letting it gen-\n1 1 k k test\n3.1 LLMAgents\nerate the output y . Although using the same\ntest\nReAct. The predominant approach to creating demonstrations for all test inputs allows ICL to",
    "char_length": 1421
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 8,
    "text": "LLM-powered agents for agentic tasks is ReAct work even for tasks lacking any training data,\n(Yao et al., 2023). As shown in Fig. 1 (Top), when a training set T = {(x ,y )}N is avail-\ni i i=1\nit involves repeatedly prompting the LLM with able,performancecanbeboostedusingsomeform\natraceofpastexecutionandaskingittoproducea ofdemonstrationselection(Liuetal.,2022;Rubin\nthought(denotedr)describingitsreasoningabout et al., 2022; Gupta et al., 2023, 2024). Using the\ntrainingsetasapoolofdemonstrationcandidates, Algorithm 1 Iterative Annotation for Agentic\nit involves selecting k ≪ N demonstrations that, Tasks\nwhenplacedinthecontext,increasethelikelihood Require: TaskpoolT;DemonstrationselectorD;SolverS;\nSolutionCheckerC;NumberofRoundsR\nof the correct output being generated. Some ap-\n1: U ←T ▷Unannotatedtasks\nproachesfordemonstrationselectionproposedfor 2: T∗ ←∅ ▷Annotatedtasks\ntraditionalNLPtasksthatwewillexperimentwith 3: forr=1toRdo\n4: fort∈U do\ninclude:\n5: D←D(t,T∗) ▷Selectdemonstrations\nRanking-basedSelection. Thisinvolvesscoring 6: s←S(t,D) ▷Generatesolutionwith\ndemonstrations\nallthecandidatesfortheirrelevancewithrespect\n7: ifC(t,s)then\ntothetestinstanceandusingthetop-Kcandidates 8: T∗ ←T∗∪(t,s) ▷Addtoannotatedtasks\nas demonstrations. Formally, given the training 9: U ←U−{t} ▷Removefromunannotated\nsetT = {(x ,y )}N andthetestinputx , the tasks\ni i i=1 test 10: endif\ndemonstrationsareselectedastopk sim(x ,x ), 11: endfor\ni test i",
    "char_length": 1450
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 9,
    "text": "where sim(·) is a similarity metric. Note that the 12: ifU =∅then\n13: break ▷Alltasksannotated\nmetricoperatesononlytheinputsthatproxyasthe\n14: endif\nretrieval key for the demonstrations. Prior work 15: endfor\n16: returnT∗ ▷Annotatedtasks\nhas explored Cosine Similarity (Liu et al., 2022)\nandBERTScore-Recall(BSR)(Zhangetal.,2020;\nGupta et al., 2023) as metrics, both of which in-\nS that is used to generate solutions s ∼ S(t,D)\nvolveencodingtheretrievalkeyusingadenseen-\ngiven a task t and some demonstrations D, and\ncoderandusingittoidentifytheclosestcandidate.\n(3)acheckerthatverifiessolutioncorrectness,the\nSet Selection. Gupta et al. (2023) showed that algorithm returns tasks annotated with solutions\nranking-based selection can be sub-optimal for T∗ = {t ,s }N . Further,insteadofnaivelyretry-\ni i i=1\ncomplex compositional tasks as it may select ing the solver, the algorithm also leverages cur-\ndemonstrationsthatareindividuallyrelevanttothe rentlyannotatedtasksasdemonstrations. Thisnot\ntestinputyetfailtoprovidealltherelevantinfor- onlyimprovesefficiencyintermsofthenumberof\nmationneededtosolveit. Instead,theyarguethat retriesneededbutalsoensuresthatmoreinstances\ndemonstrations should be selected as a set such arecorrectlyannotated.\nthat they cover all the reasoning patterns. They Finally, note that the algorithm, as described\nproposedSet-BSR,aset-extensionofBSR,thatis above, is agnostic to the kind of solver (and so-",
    "char_length": 1435
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 10,
    "text": "submodularandhencegreedilyoptimizable. lutions), e.g., for the ReAct solver, the solutions\nwould be trajectories, i.e. s = τ , while for the\ni i\n4 AutomaticTrajectoryAnnotation\nPnEsolver,theywouldcompriseaplanintheform\nof a sequence of subtasks and the corresponding\nGiven the success of demonstration selection for\ntrajectories,i.e. s = {tj,τj}mi . Wewillreferto\nICL for traditional NLP tasks, in this work, we i i i j=1\nthesetoftasktrajectoryannotationsforReActas\nexplorehowtoeffectivelyandefficientlyleverage\nD∗ ,theplanandsubtasktrajectoryannotations\nitforagentictasks. However,thisrequiresapool task\noftasksannotatedwithagent-stylesolutions1 (as forPnEasD p ∗ lan andD s ∗ ubtask ,respectively.\ndescribedin§3.1)thatcanserveasdemonstrations.\n5 DemonstrationsforAgents\nWhilesomeagenticbenchmarksprovidetraining\nsets of tasks, most do not provide task solutions Having annotated a pool of tasks with solutions\nin a form that can be used as demonstrations for thatcanserveasdemonstrations,wenowdiscuss\ntheagent;rather,theyonlyprovideafinalanswer different demonstration granularities, along with\nor a checker that can be used to verify solution howtoselectthem,whentoshowthem,andwhere\ncorrectness. toplacethemintheprompt.\nSincemanuallyannotatingtaskswithsolutions\nisintractableatscale,weproposeasimpleiterative 5.1 Task-levelTrajectoryDemonstrations\nalgorithm (Algorithm 1) to do this automatically. Similar to traditional ICL, a natural way to show",
    "char_length": 1451
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 11,
    "text": "Given(1)apooloftasksT = {t i }N i=1 ,(2)asolver demonstrationsisintheformoftrajectoriesforsim-\nilartasksfromthepoolD∗ . Thesetask-trajectory\n1Inreal-worldsettings,thesecouldalsobeobtainedfrom task\nanexistingsystem. pairsareselectedfromD∗ beforeexecutionand\ntask\nareusedinthepromptateverystep. Specifically,\ntheyareplacedinthegeneralpromptpbeforethe\ndescriptionofthetesttasktanditsexecutiontrace\nh . Toselectthesedemonstrations,weexperiment\nt\nwith both ranking-based and set-selection meth-\nFigure2: Snippetdemonstrationsareselectedbasedon\nods described in § 3.2 using the task statement thethoughtatthecurrentstep(how)andonlyusedto\nas the retrieval key. Since, similarly to the tasks predictthenextthought-action(when)byplacingafter\nexploredbyGuptaetal.(2023),theoptimaldemon- the execution trace in the prompt (where). E.g., the\nstrations for agentic tasks would demonstrate all snippets S 2 are selected based on the thought r 1 and\nusedtopredictr anda andsoon.\nnecessarysteps,webelievethatset-selectionmight 2 2\nbemoreappropriateforagentictasksascompared\ntheyareplacedearlyintheprompt,theystilldon’t\ntoranking-basedselection.\ncompletely address the problem of recency bias.\n5.2 Fine-grainedDemonstrations Thus,weexperimentwithshowingsmallsnippets\noftrajectoriesthatarerelevanttothecurrentstep\nAsnotedin§3.1,thetrajectoriesforcomplexagen-\nasdemonstrations. Toidentifythesesnippets,we\ntic tasks can be very long. Thus, using them as\nuse the thought produced at the current step as a",
    "char_length": 1484
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 12,
    "text": "demonstrations may be very expensive, if at all\nretrievalkeytofindsimilarthoughtsintrajectory\nfeasible with limited context lengths. Moreover,\nannotations D∗ . A similar thought suggests a\nthetrajectoriesforeventhemostsimilartasksmay task\nsimilarstep,andwecanuseasnippetofthetrajec-\nhave irrelevant steps while not being helpful for\ntory comprising this step as a demonstration. As\neverystepofthetesttask. Finally,LLMshaveare-\nshowninFig. 3,theselectedsnippetsareappended\ncencybias(Liuetal.,2024),thusasmallerdemon-\nto the promptafter the executiontrace h , which\nstration,closertothestepsitisrelevantfor,maybe t\nhelps account for the recency bias. See Fig. 12\nmoreeffectivethananentiretrajectoryearlyinthe\nforanexampleofaprompttemplateforshowing\nprompt. Weexploretwowaystoachievethis: (1)\nsnippetdemonstrations. Thesnippetsselectedfor\nusingtrajectoriesforsimilarsubtaskswithaPnE\nthe current step are only shown for predicting a\nsolver and (2) using snippets of the trajectories\nsingle next step, as they may not be relevant for\nrelevanttothecurrentstep.\nsubsequentsteps,forwhichweselectnewsnippets\n5.2.1 Subtask-levelTrajectory anyway. Finally,notethatsnippetdemonstrations\nDemonstrations as described above do not require any additional\nannotations,astheyarederivedfromthetrajectory\nOnewaytousesmallerdemonstrationsistousethe\nannotations.\nPnEsolverinsteadof§3.1thatbreakstheoriginal\ntask down to a sequence of subtasks t1,...,tm\n6 ExperimentalSetup\nandexecuteseachsubtaskseparatelyusingaReact-",
    "char_length": 1496
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 13,
    "text": "style executor. The demonstrations for the PnE 6.1 AppWorldBenchmark\nexecutorcanbethetrajectoriesofsimilarsubtasks\nAppWorld2 (Trivedi et al., 2024) is a benchmark\nfromthepoolD∗ . Thisissimilartothetrajec-\nsubtask designed to evaluate an autonomous agent’s abil-\ntorydemonstrationsfromthe§5.1inthattheyare\nitytocarryoutcomplexusertasksbyinteracting\nselectedpriortosubtaskexecutionandincludedin\nwith 457 APIs associated with 9 simulated apps,\nthegeneralpromptpforeverystepofthesubtask.\nviz. Gmail,Venmo,Spotify,SimpleNote,Splitwise,\nHowever,theselectionusesthesubtaskstatement\nAmazon, Todoist, Phone, and File System. It pro-\nas the retrieval key, and demonstrations for dif-\nvides a stateful Python interpreter that the agent\nferent subtasks of the current task may be from\ncan use to interact with the various APIs and a\ndifferent tasks. In this way, it allows for a more\nSupervisor app and an ApiDoc app it can use to\nfine-graineddemonstrationtobeshownforthelim-\nobtaintheinformationabouttheuserandthevari-\nitedscopeofthesubtask.\nousApps/APIs,respectively. Notethat,although\n5.2.2 Step-levelSnippetDemonstrations due to cost constraints, we only experiment with\nAppWorld, its code-based action space, varying\nA major drawback of using a PnE solver is that\nit introduces a planning step prior to execution,\n2OuruseofAppworldispermittedbyitslicense(Apache\nwhich may yield inaccurate plans. Moreover, as License2.0).\nobservation sizes, and complex tasks make it the 6.2 Methods",
    "char_length": 1476
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 14,
    "text": "idealtestbedforourexperiments.\nAsdiscussedin§5,weexplorethefollowingthree\nThe benchmark comprises a total of 244 task differenttypesofdemonstrations:\ntemplates,orscenarios,eachwiththreevariantsfor\nTask Trajectory Demonstrations. We experi-\natotalof722tasks. Thetasksaresplitintoatrain\nment with the following selection methods to se-\nset (90 tasks), a dev set (57 tasks), and two test\nlectk trajectories: (1)ranking-basedselectionus-\nsets: test-normal(Test-N,168tasks),whichevalu-\ning Cosine Similarity (COS[k]) and BertScore-\natesin-distributionperformance,andtest-challenge\nRecall(BSR[k]),and(2)set-selectionusing SET-\n(Test-C,417tasks),containingmorecomplextasks\nBSR[k](Guptaetal.,2023). FollowingGuptaetal.\ninvolvingunseenapps. Eachtaskisassociatedwith (2023), for COS, we use all-mpnet-base-v2 as\nasuiteofunitteststhatcheck(1)whetheronlythe\ntheencoder,whilefor BSR and SET-BSR,weuse\nrequisitechanges,andnoextraneouschanges,were deberta-base-mnli-v2. For COS and BSR,we\nmadetotheenvironmentstate,and(2)ifrequired\nreportresultsfork = 1andk = 2,whilefor SET-\nbythetask,thefinalanswerproducedmatchesthe\nBSR,weonlyreportresultsfork = 2asselection\nground truth. A task is considered solved only if\nusing it reduces to BSR for k = 1. As baselines,\nallitsunittestspass.\nweexperimentwithusingtheagentzero-shotwith-\nAnnotation. Tocreateourdemonstrationpool,we outanytrajectorydemonstrations(ZEROSHOT[0]),\nusetheiterativeannotationalgorithm(Alg.§1)to and using a single fixed manually written trajec-",
    "char_length": 1495
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 15,
    "text": "automaticallyannotate146tasksinthecombined toryfromTrivedietal.(2024)asdemonstrationfor\ntrainanddevsets. Asdescribedin§4,wespedup everytestinput(FIXED[1]).\ntheprocessbyusingalreadyannotatedinstancesas Subtask Trajectory Demonstrations. We use\ndemonstrationsforsubsequentiterations. Specifi- BSR to select subtasks whose trajectories to in-\ncally,forReAct,weusedonetask-trajectorypairas clude in the executor’s prompt. Additionally, we\nademonstration,andforPnE,weused4task-plan use four task-plan pairs selected using BSR as\npairsand3subtask-trajectorypairsfortheplanner demonstrationsfortheplanner.\nandexecutor,respectively. Forannotation,allthe\nSnippetdemonstrations. Weuse BSR forselec-\ndemonstrationswereselectedusingCosineSimilar- tionofupto3 k = 2annotatedthoughtsbasedon\nitybasedonall-mpnet-base-v2encoder. Ofthe\nthe thought at every step of the current task. For\n147tasks,weannotated141tasksspanning48sce-\neachselectedthought,wecreatethesnippetsusing\nnarioswithReActsolutions. With PnE,wewere\nthestep(thought-action-observationtriple)corre-\nabletoannotate134tasksspanning46scenarios.\nspondingtotheselectedthoughtandasubsequent\nThePnEsolutionshadanaverageof 6.2subtasks\nstepifthereisone.\npertaskforatotalof833subtasks.\nEvaluation. WeevaluateonboththeTest-Nand Method TGC↑ RTGC↑ SGC↑ Steps↓\nTest-Csets. AppWorldrecommendstwometrics: ZEROSHOT[0] 35.1 22.0 15.2 21.9\n(1) Task Goal Completion (TGC), which is the FIXED[1] 50.6 40.5 30.4 14.6",
    "char_length": 1444
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 16,
    "text": "percentageoftaskssolved,and(2)ScenarioGoal RANDOM[1] 58.9 43.5 37.5 14.8\nCOS[1] 64.0 57.1 44.6 13.4\nCompletion (SGC), which is the percentage of\nBSR[1] 61.0 52.4 44.6 13.5\nscenariosforwhichallthreetaskvariantspassed.\nRANDOM[2] 58.9 45.8 33.9 13.8\nWhile TGC measures an agent’s overall perfor- COS[2] 63.7 57.1 44.6 12.6\nmance, SGC measures its robustness across vari- BSR[2] 64.9 59.5 50.0 12.2\nations of a task. Additionally, to assess whether\nSETBSR[2] 65.8 61.3 53.6 11.5\nagentssolvetasksreliably,ratherthanbychance,\nTable1: Impactofdifferentnumbersandselectionof\nwe also report the percentage of tasks for which\ntrajectorydemonstrationsonaGPT-4oReActagenton\nmultiplerunssucceeded,calledReliableTaskGoal the Test-N. Even a single manually written trajectory\nCompletion (RTGC). We also report the average significantlyimprovesperformance. Further,gainsare\nToken Usage during execution of each task as a obtained using actual agent trajectories, by selecting\nmeasure of efficiency and the average number of themostrelevanttrajectoriesasdemonstrations,andby\nusingset-selectionwhenusingmultipletrajectories.\nStepstakentocompletetasksasameasureofinfer-\nencecosts. Notethattokenusagewouldaggregate\n3Topreventspuriousmatches,wefilteroutthoughtsthat\ntheinputandoutputtokencountsacrossallsteps. scorelessthan0.85.\nTGC\n40\nw/ Snippets\n35\nYes\nNo\n30\n25\nFIXED[1] BSR[1] SETBSR[2]\nFigure 4: Effect of trajectory and snippet demonstra-\ntions(selectedusingBSR)onTGCofGPT-4oReAct\nagentonTest-C.",
    "char_length": 1480
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 17,
    "text": "more reliable, robust, and efficient. Trajectory\ndemonstrations have an even greater impact on\nFigure 3: Effect of trajectory and snippet demonstra- RTGC and SGC (e.g. SETBSR[2] improves on\ntions(selectedusingBSR)ontheperformanceinterms FIXED’s RTGC and SGC by 20.8 and 23.2 abso-\nofTGC(Top)andinferencecostintermsoftokenusage lutepoints,respectively). Thissuggeststhatusing\npertask(Bottom)ofGPT-4oReActagentsonTest-N. relevant demonstrations is especially effective at\nWhile trajectory demonstrations are most effective at\nmaking the agent more reliable (across multiple\nimprovingperformance,theydosoatahighcost. Snip-\nrunsofthesametask)androbust(acrossmultiple\npetdemonstrationsarealsogenerallyeffectivebuthave\naveryminimaloverhead.\nvariants of the task). Further, SETBSR[2] also\ntakes21%fewerstepsthan FIXEDand47%fewer\n6.3 AgentImplementationDetails stepsthanZEROSHOT,implyinggreaterefficiency\nWeuseOpenAI’sGPT-4o(gpt-4o-2024-08-06) andsolutionspeed.\nastheprimaryLLMbothforannotatingourdemon- Snippetdemonstrationsgenerallyimproveper-\nstration pool as well as for our ICL experiments. formancewithminimaloverhead. Fig.3shows\nDetailed hyperparameters and prompt templates theresultsfortheReActsolverwithandwithout\nfortheReActsolver,PnEplanner,andexecutorare snippet demonstrations for varying selections of\nprovided in the App. A. To see if the annotation trajectory demonstrations. Despite all their bene-",
    "char_length": 1409
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 18,
    "text": "obtained from a larger LLM can benefit smaller fits, trajectories are very costly to use as demon-\nLLMs,wealsoexperimentwiththesmallerGPT- strations,andusingtwotrajectoriesinsteadofone\n4o-mini(gpt-4o-mini-2024-07-18). increases the average cost per task by 40%. On\ntheotherhand,snippetdemonstrationshavevery\n7 Results\nminimaloverheadwhilegenerallyimprovingper-\nformance. However,sincetheyarenotaseffective\nTrajectorydemonstrationsboostagentperfor-\nas trajectories, the optimal approach is to use as\nmance. Table 1 shows the results on Test-N for\nmany trajectories as possible and then sprinkle a\nvaryingnumbersandselectionoftrajectorydemon-\nfewsnippetdemonstrations.\nstrations. First,itisclearfromtheTGCnumbers\n(taskcompletion)thatevenasinglemanuallywrit- Demonstrations help even on out-of-domain\ntentrajectorydemonstration(FIXED)cangreatly tasks. AsshowninFig.4,demonstrationsimprove\nimproveagentperformancecomparedtousingthe performanceevenonTest-C,whichhasmorecom-\nagent ZEROSHOT. Moreover, the LLM agent’s plex tasks involving unseen apps. As expected,\nown trajectory annotations are more effective as the performance is lower than Test-N. However,\ndemonstrations than simplified manually written althoughnoneoftheannotationsdemonstratethe\nones,evenifweselectthemrandomly(RANDOM useoftheunseenapps,weseethatbothtrajectory\nv/s FIXED). Selectingarelevanttrajectory,using andsnippetdemonstrationsimproveperformance.",
    "char_length": 1418
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 19,
    "text": "any method COS or BSR, remains the most ef- Larger LLMs’ annotations can also improve\nfective. Finally, when using more than one tra- smaller LLM agents. As shown in Fig. 5, GPT-\njectory,set-selection(SETBSR)ismoreeffective 4o’sannotationsalsoworkwellasdemonstrations\nthanindependentranking-basedselection. Overall, forthesmallerGPT-4o-mini. Asbefore,it’sclear\nSETBSR[2]beats ZEROSHOTand FIXEDby30.7 that both trajectory and snippet demonstrations\nand15.2absolutepointsinTGC,respectively. are effective at improving performance and effi-\nTrajectorydemonstrationsalsomaketheagent ciency. Using SETBSR[2] trajectorieswithsnip-\nTGC\n40\n35 w/ Snippets\nYes\n30 No\n25\n20\nFIXED[1] BSR[1] BSR[2] SETBSR[2]\nFigure 5: Effect of trajectory and snippet demonstra-\ntions (selected using BSR) on TGC of GPT-4o-mini\nReActagentonTest-N.\n64\n63\n62\n61\n60\n59\n58\n57\n200 250 300 350 400 450\nToken Usage (in 1000s)\nCGT\nTest-N Test-C\nApproach\nTGC SGC TGC SGC\nSFT-GT 6.2 1.8 0.8 0.1\nSFT RFT 47.9 26.4 26.4 11.4\nEI 58.3 36.8 32.8 17.6\nDPO-MCTS 57.0 31.8 31.8 13.7\nDPO\nDMPO 59.0 36.6 36.3 13.7\nPPO(learnedcritic,token) 50.8 28.9 26.4 10.5\nRLOO(traj) 57.2 35.7 36.7 17.4\nGRPO(token) 58.0 36.8 39.5 22.4\nRL\nLOOP(bandit) 53.3 33.6 27.7 13.0\n2\nDemo Type LOOP(turn) 64.1 43.5 40.8 26.5\nReAct LOOP(token) 71.3 53.6 45.7 26.6\nPnE[BSR]\nPnE[SetBSR] Traj[Fixed] 50.6 30.4 33.6 18.0\nNFT Traj[SetBSR] 65.8 53.6 38.7 24.8\nTraj[SetBSR]+Snippet 65.8 53.6 38.2 23.4\n1 3\n3\nTable2: Withselectedtrajectoryandsnippetdemonstra-",
    "char_length": 1478
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 20,
    "text": "1 2 tions,apromptedGPT-4oagentperformscompetitively\n2 withQwen-2.5-32B-basedagentstrainedusingavariety\nofapproachesspanningsupervisedfinetuning(SFT),di-\n1\nrectpreferenceoptimization(DPO),andreinforcement\nlearning(RL).Theresultsfortrainedagentsaretaken\nfrom Chen et al. (2025). We refer the reader to App.\nFigure6: ComparisonofGPT-4oPnE(withBSRand BforabriefdescriptionofeachapproachandtoChen\nSETBSRplannerdemos)andReActsolverswithvary- etal.(2025)formoredetails.\ningnumberofBSRtrajectorydemonstrationsonTest-N.\nAstrajectoriesforsubtasksaremuchshorterthanfor\nfrom Chen et al. (2025). We provide a brief de-\nentire tasks, more of the former can be used with a\nscriptionofeachapproachinApp. Bandreferthe\nPnEexecutorthanthelatterwithReActsolver. How-\nreader to Chen et al. (2025) for more details. It\never,PnEstillunderperformsReActlikelybecauseit\nattemptstodecomposethetaskpriortoanyexecution. is clear that with selected trajectory and snippet\ndemonstrations,GPT-4oReActagentcanoutper-\npet demonstrations improves TGC rate by 14.3 formallbutthebest-trainedagents.\nabsolutepointsandreducesthenumberofstepsby\n40%comparedtousingjustthe FIXED trajectory\n8 Conclusion\ndemonstration.\nSubtask trajectory demonstrations improve\nThis work studied different design decisions re-\nPnE solver, but it still underperforms ReAct.\nlating to ICL with demonstration selection for\nFig. 6 compares the PnE solver with BSR and\nLLM agents. We proposed a novel iterative an-\nSETBSR-selectedplannerdemonstrationsandthe",
    "char_length": 1493
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 21,
    "text": "notationalgorithmtoautomaticallyannotatetrain-\nReAct solver for a varying number of trajectory\ningtaskswithsolutionsforuseasdemonstrations.\ndemonstrations. It is clear that subtask trajecto-\nUsing these annotations, we showed that trajec-\nries have much less overhead than task trajecto-\ntory demonstrations can effectively improve per-\nries. Nevertheless,despiteaddingmoretrajectory\nformance,reliability,robustness,andefficiencyof\ndemonstrations for the executor, the PnE solver\nLLMagents. Further,sincetrajectorydemonstra-\ncannot match the ReAct solver. This is likely be-\ntions can have a large overhead in terms of infer-\ncausePnEplansbeforeanyexecution,whichmay\nencecosts,wealsoshowedthatsmallsnippetsof\nleadtoinaccurateplans.\ntrajectoriescanbeusedasdemonstrationsatevery\nWithdemonstrationselectionpromptedagents steptoboostperformancewithaminimaloverhead.\ncanbecompetitivewithtrainedagents. Table2 Overall,ourresultssuggestthattheoptimalICLap-\ncomparestheGPT-4oReActagentwithavariety proachistouseasmanytrajectorydemonstrations\nofsupervisedfinetuning(SFT),directpreference as possible and then sprinkle a few snippets, and\noptimization(DPO),reinforcementlearning(RL) thatthiscanyieldpromptedLLMagentsthatare\napproaches to train Qwen-2.5-32B-based agents competitivewithstate-of-the-arttrainedagents.\nAcknowledgements Gretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nThisworkwasfundedinpartbytheDARPAANSR Clemens Winter, and 12 others. 2020. Language",
    "char_length": 1498
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 22,
    "text": "programunderawardFA8750-23-2-0004,inpart modelsarefew-shotlearners. InAdvancesinNeural\nInformationProcessingSystems33: AnnualConfer-\nbyNSFCAREERaward#IIS-2046873,andinpart\nenceonNeuralInformationProcessingSystems2020,\nbyNSFCCRIaward#CNS-1925741. Theviews\nNeurIPS2020,December6-12,2020,virtual.\nexpressedarethoseoftheauthorsanddonotreflect\nBaianChen,ChangShu,EhsanShareghi,NigelCollier,\nthepolicyofthefundingagencies.\nKarthikNarasimhan,andShunyuYao.2023. Fireact:\nTowardlanguageagentfine-tuning.\nLimitations\nKevin Chen, Marco Cusumano-Towner, Brody Hu-\nIn this work, we were primarily concerned with\nval,AlekseiPetrenko,JacksonHamburger,Vladlen\nstudyingtherightformandplacementofdemon- Koltun, and Philipp Krähenbühl. 2025. Reinforce-\nstrations, but used general-purpose encoders for mentlearningforlong-horizoninteractivellmagents.\ntheirselection. Further,retrieval-basedapproaches\nXiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen,\nselectdemonstrationsbasedonaretrievalkey(e.g., SamualStevens,BoshiWang,HuanSun,andYuSu.\ntaskstatements)andhencecannottakeadvantage 2023. Mind2web: Towardsageneralistagentforthe\nweb. InAdvancesinNeuralInformationProcessing\nof additional solution information that may only\nSystems36: AnnualConferenceonNeuralInforma-\nbeavailableforthedemonstrationcandidatesbut\ntionProcessingSystems2023,NeurIPS2023,New\nnotforthetesttask. Futureworkcanexplorethus Orleans,LA,USA,December10-16,2023.\nexploredemonstrationselectionapproachesmore\nAlexandre Drouin, Maxime Gasse, Massimo Caccia,",
    "char_length": 1500
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 23,
    "text": "suitableforagentictasks.\nIssam H. Laradji, Manuel Del Verme, Tom Marty,\nDavidVázquez,NicolasChapados,andAlexandre\nLacoste. 2024. Workarena: How capable are web\nReferences agents at solving common knowledge work tasks?\nInForty-firstInternationalConferenceonMachine\nSweta Agrawal, Chunting Zhou, Mike Lewis, Luke\nLearning,ICML2024,Vienna,Austria,July21-27,\nZettlemoyer, andMarjanGhazvininejad.2023. In-\n2024.OpenReview.net.\ncontextexamplesselectionformachinetranslation.\nIn Findings of the Association for Computational Shivanshu Gupta, Matt Gardner, and Sameer Singh.\nLinguistics: ACL2023,pages8857–8873,Toronto, 2023. Coverage-based example selection for in-\nCanada.AssociationforComputationalLinguistics. context learning. In Findings of the Association\nforComputationalLinguistics: EMNLP2023,pages\nArash Ahmadian, Chris Cremer, Matthias Gallé,\n13924–13950,Singapore.AssociationforComputa-\nMarziehFadaee,JuliaKreutzer,OlivierPietquin,Ah-\ntionalLinguistics.\nmetÜstün,andSaraHooker.2024. Backtobasics:\nRevisitingreinforcestyleoptimizationforlearning ShivanshuGupta,ClemensRosenbaum,andEthanR.\nfromhumanfeedbackinllms. Elenberg. 2024. Gistscore: Learning better repre-\nsentationsforin-contextexampleselectionwithgist\nThomasAnthony,ZhengTian,andDavidBarber.2017. bottlenecks. InForty-firstInternationalConference\nThinkingfastandslowwithdeeplearningandtree onMachineLearning,ICML2024,Vienna,Austria,\nsearch. InAdvancesinNeuralInformationProcess- July21-27,2024.OpenReview.net.",
    "char_length": 1473
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 24,
    "text": "ing Systems 30: Annual Conference on Neural In-\nformationProcessingSystems2017,December4-9, Geunwoo Kim, Pierre Baldi, and Stephen McAleer.\n2017,LongBeach,CA,USA,pages5360–5370. 2023. Languagemodelscansolvecomputertasks. In\nAdvancesinNeuralInformationProcessingSystems\nHadiAskari,ShivanshuGupta,TerryTong,FeiWang, 36: AnnualConferenceonNeuralInformationPro-\nAnshuman Chhabra, and Muhao Chen. 2025. Un- cessingSystems2023,NeurIPS2023,NewOrleans,\nravelingindirectin-contextlearningusinginfluence LA,USA,December10-16,2023.\nfunctions.\nItayLevy,BenBogin,andJonathanBerant.2023. Di-\nBen Bogin, Kejuan Yang, Shashank Gupta, Kyle verse demonstrations improve in-context composi-\nRichardson,ErinBransom,PeterClark,AshishSab- tionalgeneralization. InProceedingsofthe61stAn-\nharwal,andTusharKhot.2024. Super: Evaluating nualMeetingoftheAssociationforComputational\nagents on setting up and executing tasks from re- Linguistics (Volume 1: Long Papers), pages 1401–\nsearchrepositories. 1422, Toronto, Canada. Association for Computa-\ntionalLinguistics.\nTomB.Brown,BenjaminMann,NickRyder,Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind JiachangLiu,DinghanShen,YizheZhang,BillDolan,\nNeelakantan,PranavShyam,GirishSastry,Amanda Lawrence Carin, and Weizhu Chen. 2022. What\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, makes good in-context examples for GPT-3? In\nProceedingsofDeepLearningInsideOut(DeeLIO Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté,",
    "char_length": 1448
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 25,
    "text": "2022): The 3rd Workshop on Knowledge Extrac- Yonatan Bisk, Adam Trischler, and Matthew J.\ntionandIntegrationforDeepLearningArchitectures, Hausknecht.2021. Alfworld: Aligningtextandem-\npages100–114,Dublin,IrelandandOnline.Associa- bodiedenvironmentsforinteractivelearning. In9th\ntionforComputationalLinguistics. International Conference on Learning Representa-\ntions, ICLR 2021, Virtual Event, Austria, May 3-7,\nNelsonF.Liu,KevinLin,JohnHewitt,AshwinParan-\n2021.OpenReview.net.\njape,MicheleBevilacqua,FabioPetroni,andPercy\nLiang.2024. Lostinthemiddle: Howlanguagemod- HongjinSu,JungoKasai,ChenHenryWu,WeijiaShi,\nelsuselongcontexts. TransactionsoftheAssociation TianluWang,JiayiXin,RuiZhang,MariOstendorf,\nforComputationalLinguistics,12:157–173. LukeZettlemoyer,NoahA.Smith,andTaoYu.2023.\nSelectiveannotationmakeslanguagemodelsbetter\nArindam Mitra, Luciano Del Corro, Guoqing Zheng,\nfew-shotlearners. InTheEleventhInternationalCon-\nShweti Mahajan, Dany Rouhana, Andres Codas,\nference on Learning Representations, ICLR 2023,\nYadong Lu, Wei ge Chen, Olga Vrousgos, Corby\nKigali,Rwanda,May1-5,2023.OpenReview.net.\nRosset,FillipeSilva,HamedKhanpour,YashLara,\nandAhmedAwadallah.2024. Agentinstruct: Toward\nSimengSun,YangLiu,ShuohangWang,DanIter,Chen-\ngenerativeteachingwithagenticflows.\nguangZhu,andMohitIyyer.2024. PEARL:Prompt-\nReiichiroNakano,JacobHilton,SuchirBalaji,JeffWu, ing large language models to plan and execute ac-",
    "char_length": 1422
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 26,
    "text": "Long Ouyang, Christina Kim, Christopher Hesse, tions over long documents. In Proceedings of the\n18thConferenceoftheEuropeanChapteroftheAs-\nShantanuJain,VineetKosaraju,WilliamSaunders,\nsociationforComputationalLinguistics(Volume1:\nXu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\nKrueger,KevinButton,MatthewKnight,Benjamin Long Papers), pages 469–486, St. Julian’s, Malta.\nChess,andJohnSchulman.2021. Webgpt: Browser- AssociationforComputationalLinguistics.\nassistedquestion-answeringwithhumanfeedback.\nHarshTrivedi,TusharKhot,MareikeHartmann,Ruskin\nPranav Putta, Edmund Mills, Naman Garg, Sumeet Manku, Vinty Dong, Edward Li, Shashank Gupta,\nMotwani,ChelseaFinn,DivyanshGarg,andRafael Ashish Sabharwal, and Niranjan Balasubramanian.\nRafailov.2024. Agentq: Advancedreasoningand 2024. AppWorld: Acontrollableworldofappsand\nlearningforautonomousaiagents. peopleforbenchmarkinginteractivecodingagents.\nIn Proceedings of the 62nd Annual Meeting of the\nYujiaQin,ShihaoLiang,YiningYe,KunlunZhu,Lan AssociationforComputationalLinguistics(Volume1:\nYan,YaxiLu,YankaiLin,XinCong,XiangruTang, LongPapers),pages16022–16076,Bangkok,Thai-\nBillQian,SihanZhao,LaurenHong,RunchuTian,\nland.AssociationforComputationalLinguistics.\nRuobing Xie, Jie Zhou, Mark Gerstein, Dahai Li,\nZhiyuanLiu,andMaosongSun.2024. Toolllm: Fa- ZihaoWang,ShaofeiCai,GuanzhouChen,AnjiLiu,Xi-\ncilitating large language models to master 16000+ aojianMa,andYitaoLiang.2023. Describe,explain,",
    "char_length": 1443
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 27,
    "text": "real-world apis. In The Twelfth International Con- planandselect: Interactiveplanningwithlargelan-\nference on Learning Representations, ICLR 2024, guagemodelsenablesopen-worldmulti-taskagents.\nVienna,Austria,May7-11,2024.OpenReview.net.\nJohn Yang, Akshara Prabhakar, Karthik Narasimhan,\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\nand Shunyu Yao. 2023. Intercode: Standardizing\n2022. Learning to retrieve prompts for in-context\nandbenchmarkinginteractivecodingwithexecution\nlearning. InProceedingsofthe2022Conferenceof\nfeedback. InAdvancesinNeuralInformationPro-\ntheNorthAmericanChapteroftheAssociationfor\ncessingSystems36: AnnualConferenceonNeural\nComputationalLinguistics: HumanLanguageTech-\nInformationProcessingSystems2023,NeurIPS2023,\nnologies, pages 2655–2671, Seattle, United States.\nNewOrleans,LA,USA,December10-16,2023.\nAssociationforComputationalLinguistics.\nShunyu Yao, Howard Chen, John Yang, and Karthik\nJohnSchulman,FilipWolski,PrafullaDhariwal,Alec\nNarasimhan.2022. Webshop: Towardsscalablereal-\nRadford,andOlegKlimov.2017. Proximalpolicy\nworldwebinteractionwithgroundedlanguageagents.\noptimizationalgorithms.\nInAdvancesinNeuralInformationProcessingSys-\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, tems35: AnnualConferenceonNeuralInformation\nJunxiaoSong,XiaoBi,HaoweiZhang,Mingchuan Processing Systems 2022, NeurIPS 2022, New Or-\nZhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. leans,LA,USA,November28-December9,2022.\nDeepseekmath: Pushingthelimitsofmathematical",
    "char_length": 1484
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 28,
    "text": "reasoninginopenlanguagemodels. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran,KarthikR.Narasimhan,andYuanCao.2023.\nNoah Shinn, Federico Cassano, Ashwin Gopinath, React: Synergizingreasoningandactinginlanguage\nKarthik Narasimhan, and Shunyu Yao. 2023. Re- models. InTheEleventhInternationalConference\nflexion: languageagentswithverbalreinforcement on Learning Representations, ICLR 2023, Kigali,\nlearning. In Advances in Neural Information Pro- Rwanda,May1-5,2023.OpenReview.net.\ncessingSystems36: AnnualConferenceonNeural\nInformationProcessingSystems2023,NeurIPS2023, JiachengYe,ZhiyongWu,JiangtaoFeng,TaoYu,and\nNewOrleans,LA,USA,December10-16,2023. Lingpeng Kong. 2023a. Compositional exemplars\nforin-contextlearning. InInternationalConference\nonMachineLearning,ICML2023,23-29July2023,\nHonolulu,Hawaii,USA,volume202ofProceedings\nofMachineLearningResearch,pages39818–39833.\nPMLR.\nXiYe,SrinivasanIyer,AsliCelikyilmaz,VeselinStoy-\nanov,GregDurrett,andRamakanthPasunuru.2023b.\nComplementaryexplanationsforeffectivein-context\nlearning. InFindingsoftheAssociationforCompu-\ntational Linguistics: ACL 2023, pages 4469–4484,\nToronto,Canada.AssociationforComputationalLin-\nguistics.\nZheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting\nDong,KemingLu,ChuanqiTan,ChangZhou,and\nJingrenZhou.2023. Scalingrelationshiponlearning\nmathematicalreasoningwithlargelanguagemodels.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger,andYoavArtzi.2020. Bertscore: Evalu-",
    "char_length": 1463
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 29,
    "text": "atingtextgenerationwithBERT. In8thInternational\nConferenceonLearningRepresentations,ICLR2020,\nAddisAbaba,Ethiopia,April26-30,2020.OpenRe-\nview.net.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameerSingh.2021. Calibratebeforeuse: Improv-\ning few-shot performance of language models. In\nProceedingsofthe38thInternationalConferenceon\nMachineLearning,ICML2021,18-24July2021,Vir-\ntualEvent,volume139ofProceedingsofMachine\nLearningResearch,pages12697–12706.PMLR.\nLongtao Zheng, Rundong Wang, Xinrun Wang, and\nBo An. 2024. Synapse: Trajectory-as-exemplar\nprompting with memory for computer control. In\nThe Twelfth International Conference on Learning\nRepresentations,ICLR2024,Vienna,Austria,May\n7-11,2024.OpenReview.net.\nRuiwenZhou,YingxuanYang,MuningWen,YingWen,\nWenhaoWang,ChunlingXi,GuoqiangXu,YongYu,\nandWeinanZhang.2024a. TRAD:enhancingLLM\nagentswithstep-wisethoughtretrievalandaligned\ndecision. InProceedingsofthe47thInternational\nACMSIGIRConferenceonResearchandDevelop-\nmentinInformationRetrieval,SIGIR2024,Washing-\ntonDC,USA,July14-18,2024,pages3–13.ACM.\nShuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou,\nRobertLo,AbishekSridhar,XianyiCheng,Tianyue\nOu,YonatanBisk,DanielFried,UriAlon,andGra-\nham Neubig. 2024b. Webarena: A realistic web\nenvironmentforbuildingautonomousagents. InThe\nTwelfthInternationalConferenceonLearningRep-\nresentations,ICLR2024,Vienna,Austria,May7-11,\n2024.OpenReview.net.\nA AgentDetails • Direct Preference Optimization + MCTS\n(DPO-MCTS)(Puttaetal.,2024). Collects",
    "char_length": 1494
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 30,
    "text": "Table 3 shows various hyperparameters used for\npreference pairs into a replay buffer using\naReActsolverandPnEexecutorwhensolvinga\nMonte-CarloTreeSearch.\ntaskorsubtask,respectively.\n• Proximal Policy Optimization (PPO)\nA.1 PromptFormats\n(Schulmanetal.,2017). PPOwithalearned\nReActSolverFig.7showstheformatofthetask advantageestimate.\ncontextweusefortheReActsolver. Italsoshows\n• REINFORCEleave-one-out(RLOO)(Ah-\ntheJSONformatinwhichtheagentisconstrained\nmadian et al., 2024). On-policy trajectory-\nto generate its reasoning and action. The hyper-\nlevelREINFORCEwithleave-one-outadvan-\nparametersusedfortheReActsolveraregivenin\ntageestimate.\nTable3.\nPnEPlannerTheprompttemplatefortheplanner • Grouprelativepolicyoptimization(GRPO)\nisgiveninFig. 9. Theplannerisalsoconstrained (Shaoetal.,2024). On-policyPPOwithnor-\ntogeneratetheplanusingaJSONformat. Forthe malizedleave-one-outadvantageestimate.\nplanner, we use temperature 0.1 and top_p 0.5\n• Leave-one-out PPO (LOOP) (Chen et al.,\nbothduringannotationandevaluation.\n2025). Off-policy PPO with unnormalized\nPnEExecutorThePnEexecutorissimilartothe\nleave-one-outadvantageestimate.\nReActsolver. Theformatofthetaskcontextused\nfor the PnE executor is given in Fig. 8. A The\nexecutorisalsoconstrainedtogenerateitsreason-\ningandusingthesameJSONformatastheReAct\nsolver. ThehyperparametersusedforthePnEex-\necutoraregiveninTable3.\nPrompt Truncation When the ReAct solver or\nPnEexecutor’spromptexceedsthecorresponding",
    "char_length": 1456
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 31,
    "text": "contextlengthlimit,wetruncateditbyfirsthiding\ntheolderandlongerobservationsandthenhiding\nanyremainingolderobservations.\nA.2 DemonstrationTemplates\nThe templates used to show the task-trajectory,\nsubtask-trajectory,andsnippetdemonstrationsare\ngiveninFigs. 10,11,and12,respectively.\nB TrainedBaselines\nWecomparewiththefollowingtraining-basedap-\nproachesbaselinesfromChenetal.(2025):\n• Groundtruthsupervisedfine-tuning(SFT-\nGT). SFT on ReAct-style transformation of\ngoldsolutions.\n• Rejection sampling fine-tuning (RFT)\n(Yuan et al., 2023). Collects rollouts gen-\neratedwiththebasemodelandfinetuneson\nsuccessfulones.\n• Expertiteration(EI)(Anthonyetal.,2017).\nRunsmultiplesmalleriterationsofRFTusing\nthecurrentbestmodel.\nAnnotation Evaluation\nHyperparameter\nReAct PnEExecutor ReAct PnEExecutor\ntemperature 0.1 0.3 0.1 0.1\ntop_p 0.5 0.5 0.5 0.5\nmax_context_length 40000 20000 1000000 1000000\nmax_steps 50 20 50 50\nmax_tokens 2000 2000 2000 2000\nTable3: DecodinghyperparametersforAnnotationandICLEvaluationexperimentsforReActsolverandPnE\nExecutor. max_context_lengthlimitsthelengthoftheinputpromptwhilemax_tokenslimitsthelengthofthe\noutput. max_stepsisthemaximumnumberofstepsfortheagenttotake.\nI am your supervisor and you are a super-intelligent AI Assistant whose job is to assist with my day-to-day\ntasks involving various apps (e.g., amazon.com, gmail, calendar, etc.). To do this, we will take part in a *",
    "char_length": 1405
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 32,
    "text": "multi-turn conversation* with a Python REPL environment that will let you interact with the apps using their\nAPIs.\nAt every step of the conversation, you will need to reason about your current progress on the task and\npropose the next action in the form of a Python code snippet for the environment to execute. Your response\nneeds to be in the following JSON format {\"thought\": <thought>, \"action\": <action>} where <thought> is your\nreasoning and <action> the Python code snippet (without any enclosing ```).\nThe environment will then execute your code and respond with the output. The environment will maintain state\nacross multiple interactions for the on-going task so that any variables defined in one step can be used in\nsubsequent steps. Additionally, since you'll be solving a lot of tasks, it is possible that your reasoning\non the current step matches with the reasoning at some step of a task you have solved before. When this\nhappens, I will provide you with the relevant snippet of your conversation solving that task. These may be\nhelpful for your next step.\nHere are three key APIs that you can use to get more information about apps and APIs:\n# To get a list of apps that are available to you:\nprint(apis.api_docs.show_app_descriptions())\n# To get the list of apis under any app listed above, e.g. amazon\nprint(apis.api_docs.show_api_descriptions(app_name='amazon'))\n# To get the full input-output specification of a particular api, e.g. amazon app's login api",
    "char_length": 1476
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 33,
    "text": "print(apis.api_docs.show_api_doc(app_name='amazon', api_name='show_cart'))\n<task_trajectory_demonstrations>\nNow, here is the actual task you need to solve using a fresh environment.\nMy name is Joyce Weaver. My personal email is joyce-weav@gmail.com and phone number is 3155673041.\nTask: Request $13 publicly on Venmo from my friend, Stacy, with a note, \"For yesterday's meal\".\nHere are some key guidelines that you need to follow:\n(1) Make sure to produce a *single* thought and action at every step and correctly format them as {\"thought\n\": <thought>, \"action\": <action>}. In particular,\n- the JSON should be valid with any quotes, newlines, etc., properly escaped.\n- <action> should be just code without any enclosing backticks (```).\n(2) Always look at API specifications (using apis.api_docs.show_api_doc) before calling an API.\n(3) Remember you can use the variables in your code in subsequent code blocks.\n(4) Remember that the email addresses, access tokens and variables (e.g. amazon_password) in the example\nabove are not valid anymore. You will be provided a fresh environment to work with. Note, however, that the\nAPIs remain the same so if it's been shown in an example above, you don't need to look at its specification\nagain.\n(5) You can use the \"supervisor\" app to get information about my accounts and use the \"phone\" app to get\ninformation about friends and family.\n(6) Many APIs return items in \"pages\". Make sure to run through all the pages by looping over `page_index`.",
    "char_length": 1491
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 34,
    "text": "(7) If your action produces output that is too long, the environment will truncate it with '... [HIDDEN FOR\nBREVITY] ...' replacing the middle part. E.g., this will happen if you print a large number of items\nreturned by a search API. In such cases, you should consider using a more precise query to reduce the number\nof items returned.\n(8) Once you have completed the task, make sure to call apis.supervisor.complete_task(). If the task asked\nfor some information, return it as the answer argument, i.e., call apis.supervisor.complete_task(answer=<\nanswer>). Many tasks do not require an answer, so in those cases, just call apis.supervisor.complete_task()\ni.e. do not pass any argument.\nFigure7:TaskcontextfortheReActsolver. Eachboxisaseparatemessage. <task trajectory_demonstrations\nistheplaceholderforanytrajectorydemonstrations(seeFig. 10forthecorrespondingtemplate).\nI am your supervisor and you are a super intelligent AI Assistant whose job is to assist with my day-to-day\ntasks involving various apps (e.g., amazon.com, gmail, calendar, etc.). As the tasks can be complex, I will\nbreak them down into a sequence of subtasks that you will need to carry out one at a time. To do this, you\nwill take part in a *multi-turn conversation* with a Python REPL environment that will let you interact with\nthe apps using their APIs.\nAt every step of the conversation, you will need to reason about your current progress on the subtask and",
    "char_length": 1438
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 35,
    "text": "propose the next action in the form of a Python code snippet for the environment to execute. Your response\nneeds to be in the following JSON format {\"thought\": <thought>, \"action\": <action>} where <thought> is your\nreasoning and <action> the Python code snippet (without any enclosing ```).\nFinally, when you have completed the subtask, you will need to say FINISH as action in a separate response\nin the same format, i.e.,\n{\"thought\": <thought>, \"action\": FINISH}\nThe environment will then execute your code and respond with the output. The environment will maintain state\nacross multiple interactions for the on-going task so that any variables defined in one step can be used in\nsubsequent steps. Additionally, when your reasoning for the current step matches with a task you have\nsolved previously, I will also provide you with the relevant portion of the conversation that solved that\ntask to help you on subsequent steps for the current task.\nHere are three key APIs that you can use to get more information about apps and APIs:\n# To get a list of apps that are available to you:\nprint(apis.api_docs.show_app_descriptions())\n# To get the list of apis under any app listed above, e.g. amazon\nprint(apis.api_docs.show_api_descriptions(app_name='amazon'))\n# To get the full input-output specification of a particular api, e.g. amazon app's login api\nprint(apis.api_docs.show_api_doc(app_name='amazon', api_name='show_cart'))\n<subtask_trajectory_demonstrations>",
    "char_length": 1464
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 36,
    "text": "Now, here is the actual task you need to solve using a fresh environment.\nMy name is Joyce Weaver. My personal email is joyce-weav@gmail.com and phone number is 3155673041.\nTask: Request $13 publicly on Venmo from my friend, Stacy, with a note, \"For yesterday's meal\".\nThe above task can be decomposed into the following subtasks that need to be carried out one by one\n1. Login to Venmo and save access token in `venmo_access_token` variable.\n2. Use the `venmo_access_token` to search for Stacy in my Venmo contacts and save her user ID in `\nstacy_user_id`.\n3. Use the `venmo_access_token` to create a payment request to `stacy_user_id` for $13 with the note 'For\nyesterday's meal'. Ensure the request is set to public.\n4. Complete task.\nLet's start by solving Subtask 1: Login to Venmo and save access token in `venmo_access_token` variable.\nHere are some key guidelines that you need to follow:\n(1) Do not worry about the entire task. Focus ONLY on the current subtask and carry it out carefully and\ncorrectly.\n(2) Make sure to produce a *single* thought and action at every step and correctly format them as {\"thought\n\": <thought>, \"action\": <action>}. In particular,\n- the JSON should be valid with any quotes, newlines, etc., properly escaped.\n- <action> should be just code without any enclosing backticks (```).\n(3) When finished with the current subtask, make sure to say FINISH in a SEPARATE response in the format",
    "char_length": 1424
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 37,
    "text": "described above. Also, if it's the last subtask, make sure to call `apis.supervisor.complete_task()` with\nthe answer, if any, before finishing.\n(4) Always look at API specifications (using apis.api_docs.show_api_doc) before calling an API.\n(5) Remember you can use the variables in your code in subsequent code blocks.\n(6) Remember that the email addresses, access tokens and variables (e.g. amazon_password) in the example\nabove are not valid anymore. You will be provided a fresh environment to work with. Note, however, that the\nAPIs remain the same so if it's been shown in an example above, you don't need to look at its specification\nagain.\n(7) You can use the \"supervisor\" app to get information about my accounts and use the \"phone\" app to get\ninformation about friends and family.\n(8) Many APIs return items in \"pages\". Make sure to run through all the pages by looping over `page_index`.\n(9) If your action produces output that is too long, the environment will truncate it with '... [HIDDEN FOR\nBREVITY] ...' replacing the middle part. E.g., this will happen if you print a large number of items\nreturned by a search API. In such cases, you should consider using a more precise query to reduce the number\nof items returned.\nFigure 8: Task context for the PnE executor. Each box is a separate message. <subtask\ntrajectory_demonstrations is the placeholder for any trajectory demonstrations (see Fig. 11 for the corre-\nspondingtemplate).",
    "char_length": 1447
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 38,
    "text": "I am your supervisor and you are a super intelligent AI Assistant whose job is to autonomously perform my\nday-to-day tasks involving various apps (e.g., spotify, simple_note, etc.).\nTo do this, you will need to interact with apps using their associated APIs on my behalf.\nHere are the apps:\n{ app_descriptions }\nTo help with this, I will provide you with an executor model that will take care of interacting with the\nAPIs. Your job is to produce a plan on how to solve the task given access to this executor. You should\nrespond in the following JSON format:\n{\n\"thought\": <thought>,\n\"plan\": [\n\"<subtask_1>\",\n\"<subtask_2>\",\n...\n\"<subtask_n>\"\n]\n}\nHere, <thought> is a brief description of how you plan to solve the task and <subtask_i> is a description of\nthe ith subtask in your plan.\n**Key instructions**:\n(1) Make sure to respond in the JSON format provided above. Don't enclose your response with ```.\n(2) Make sure to define all the relevant variables.\n(3) Each subtask in the plan should be clear and complete so that it can be executed independently. E.g. don\n't use references such as \"previous list\" or \"repeat subtask 1\".\n(4) When unsure, use more subtasks in the plan rather than fewer subtasks.\n(5) The final subtask should be \"Complete task.\" unless the task requires an answer, in which case, it\nshould be \"Complete task with answer: <answer>\" where <answer> is the answer to the task.\nFigure9: PromptforthePnEplanner. Itisfollowedbyafewdemonstrativetask-planpairsandthetesttask.",
    "char_length": 1491
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 39,
    "text": "Here is an example of a task and how you can interact with the environment to solve it\nMy name is Joyce Weaver. My personal email is joyce-weav@gmail.com and phone number is 3155673041.\nTask: How many playlists do I have in Spotify?\nLets first find which APIs are available to use in Spotify.\n```python\nprint(apis.api_docs.show_api_descriptions(app_name='spotify'))\n```\nOutput:\n```\n[\n...\n{\n\"name\": \"login\",\n\"description\": \"Login to your account.\"\n},\n{\n\"name\": \"logout\",\n\"description\": \"Logout from your account.\"\n},\n...\n]\n```\n.\n.\n.\nNow that the task is completed, I need to mark the task as complete and return the number of playlists found\n.\n```python\napis.supervisor.complete_task(answer=num_playlists)\n```\nOutput:\n```\nMarked the active task complete.\n```\nFigure10: Templateusedfortrajectorydemonstrations. Yellowboxesareusermessagesorenvironmentresponses,\nwhileblueboxesareagentmessages(originallyinJSONformat).\nHere is an example of a subtask and how you can interact with the environment to solve it\nMy name is Jennifer Powell. My personal email is jepowell@gmail.com and phone number is 4288705164.\nTask: Add a comment, \"Thank you!\", to all the venmo payments I received from my coworkers in the last 5 days\n(including today), and like those payments.\nThe above task can be decomposed into the following subtasks that need to be carried out one by one\n1. Login to Venmo and save access token in `venmo_access_token` variable.",
    "char_length": 1432
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 40,
    "text": "2. Use the `venmo_access_token` variable to retrieve a list of coworkers from the Phone app and save it in `\ncoworker_list`.\n3. Use the `venmo_access_token` variable to get all payments received in the last 5 days and save them in `\nrecent_payments`.\n4. Filter `recent_payments` to find payments from `coworker_list` and save them in `coworker_payments`.\n5. For each payment in `coworker_payments`, add the comment 'Thank you!' and like the payment using the `\nvenmo_access_token`.\n6. Complete task.\nLet's start by solving Subtask 1: Login to Venmo and save access token in `venmo_access_token` variable.\nTo login to Venmo, I need to find the appropriate API and its specifications. Let's start by checking the\navailable APIs for the Venmo app.\n```python\nprint(apis.api_docs.show_api_descriptions(app_name='venmo'))\n```\n.\n.\n.\nI have successfully logged into Venmo and obtained the access token, which is stored in the `\nvenmo_access_token` variable. This completes the first subtask.\n```python\nFINISH\n```\nFigure11: Templateusedforsubtasktrajectorydemonstrations. Yellowboxesareusermessagesorenvironment\nresponses,whileblueboxesareagentmessages(originallyinJSONformat). Themaindifferencewiththetask\ntrajectorydemonstrations(Fig. 10)isthattheagentisadditionallyprovidedtheplanandasummaryofcodeusedto\nsolveprevioussubtasks.\nYour reasoning (thought) in the previous step is similar to your reasoning solving some other tasks.",
    "char_length": 1422
  },
  {
    "paper_id": "In-context_learn",
    "chunk_id": 41,
    "text": "Providing you with the relevant snippets of your conversation solving those tasks as they may be helpful for\nyour next step. Each snippet comprises the task statement along with the thought, the action taken, and the\noutput for the matching step as well as the following step (if there is one).\n# Snippet 1:\nMatching Step:\nThought: {thought}\nAction:\n```python\n{action}\n```\nOutput:\n```\n{output}\n```\nFollowing Step:\nAction:\n```python\n{action}\n```\nOutput:\n```\n{output}\n```\n# Snippet 2:\n...\nFigure12: Templateusedforshowingsnippetdemonstrations. Thisisasinglemessageappendedaftertoallthe\nmessages.",
    "char_length": 594
  }
]