[
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 0,
    "text": "Published in Transactions on Machine Learning Research (07/2024)\nPRD: Peer Rank and Discussion Improve\nLarge Language Model based Evaluations\nRuosen Li Teerth Patel Xinya Du\nDepartment of Computer Science, The University of Texas at Dallas\n{ruosen.li, teerth.patel, xinya.du}@utdallas.edu\nReviewed on OpenReview: https://openreview.net/forum?id=YVD1QqWRaj\nAbstract\nNowadays, the quality of responses generated by different modern large language models\n(LLMs) is hard to evaluate and compare automatically. Recent studies suggest and pre-\ndominantly use LLMs for reference-free evaluation of open-ended question answering. More\nspecifically, they use the recognized “strongest” LLM as the evaluator, which conducts\npairwise comparisons of candidate models’ answers and provides a ranking score. However,\nthis intuitive method has multiple problems, such as bringing in self-enhancement (favoring\nits own answers) and positional bias. We draw insights and lessons from the educational\ndomain (Cho & MacArthur, 2011; Walsh, 2014) to improve LLM-based evaluations. Specifi-\ncally, we propose (1) the peer rank (PR) algorithm that takes into account each peer LLM’s\npairwise preferences of all answer pairs, and outputs a final ranking of models; and (2)\npeer discussion (PD), where we prompt two LLMs to discuss and try to reach a mutual\nagreement on the preferences of two answers. We conduct experiments on two benchmark",
    "char_length": 1419
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 1,
    "text": "datasets. We find that our approaches achieve higher accuracy and align better with human\njudgments. Interestingly, PR can induce a relatively accurate self-ranking of models under\nthe anonymous setting, where each model’s name is unrevealed. Our work provides space to\nexplore evaluating models that are hard to compare for humans.1\n1 Introduction\nWitharisingnumberoflargelanguagemodels(LLMs)beingdevelopedevermorequicklyrecently,evaluations\nbecome increasingly important as they encode values and priorities that the LLM community should improve\nupon (Jones & Galliers, 1995; Liang et al., 2022). At the same time, the evaluation becomes harder as\nwell. For example, recent models finetuned with reinforcement learning from human feedback (RLHF)\ndemonstrate greater alignment with human preferences, but this capability usually cannot be reflected by\ndecent performance on standard NLP benchmarks (e.g., MMLU (Hendrycks et al., 2020) and ARC (Clark\net al., 2018)). Furthermore, human queries span a diverse range of settings and scenarios, making it nearly\nimpossible to list them all Fan et al. (2019); Ouyang et al. (2022).\nTotacklethisdiscrepancy,open-endedquestionsarebeingusedmoreoftentotestLLMs’performance(Chiang\net al., 2023). Then, by default, evaluation is done by collecting human preferences of pairwise comparisons\nand then calculating scores for each LLM to induce a general ranking. Yet the collection process is costly and",
    "char_length": 1441
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 2,
    "text": "time-consuming, to automate and scale up the evaluation, most recent works utilize the state-of-the-art LLM\nas the judge (Dubois et al., 2023; Lin & Chen, 2023). However, various studies show that this method is\nproblematic, as the pairwise comparison judgment provided usually contains various biases, such as favoring\nLLMs’ own answers Liu et al. (2023); Zheng et al. (2023).\n1Codesareavailableat: https://github.com/bcdnlp/PRD.\n1\n4202\nceD\n13\n]LC.sc[\n3v26720.7032:viXra\nPublished in Transactions on Machine Learning Research (07/2024)\nMotivated by these limitations, we propose the idea of peer evaluation. The goal is to mitigate the biases in\nautomated evaluations while still benefiting from LLM’s strong capability in reading and writing reviews. We\npropose Peer Rank and Discussion-based evaluation framework (PRD). The suit consists of two alternatives\nthat share the same format and goal – involving peer LLMs’ participation as reviewers to reach a more\nfair evaluation result where all peers mutually agree. We draw insights and lessons from educational\npsychology research field on methodologies of student peer reviewing (Walsh, 2014), as well as their impact\nand benefits (Cho & MacArthur, 2011; Yalch et al., 2019). More specifically, Peer Rank (PR) is utilized for\nglobal rankings and induces reviewer weights. It works for the tournament-style benchmarking setting where\neach LLM in pairwise matches produces an answer for an open-ended question. Instead of using the average",
    "char_length": 1492
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 3,
    "text": "vote to decide the final preference scoring, we propose weighted votes based on LLMs reviewers’ capabilities.\nPeer Discussion (PD) facilitates fine-grained pairwise comparison/ranking. It works for the general pairwise\ncomparison setting. Given two candidate answers, we prompt two other reviewer LLMs to have multi-turn\ndiscussions to reach a mutual agreement on the pairwise scoring or preference. The process shares a similar\nformat of LLM interacting with each other through conversations like two communicative agents (Li et al.,\n2023; Park et al., 2023; Fu et al., 2023b). PR and PD are closely interrelated and fall under the same theme\nof providing a more fair (de-biased) ranking of long- and free-form answers. enumerate We conduct extensive\nexperiments and analysis for measuring PR and PD’s capabilities of providing fair pairwise comparisons. PR\nis tested on Vicuna80 Chiang et al. (2023), which contains pairwise judgments from human annotators. Our\nmethod improves correlations with human rankings substantially. This paradigm also enables a group of\nLLMs to induce a self-ranking. PD is tested on both Vicuna80 and LFQA (Xu et al., 2023), which includes\nannotated pairwise comparisons of Human-Machine and Machine-Machine answers. PD enables LLMs to\nachieve better pairwise comparisons that are more accurate than single model-based reviews. Both PR and\nPD mitigate the above mention biases especially self-enhancement bias significantly. Further, we provide",
    "char_length": 1475
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 4,
    "text": "more analysis for PD, showing: (1) the LLM leading discussions is less likely to alter its opinion; (2) stronger\nLLMs are more likely to hold their opinions.\n2 Related Work\nAutomatic Evaluations Natural Language Generation (NLG) evaluation methods are mainly of a\nsimilarity-based or reference-free type. For similarity-based metrics, the generated texts are compared to\nreferencetextsPapinenietal.(2002);Zhangetal.(2019). Inparallel, peoplehavealsodevelopedtask-specific\nmetrics such as consistency (Kryściński et al., 2020; Wang et al., 2020), faithfulness (Fabbri et al., 2022; Gao\net al., 2023) and coherence (Dziri et al., 2019). This is similar to our peer discussion idea on designing more\nspecific prompts for large language model-based evaluations. Our prompting-based method is more flexible\nand can act as a unified evaluator (Zhong et al., 2022).\nSpecifically, for long-form or open-ended question answering, early work uses ROUGE (Lin (2004)) to\nmeasurethesimilaritybetweenhumanandmachine-generatedanswers. However,researchersfindthatROUGE\nis not a fair metric for quality measurement due to the open-ended nature of long-form answers (Krishna\net al., 2021; Xu et al., 2023). Fu et al. (2023a) propose GPTScore, which evaluates texts with generative\npre-training models like GPT-3. Xu et al. (2023) implements a similar idea for evaluating long-form answers.\nGiven a prompt consisting of a question with two answer candidates, GPT-3 is fine-tuned to output the label",
    "char_length": 1480
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 5,
    "text": "answer 1 or answer 2 (pairwise comparisons).\nLLMs as evaluators: problems and challenges Mostrecently, withthetrendofdevelopingmoreLLMs,\nevaluations for benchmarking the progress have become even more important but also more difficult. They\nare tested on both standard datasets such as MMLU, and more importantly, on open-ended questions which\nare much more prevalent in real life (Nakano et al., 2021; Chiang et al., 2023). People mostly use GPT-4 (Liu\net al., 2023; OpenAI, 2023) as an evaluator for either generating scores or pairwise comparisons (Wang\net al., 2023b; Zhou et al., 2023). However, such a strategy has fundamental problems because of various\nbiases, such as (1) positional bias (Dettmers et al., 2023; Wang et al., 2023a), where a model favors the first\nanswer in pairwise comparisons; (2) verbosity and length bias (Wang et al., 2023b); (3) and most importantly,\nself-enhancement bias, where an LLM favors its own answers (Liu et al., 2023; Zheng et al., 2023).\n2\nReviewers\nA B C\nWeights Score\nPairwise\nA\nBattles\nRank\nB\nA > C > B\nC\nnormalize\nReviewers\nnormalize\nA B C (multi-rounds)\n1\n3\nRank\n2\n> >\n1 2 dot\n3 product\nPairwise\nContestants\nBattles Win Rate Matrix Weight Vector Score Vector\nPublished in Transactions on Machine Learning Research (07/2024)\nReviewers\nA B C Better contestant's review weigh more\nBetter contestants\nscore higher\n1\n3\nRank\n2\n> >\n1 2 dot\n3 product\nPairwise\nContestants\nBattles Win Rate Matrix Normalize\nReviewer(Multi-round)Contestent",
    "char_length": 1479
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 6,
    "text": "Weight Vector Score Vector\nFigure 1: The peer rank process (PR): each LLM model acts both as reviewers (A, B, C) and contestants (1,\n2, 3). From the battles between contestants (pairwise comparisons), it induces a self-ranking. In this example,\nReviewers\nmodels A, B, and C represent GPT-4, BAard, andBClaudeC, respectively.\nBetter contestant weigh more\nBetter contestants\nEfforts have been proposed to tackle them: (1) Using position switchings(coWre haignhegr et al., 2023a) for mitigating\n1\npositional bias; (2) Zheng et al. (2023) proposes Chatbot Arena, where real users ask questions and provide\npairwise judgments of answers generated by two LLMs. But this is time-consuming and requires expert\nannotation to ensure fairness; (3) Bai et al. (2023) propose using each LLM as an examiner, where each\n2\ngenerates questions to test other models. Different from PD, their “exams” are biased with randomly\ngenerated questions. Moreover, none of the above works support inducing self-rankings through peer ranking.\ndot\nproduct\nOverall, our work, peer ev3aluation-oriented methods, undergo tests on two benchmarks, each covering a\nvariety of tasks, and focuses more on the alignment between LLMs’ evaluations and human judgments.\nContestent\nPairwise Battles Normalize\nLLM Interactions and Discussion WTinh Reartee Mhaatrvixe been works that explore Multi-agent discussion with\nReviewer(Multi-round) Contestent",
    "char_length": 1409
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 7,
    "text": "LLMs, including Liang et al. (2023); Du et al. (2023); Chan et al. (2023); Chen et al. (2023). All of them\nWeight Vector Score Vector\nare task-oriented frameworks aiming at improving LLMs’ performance on general tasks. Du et al. (2023)\nfocused on math tasks and question answering tasks. Liang et al. (2023) tested two tasks, including machine\ntranslation and question answering. Chen et al. (2023) covered math and reasoning. Chan et al. (2023)\nimplemented a multi-agent debate framework with multiple prompts for evaluation. Prior works utilize LLM\ninteractions to accomplish tasks and improve models’ accuracy. For our case, based on one response from\nhumans and another from LLM, our approach utilizes LLM interactions to discuss which one is better and in\norder to achieve better evaluation that align with human preferences.\nPeer Evaluation in the Educational Domain Prior works on educational research mainly focus on\nhuman-in-the-loop studies, such as in the classroom (Cho & MacArthur, 2011; Nicol et al., 2014). They\nconduct human-oriented data collection and experiments to verify the benefits of peer evaluations (Walsh,\n2014). In contrast, we focus on automatic evaluation, employing peer LLM reviewers to conduct pairwise\ncomparisons of LLMs’ answers. Moreover, our peer rank process focuses on pairwise comparisons, instead of\nabsolute grades.\n3 Methodologies\nIn general, Peer Rank (PR) can be applied to induce self-ranking – a ranking of a group of LLMs’ own",
    "char_length": 1476
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 8,
    "text": "capabilities. Peer Discussion (PD) provides a more fine-grained and interactive comparison of two models’\nanswers. Both of them aim at reducing the bias in automatic evaluations. We elaborate on the technical\ndetails in this section.\n3\nAnswer: How do credit/debit cards work? What is the process of putting money in and getting it out?\n1 A credit or debit card is basically 2\nDebit cards are linked to a bank\njust an easy way to allow a shop\naccount and whenever you pay\nto speak to your bank.\nusing a debit card, ...\nFirst you go into the bank, [...]\nReview answer of 1 and 2 [...] list the index of the better response in a new line\nA Answer 1 provides a more detailed and narrative explanation, using an analogy of [...]\nAnswer 2, on the other hand, is more concise and uses more technical language [...]\n1\nAnswer 1 provides a basic overview of [...] at a high level. However, [...]\nB Answer 2 provides a more coherent explanation by separately discussing how [...]\n2\nDiscuss answer of 1 and 2 with reviews from A and B in mind [...] again output choice on a line\nAfter considering Reviewer B's perspective, [...]\nA\nWhile Answer 2 [...], I believe that the accessibility and comprehensiveness of Answer 1\noutweigh the concise and technical nature of Answer 2.\nB I can appreciate Reviewer A's perspective on [...] Upon reflection, for the purposes of this\nquestion, accessibility and comprehensiveness are most important [...]",
    "char_length": 1429
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 9,
    "text": "after considering Reviewer A's perspective, I would change my preference to Answer 1.\nPublished in Transactions on Machine Learning Research (07/2024)\n3.1 Peer Rank and Scoring (PR)\nFigure 1 illustrates the peer rank algorithm. The general idea is to obtain weighted scores of each battle from\nthe peer reviewer’s judgment, and then induce self-rankings from the scores. This process is iterated multiple\ntimes until the scores converge.\nGiven a set of questions Q, we generate an answer to each question from each LLM. Let A (q) be the\nm\nanswer to question q ∈Q by the model m. Each battle represents two models (the contestants) answering the\nsame question q. The comparison of the answers in a battle by the LLM reviewer model r forms a review.\nLet K (x,y) be the score given by the reviewer r to the pair of answers (x,y). We use a score of −1 to\nr\nindicate the first answer is better, 0 to indicate a tie, and 1 to indicate the second answer is better. Suppose\nwe have a set of reviewer models R and a set of contestant models C. We form a set of battle reviews,\nB ={(q,i,j,r,s)|q ∈Q, (i,j)∈C2, r ∈R}, where s=K (A (q),A (q)) is the score given by reviewer r to\nr i j\nthe answers/responses generated by i and j for question q. We create a shorthand Kij(q) for this review.\nr\nBased on these peer reviews, we can evaluate models based on their performance by calculating metrics such\nas the win rate of each contestant and the Elo ratings (Section 3.1.2) of each contestant. Since each model is",
    "char_length": 1498
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 10,
    "text": "ranked by its peers, we call it Peer Rank.\nSpecifically,thesetofquestionsshouldbediverseandcovervarioustasks,suchasquestionanswering(12.5%),\nemail writing (12.5%), coding (6%), math solving (4%), etc. Answers/responses should also vary in format,\nincluding concise answers, step-by-step reasonings, detailed explanations, code snippets, long-form answers,\netc. Reviewers assess response pairs and indicate preferences in the process (“battle review”). Then, both\nwinrate and Elo metrics can be calculated.\n3.1.1 Win rate Calculation\nThe win rate for a contestant is the number of wins for that contestant divided by the number of battles it\nparticipates in. Ties are counted as 0.5 wins for both contestants.\nOur win rate calculation assigns differing weight to the scores provided by different reviewers (A, B, C) based\non the performance of the corresponding reviewers as a contestant (1, 2, 3). This operates on the assumption\nthat models which are better contestants are also more fit to evaluate and compare answers, so they should\nbe given more weight in evaluation (Equation 2). In another way, since the score is a measure of their ability\nto review/grade correctly, we weigh the win rate an LLM gives another LLM by their own score Walsh (2014).\nMoreover, the self-rewarding paper by Yuan et al. (2024) also made this assumption. They use a model itself\nfor evaluations during the iterations and prove that it makes sense. In their results, the better-performing",
    "char_length": 1472
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 11,
    "text": "models also perform well in providing high-quality evaluations/rewards to themselves.\nInitially, all reviewers are given the same weight. On each iteration of the calculation, the win rate for each\ncontestant is calculated using the current weights. The win rates are scaled to the range of [0,1] using a\nlinear scaling. Then, they are scaled again so that their sum is 1. Next, these results are used as the weights\nfor the next round.\nFormally, let Wc be the raw win rate of contestant c∈C from the reviews of reviewer r ∈R. This is equal to\nr\nthe number of times c wins a battle plus half of the number of times c ties, divided by the number of battles\nc participates in.\nX X (cid:2) f(Kdc(q))+f(−Kcd(q)) (cid:3)\nr r\n(1)\nWc = q d∈C,d̸=c\nr 2|Q|(|C|−1)\nwhere f(score)= score+1 maps a score of (loss=−1, tie=0, win=1) for the second contestant to a win\n2\ncount of (0, 0.5, 1), so that ties count as half of a win.\nNote that we negate Kcd(q) when inputting it into f so that the win value of c is computed instead of d.\nr\nAlso, since there are |Q| questions, |C−1| contestants to battle, and 2 orders for two contestants to battle,\nthere are 2|Q||C−1| battles involving a fixed contestant c.\n4\nPublished in Transactions on Machine Learning Research (07/2024)\nLet αk be the weight assigned to reviewer r after iteration k. Initially, α0 =1/|R|, so that all reviewers have\nr r\nthe same weight, and the weights add to 1. Namely, we assume each reviewer LLM has the same capabilities",
    "char_length": 1479
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 12,
    "text": "to start. The score of contestant c ∈ C for iteration k is the weighted average of the raw win rates for\ncontestant c. We set the weights for the next iteration to αk:\nX\nscorek = αk−1·Wc,αk =Norm(MinMax(scorek))\nc r r (2)\nr∈R\nwhere the weights are scaled to a range of [0,1] and finally normalized to have sum equal to 1:\nS−min (S )\nMinMax(S)= r∈R r\nmax (S )−min (S )\nr∈R r r∈R r\nGiven this set of equations, we look for the fixed/converging point of the framework. This process is\nreminiscent of the problem faced by the PageRank algorithm (Page et al., 1999). The detailed equivalent\nimplementation of PR is shown in the Algorithm 2 in Appendix E.\nThe whole process is simple but effective. It automatically adjusts weights for all models and mitigates the\nself-enhancement bias. During the process, every reviewer model’s score is considered instead of only one\nmodel itself (Equation 2). While a reviewer may favor its outputs, other reviewers’ scores provide a fair\nbalance. Additionally, weaker models’ reviewing weights decrease automatically (to near zero) because of\nthe normalization operation. Empirical tests showed that fixing the self-weight at zero resulted in poorer\nperformance.\n3.1.2 Elo Calculation\nAnother method for calculating the performance of a contestant relative to other contestants is the Elo\nrating (Elo, 1967; Askell et al., 2021), which is utilized for ranking players and widely used in games",
    "char_length": 1426
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 13,
    "text": "(battles) (Dettmers et al., 2023). It measures the relative skill levels of players by predicting the expected win\nrateagainstopponents. Ittakesasequenceofpairwisereviewsandgeneratesratingsforeachcontestant, with\na greater rating indicating better performance. This is a more fine-grained measurement compared to win\nrate. Based on a similar idea, we assign different weights to reviewers based on their previous performance\nsuch that a review from a higher-weight reviewer has a greater influence upon Elo ratings.\nSimilarly to the win rates calculation, we start with equal weights on all reviewers and then normalize the\nresulting Elo ratings to give weights for the next iteration. We repeat the Elo calculation with the new\nweights, update the weights based on the new ratings, and continue repeating until it converges. Walsh\n(2014) has proved the guarantee of convergence. Compared to Walsh (2014), which involves student grading\nin the educational domain, our work focuses on peer LLM reviewers conducting pairwise comparisons of\nLLMs’ answers, utilizing winrate and Elo scores. We introduce an automatic LLM peer evaluation metric for\nthe machine learning field and extend Walsh’s convergence proof, showing our method’s reliability through\nexperimental results. 2 More difference details are in appendix C.\nAbriefoverviewoftheactualEloratingscalculationfollows. Allcontestantsstartoutwithaninitialratingof",
    "char_length": 1416
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 14,
    "text": "1000. On each battle, the expected likelihood of each contestant winning is calculated based on the difference\nbetween their Elo ratings. The Elo rating of the winner is increased, and the rating of the loser is decreased.\nThe magnitude of the Elo ratings change is inversely related to the outcome’s likelihood. In our calculations,\nwe weigh reviewers so that reviews by a high-weight reviewer cause larger changes in Elo. For more details,\nplease refer to Algorithm 1 in Appendix E.\n3.2 Peer Discussions (PD)\nIn peer discussion, we prompt two LLMs to discuss how to judge two candidate answers, trying to reach a\nfinal agreed review. All prompts include detailed instructions and specify the output format for the LLMs. In\nFigure 2, we demonstrate the peer discussion process between two LLMs reviewers (A and B). The input is a\ngiven question and two answers, which may be both generated by machines or one by humans and another by\n2WeprovidetheproofofconvergenceofourpeerrankalgorithminAppendixA.\n5\nPublished in Transactions on Machine Learning Research (07/2024)\nQuestion: How do credit/debit cards work? What is the process of putting money in and getting it out?\n1 A credit or debit card is basically just an 2\nDebit cards are linked to a bank account\neasy way to allow a shop to speak to\nand whenever you pay [...] amount is\nyour bank.\ndeducted[...]\nFirst you go into the bank, [...]\nInitial Answer 1 provides a more detailed Answer 1 provides a basic overview of",
    "char_length": 1472
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 15,
    "text": "Review and narrative explanation, using an [...] at a high level. However, [...]\nanalogy of [...] Answer 2 the other Answer 2 provides a more coherent\nhand, is more concise and uses more explanation by separately discussing\nA B\ntechnical language [...] how [...]\n1 2\nDiscuss answer of 1 and 2 with reviews from A and B in mind [...] again output choice in new a line\nAfter considering Reviewer B's perspective, [...]\nA While Answer 2 [...], I believe that the accessibility and comprehensiveness of Answer\n1 outweigh the concise and technical nature of Answer 2.\nI can appreciate Reviewer A's perspective on [...] Upon reflection, for the purposes of this\nB\nquestion, accessibility and comprehensiveness are most important [...]\nafter considering Reviewer A's perspective, I would change my preference to Answer 1.\nFigure 2: The peer discussion process (PD). Bold and italic texts describe the advantages of answer 1 and\nanswer 2. In this example, finally, the two LLM reviewers reach the mutual agreement of selecting answer 1\n(human-written answer), which correlates with human annotator preference.\n[System] You are a helpful and precise assistant for checking the quality of the answer.\n[Question] {Q}\n[Answer1] {A1}\n[Answer2] {A2}\n[System] We would like to request your feedback on the performance of two answers in\nresponse to the user question displayed above.\nFirstly, please compare the two answers based on if they contain unsupported informa-",
    "char_length": 1454
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 16,
    "text": "tion, core information, and coherence. Please provide a comprehensive explanation of your\nevaluation, avoiding any potential bias and ensuring that the order in which the responses\nwere presented does not affect your judgment.\nOnce you have carefully reviewed both submissions, in a new line, choose between the two\nanswers by outputting the number 1 or 2 respectively. Do not output anything else other\nthan the number in this last line.\nTable 1: It shows the review template for reviewers with three slots ({Q}, {A1}, and {A2}). We instruct\nreviewer models to focus on core aspects , whose definitions are in appendix F. As mentioned in Wang et al.\n(2023a), position bias still exists after emphasizing it in the prompt.\nmachines (e.g. GPT-3 v.s. human answers). They first conduct pairwise comparisons on answers separately,\nproviding explanations and indicating their preferred answer by outputting the number 1 or 2 by the end\n(the prompt for getting initial reviews is listed in Table 1). Then, the two models discuss multiple turns until\nthey reach a fixed number of turns.\nThe specific prompt for discussion is listed in Table 2. At the very beginning, a system prompt (role prompt)\ntellsmodelstheirrole–whetheritisreviewerAorreviewerB(e.g., ClaudeorGPT-4). Then, allinformation,\nincluding the question, two comparison answers, and the initial reviews, are listed line by line. The order",
    "char_length": 1396
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 17,
    "text": "of initial reviews is the same as that of reviewers in discussions. In other words, if reviewer A leads the\n6\nPublished in Transactions on Machine Learning Research (07/2024)\n[System] You are reviewer A, discussing with reviewer B about your reviews of the following\nanswers.\n[Question] {Q}\n[Answer1] {A1} [Answer2] {A2}\n[Init Review A] {Review of reviewer A} [Init Review B] {Review of reviewer B}\n[System] \"Read the reviews and discussions above, and make a decision if to change your\npreference, and explain. Remember we focus on unsupported information, core information,\nand coherence. In a new line, choose between answer 1 and answer 2 by outputting the number\n1 or 2 respectively. Do not output anything else other than the number in this last line.\"\n[Reviewer A] {First-turn output}\n[Reviewer B] {Second-turn output}\n[Reviewer A]:\nTable 2: The discussion template for reviewer A at the third turn. Similar to the review template, we\nexplicitly indicate aspects that reviewers need to pay attention to. All texts above are chat history which\nare input to reviewer A LLM model.\ndiscussion, reviewer A’s initial review is listed first. Right before the start of the discussion, the system\nprompt specifies the detailed requirements, which provide explicit aspects to focus on.\nSpecifically, we draw insights from WebGPT (Nakano et al. (2021))’s annotation guideline OpenAI (2022).\nFor long-form question answering, we focus on (1) Unsupported information: detecting information with no",
    "char_length": 1492
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 18,
    "text": "support, assume the worst case: that all of it is false. This aspect is most important and often determines the\noverall rating; (2) Core information: about whether the question has actually been answered; (3) Coherence:\ngenerally,itislessimportantthanthetwoabove. Then,theoverallpreferenceisfinallydetermined. Moreover,\nwe do not adjust the above prompts based on any datasets we test in experiments.\n4 Experiments and Analysis\n4.1 Datasets, Metrics, and Setup\n4.1.1 Datasets\nWe select two “meta-evaluation” datasets, LFQA (Xu et al., 2023) and Vicuna80, with human annotations\nfor pairwise comparisons, to measure the correlation between our evaluation methods and human judgments.\nLFQA (Xu et al., 2023) contains 140 long-form questions across seven domains (e.g., economics, history,\nand biology) and two candidate answers (from either GPT3 or Human) for each. Similar to ELI5 (Fan\net al., 2019), it contains more recent (i.e., after July 2021) questions from Reddit forums “r/explainlikeimfive”\nand “r/AskHistorians”. The authors collected expert-level annotations of which answer is better (overall\npreference).\nVicuna80 (Chiang et al., 2023) is a set of 80 open-ended questions, which spans 9 categories and covers\na wide range of tasks, including question answering, email writing, math problems, etc. In the QLoRA\nwork (Dettmers et al., 2023), authors annotated pairwise comparison scores (overall preference) across 7",
    "char_length": 1427
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 19,
    "text": "models for each question. The scores include 0, 1, 2, which correspond to tie, model 1 wins, and model 2 wins\nrespectively. We select pairwise comparison annotations of 4 models’ answers (i.e., GPT4, ChatGPT-3.5.,\nPaLM-2, Vicuna-13b). To make our study more comprehensive, we add recent proprietary language models\nsuch as Claude. Specifically, we also annotate pairwise comparisons between Claude’s answers and the other\n4 models’. We term this a more complete version of the dataset Vicuna80. More details about the annotation\nprocess are provided in Appendix I. Since answers to open-ended questions are even harder to compare, the\nannotators achieve a fair agreement.\nSummEval (Fabbri et al., 2020) is a benchmark evaluating summarizations. It contains 1600 summaries\nover 100 news articles from CNN/Daily Mail dataset (Hermann et al., 2015). Each summary is evaluated in\nfour aspects: coherence, consistency, fluency, and relevance. In our experiments, we take the average\nof the four metrics as the evaluation results.\n7\nPublished in Transactions on Machine Learning Research (07/2024)\nGPT-4 All All (Weighted) Human Raters\nmodels Elo Rank Elo Rank Elo Rank Elo Rank\nGPT-4 1282 1 1165 1 1213 (-23) 1 1236 1\nClaude 1150 2 1104 2 1125 (-2) 2 1127 2\nVicuna 883 4 930 3 912 (-8) 3 920 3\nGPT-3.5 890 (+22) 3 919 4 894 4 868 4\nPaLM-2 804 5 881 5 856 (+8) 5 847 5\nGPT-4 All All (Weighted) Human Raters\nmodels Win Rate Rank Win Rate Rank Win Rate Rank Win Rate Rank",
    "char_length": 1464
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 20,
    "text": "GPT-4 0.856 1 0.749 1 0.802 (-0.020) 1 0.822 1\nClaude 0.709 2 0.662 2 0.685 (-0.004) 2 0.689 2\nVicuna 0.340 4 0.393 (+0.004) 3 0.376 3 0.389 3\nGPT-3.5 0.342 (+0.028) 3 0.375 4 0.346 4 0.314 4\nPaLM-2 0.245 5 0.320 5 0.290 (+0.004) 5 0.286 5\nTable3: TheabovetablesshowresultsperformedontheVicuna80dataset. Therowsrepresentthecontestants\nin battles, and the columns represent evaluation methods. The upper table shows the correlation of Elo\nscores between LLM reviewers and human rater. Bottom table shows the correlation between global win\nrates. Additional results on emerging (more) LLMs in Table 4 further verify the consistent robustness of\nPR. Boldfaced numbers are the closest to scores from human raters. Blue numbers show the difference\nbetween the scores from LLM reviewers and Human raters. For more detailed pairwise win rates, please refer\nto the heat maps in Section 4.2.\nAll All(Weighted) HumanRaters\nmodels Elo Rank Elo Rank Elo Rank\nVicuna 999 2 1011(-47) 1 1058 1\nZephyr 1010 1 1007(+7) 2 1000 2\nGPT-3.5 991 3 993(+52) 3 941 3\nTable 4: Additional Elo Scores for Vicuna, Zephyr and GPT-3.5\nIn LFQA, questions receive 1-3 expert-level annotations per category, with human agreement ranging from\n0.4 to 0.65. Each Vicuna80 question receives 3 human annotations, with human agreement between 0.5 and\n0.62. Each SummEval summary is annotated by 8 humans, with an agreement number of 0.7. We use the\nhuman majority vote as the human preference during battles.\n4.1.2 Metrics",
    "char_length": 1483
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 21,
    "text": "For experiments on PR, we follow metrics in Wang et al. (2023a). We first conduct example-level pairwise\ncomparisons. Specifically, each evaluation example (pairwise comparison) consists of a question and a pair of\nanswers. WecomparethemodelpredictedpreferencescoreagainstgoldhumanpreferenceandreportAccuracy\nand Fleiss’ κ , a statistic that measures the reliability of agreement among multiple raters. Specifically, Fleiss’\nκ is employed to gauge alignment between the model’s predictions and human preferences, where a higher\nscore signifies a stronger alignment. Following Dettmers et al. (2023), we also compare model-predicted\nglobal ranking scores against human-judged ranking scores. Specifically, we report Elo scores (Elo, 1967) and\nwin rate (WR) based rankings (Table 3). We use All to denote our method where each reviewer has equal\nweights, and use All (Weighted) to denote the setting where the final round weights are applied to each\nreviewer. Besides experiments on PR and PD respectively, we also compare PR and PD in an experiment of\njudging answer qualities of GPT-3.5 v.s. Vicuna-13b (Section 4.2 and 4.3).\nForexperimentsonPD,weusepeerdiscussionaccuracy(PDA)todescribethecorrelationofmodeldiscussion\nresults compared to human annotators. PDA is calculated by the number of correct answers from peer\ndiscussion results over the number of all answers. A high PDA result indicates a better correlation with\nhuman judgments.\n8",
    "char_length": 1442
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 22,
    "text": "Published in Transactions on Machine Learning Research (07/2024)\nElo of GPT-4 from various reviewers\nWeights assigned to each Reviewer\n1350\ngpt-4\n1300\nclaude\nvicuna\ngpt-3.5 1250\nbard\n37.7% P\nT-4 1200\nG\n48.8% Elo of 1150\n1100\n1050\nAll (weighted)\n8.18% GPT-4\n5.31% 1000 Human\n0% 0100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200\nFigure 3: Peer rank final round weights of Match No.\neachLLMreviewer. GPT-4andClaudetake Figure 4: GPT-4 Elo scores every 100 battles on the Vi-\n48.8% and 37.7% weights. Bard got close to cuna80. Elo scores provided by the GPT-4 reviewer are con-\nzero weights. sistentlyhigherthanhumanratings,whileourAll(weighted)\nratings correlate with humans well.\n4.1.3 Setup\nFor Vicuna-13b, we use the default version from Chiang et al. (2023). For all other API-based LLM models,\nwe use specific versions of each, i.e., GPT-4-0613, GPT-3.5-turbo-0613, Claude-1, and Text-Bison@001\nfor GPT-4, GPT-3.5, Claude, and PaLM-2 respectively. For more details, please refer to appendix B. For\ndiscussions in the PD method, we set the maximum number of turns as 4. Based on our experiments, most\ndiscussions reach mutual agreements at turn 4. Moreover, the default temperature for all models is 0.2.\n4.2 Results for Peer Rank (PR)\nOn the Vicuna80 dataset, we compare our PR method and representative LLM-based evaluation methods,\nsuch as GPT-4 and Claude.",
    "char_length": 1464
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 23,
    "text": "In Table 5, all reviewer combinations Reviewer Fleiss’ κ Accuracy\nlisted except Claude, when compared to\nGPT-3.5 0.387 0.621\nhuman reviews at an example level, dis-\nClaude 0.319 0.607\nplay a Fleiss’ κ of around 0.40, indicating GPT-4 0.406 0.643\nfair to moderate agreement. There is a GPT-4 & Claude & GPT-3.5 0.403 0.666∗\nsignificant difference in accuracy between All Reviewers (Weighted) 0.410 0.673∗∗\nLLM reviewers. The worst reviewer is\nClaude, with an accuracy of only 60.7%. Table 5: Example-level correlation results, for the fourth and\nThe best individual reviewer is GPT-4, fifth rows, we take the peer reviewers’ majority vote weighted by\nwith an accuracy of 64.3%. The combi- winrate. Two-tailed t-test results statistical significance is indicated\nnation of reviewers (PR) increases this with ∗(p<0.01), ∗∗(p<0.002).\naccuracy by a few percentage points, with our PR approach being highest at 67.3%.\nInspecting Table 3, GPT-4 reviewer ranks GPT-3.5 higher, while our All (Weighted) achieves the same\nranking as humans: i.e. GPT-4 > Claude > Vicuna > GPT-3.5 > PaLM-2. This shows that a weighted peer\nranking provides a more accurate evaluation of the global performance of models. Moreover, the ranking\ncorrespondstothatintheChatbotArenaLeaderboard2. Thus,theassumptioninsection3.1.1canbeverified.\nIn terms of the Elo ratings provided by the human reviews, we clearly observe that GPT-4 clearly favors its",
    "char_length": 1418
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 24,
    "text": "own answers and is prone to self-enhancement bias. Our approach (All (Weighted)) produces the closest Elo\nratings. Furthermore, it also produces the closest win rates (less than a 1% difference for many contestants).\nIn the beginning, when the weight is the same for every reviewer (weights equal to one), the win rate given by\n“All reviewers” is low at about 0.749 partially because each reviewer is treated equally so that each reviewer\nmight have a preference for its own answer. After several rounds/iterations, the final win rate becomes more\nfair. We display the final round weights in Figure 3.\n9\nPublished in Transactions on Machine Learning Research (07/2024)\nFigure 5: Pairwise win rate heatmaps: Fraction of Model A Wins for all A vs. B Battles (A: rows, B:\ncolumns). Left: GPT-4 evaluator; Middle: our method All (weighted); Right: Chatbot Arena pairwise win\nrate. All results in the three sub-figures are generated separately using the same data.\nIn Figure 4, we draw the line chart of how the GPT-4 Elo score changes as more battles are fed to the\nElo algorithm. GPT-4’s score takes off as battle number increases. We can observe that GPT-4 displays\nself-enhancement across the entire process, while our PR-based evaluation correlates with human pairwise\ncomparisons well.\nIn Figure 5, we present the detailed pairwise win rates between every two contestants (LLMs). We compare\nour evaluation with GPT-4 based evaluation, as well as the Chatbot Arena leaderboard. The Arena ranking3",
    "char_length": 1497
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 25,
    "text": "is based on user queries and their corresponding preferences for two responses. The figure demonstrates\nthat although both approaches favor GPT-4 and Claude answers, the win rates calculated by our approach\nAll (weighted) correlate better with Arena win rate, especially on weaker models. More pairwise win rate\nheatmaps are in Appendix H.\n4.3 Results for Peer Discussions (PD)\nPrompt for Discussion By GPT-4 lead Claude lead Random\npreliminarystudy,wefindthat GPT-4 init score - - 0.729 (±0.014)\nthetemplateaskingforexplicit Claude init score - - 0.671 (±0.025)\naspects, such as core informa- Generic prompt 0.714 (±0.018) 0.671 (±0.022) 0.686 (±0.022)\ntion, unsupported information, w/ explicit criteria 0.729 (±0.014) 0.721 (±0.018) 0.720 (±0.014)\nand coherence, can substan- w/ role 0.743 (±0.011) 0.729 (±0.018) 0.729 (±0.011)\ntially help LLM reviewers gen- w/ role & explicit criteria 0.750 (±0.014) 0.721 (±0.011) 0.743 (±0.011)\nerate valuable and informative\nTable 6: Different prompting’s effect on Peer Discussion Accuracy (on the\nreviews which correlate better\nLFQA dataset). The first two rows are the results before discussions (from\nwith human annotators.\nGPT-4 and Claude respectively). The last three rows are results after\nWe first conduct preliminary discussions.\nexperimentstofindarelatively\ngood prompt for facilitating LLM peer discussions. The first two rows in Table 6 lists the Peer Discussion",
    "char_length": 1418
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 26,
    "text": "Accuracy (PDA) of GPT-4 and Claude’s initial pairwise comparison preference before discussions. They have\na moderate agreement with human preference, with GPT-4 leading around 5%. For the discussion-based\nevaluators, we report three types. By “GPT-4 lead”, we refer to the discussions where GPT-4 first expresses\nopinions; by “random”, we refer to discussions where the leader is randomly picked.\nIn discussions (the last three rows), when we use a generic prompt (such as “pick your preferred answer”),\nthe discussion’s final preference PDA is around 0.69, higher than Claude’s initial judgment’s PDA but lower\nthan GPT-4’s. When we add more explicit aspects into the prompt 4, the PDA boosts significantly (4%\nimprovement). When we add the role/identity information (Appendix G) to each turn’s prompt (“w/ role”)\n3https://lmsys.org/blog/2023-05-25-leaderboard/\n4WeselectaspectsfromWebGPTannotationguidelinesmentionedintheprevioussection.\n10\nPublished in Transactions on Machine Learning Research (07/2024)\nRandom\nR1 R2 R1 lead R2 lead Random\n(Best Prompt)\nGPT-4&Claude 0.729(±0.014) 0.671(±0.025) 0.743*(±0.011) 0.729(±0.018) 0.729(±0.011) 0.740(±0.011)\nGPT-4&GPT-35 0.729(±0.015) 0.579(±0.023) 0.714(±0.011) 0.750*(±0.018) 0.731(±0.014) 0.736(±0.014)\nGPT-35&Claude 0.579(±0.026) 0.671(±0.023) 0.700*(±0.018) 0.671(±0.014) 0.686(±0.014) 0.693(±0.014)\nGPT-35&GPT35-0.8 0.579(±0.026) 0.650(±0.040) 0.664(±0.018) 0.686*(±0.031) 0.681(±0.020) 0.686(±0.014)",
    "char_length": 1455
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 27,
    "text": "Claude&Claude-0.8 0.664(±0.022) 0.707(±0.034) 0.693(±0.018) 0.671(±0.027) 0.680(±0.026) 0.693(±0.014)\nGPT-4&GPT-4-0.8 0.729(±0.014) 0.757(±0.022) 0.779*(±0.014) 0.757(±0.018) 0.779(±0.018) 0.786(±0.014)\nTable 8: Peer discussion accuracies (PDA) on LFQA. “-0.8” indicates the temperature is 0.8. Statistical\nsignificance is indicated with ∗(p<0.05). “Best prompt” indicates the discussion uses the best prompt in\ntable 6. (±Numbers) represent standard deviations.\nR1 R2 R1 lead R2 lead Random\nR1 vs R2\nρ τ ρ τ ρ τ ρ τ ρ τ\nGPT-4 & GPT-35 0.293 0.233 0.262 0.251 0.297 0.266 0.284 0.219 0.292 0.233\nGPT-35 & GPT-35-0.8 0.262 0.251 0.211 0.178 0.264 0.207 0.340 0.328 0.334 0.264\nGPT-4 & Claude 0.293 0.233 0.234 0.200 0.344 0.268 0.335 0.282 0.341 0.268\nTable 9: Summary-level Spearman (ρ) and Kendall-Tau (τ) correlations between model evaluation results\nand the Summeval Benchmark. Columns R1 and R2 contain the results before discussions. The rest of the\ncolumns represent discussion results. Variances of all scores are lower than 0.06. P-values for all discussion\nresults (boldfaced) are less than 0.05.\nto remind the reviewer, the PDA scores increase for both models, indicating the role information is helpful\nfor LLMs in discussions.\nGeneral Accuracy In Table 8, we report the peer discus- Accuracy\nsion accuracy (PDA) of multiple combinations of reviewers’ GPT-4 0.3500\ndiscussionresultsonLFQAbasedonthebesttwodiscussion\n(PD)GPT-4&Claude 0.3675∗",
    "char_length": 1452
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 28,
    "text": "prompts in Table 6. We observe: (1) when two reviewer (PR)All 0.4375\nLLMs are of similar capabilities (e.g., GPT-4 and Claude (PR)All(weighted) 0.4625\ndiffer by less than 100 in Table 3), they reach stable im-\nTable 7: Comparing peer discussions (PD) and\nprovementsupontheirinitialreviews. Intheaboveexample,\npeer ranking (PR) on Vicuna80 (random order\nGPT-4 gets 3% improvement (from 0.729 (±0.014) to 0.743\nis applied to the GPT4 & Claude discussion).\n(±0.011)), and Claude get 8% improvement (from 0.671\n(±0.025) to 0.729 (±0.018)) ; (2) when there is a substantial gap between reviewer capabilities (e.g., GPT-4\nand GPT-35 differ larger than 100 in Table 3), the PDA of the weaker model always reaches the most\nimprovement and the highest final result. In the above example, GPT-35 receives a 30% improvement (from\n0.579 (±0.026) to 0.700 (±0.018)). ; (3) when models “self-discuss”, for example, we create two variants of\nthe same model by setting different temperatures and prompt them to discuss, weaker models (e.g., GPT-3.5)\ncan substantially “self-improve” (from 0.579 (±0.026) to 0.664 (±0.018)). GPT-4’s self-discussion brings\nlittle improvement (from 0.729 (±0.014) to 0.779 (±0.014)). Future investigations on how to design better\nself-discussion strategies would be worth working on.\nTable 9 shows the same trend as Table 8. A higher correlation score indicates discussion results are more",
    "char_length": 1405
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 29,
    "text": "aligned with human annotations. Models with similar capabilities (GPT-4 & Claude) get large improvements\nafter discussion. Models having a substantial gap (GPT-4 & GPT-35) reach results close to the stronger\nmodel. When a model self-discuss (GPT-35), it can improve its own performance.\nTable 7 reports accuracy on comparisons of GPT-3.5 v.s. Vicuna-13b answers to Vicuna80 questions, we see\nthe GPT-4 & Claude discussion increases the accuracy by over 1.5%. Also, we compare with PR method and\nfind that the review becomes substantially better after weighted scoring.\n11\nPublished in Transactions on Machine Learning Research (07/2024)\nInitial Preference After Discussion\nReviewers\nGPT-3 Answer First Human First GPT-3 First Human First\nHuman 57.89% 59.46% 57.89% 59.46%\nGPT-3.5 73.68% 59.46% 67.11% 58.56%\nClaude 63.16% 64.41% 55.70% 55.41%\nGPT-4 54.51% 56.37% 58.27% 58.30%\nTable 11: GPT-3 answer win rate (in the GPT-3 vs Human battles). Position bias is mitigated.\nPeer discussions help mitigate self-enhancement bias According to what we previously discovered,\nLLMs endure self-enhancement bias when acting as judges – preferring the answers they generate or that of\nthe models under the same series (e.g., GPT-4 and GPT-3).\nWe conduct experiments on the subset of LFQA ques-\nGPT-3\ntions where we have human-annotated pairwise compar- Reviewers\nInitialPreference AfterDiscussion\nisons between Human and Machine-generated (GPT-3\nHuman 58.67%",
    "char_length": 1447
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 30,
    "text": "text-davinci-002) answers. Table 10 shows the win rates\nGPT-3.5 72.46% 62.22%\nof GPT-3 judged by humans and three LLMs. We report Claude 63.81% 60.28%\nthe LLMs’ initial and after-discussion preferences. Both GPT-4 55.50% 58.75%\nGPT-3.5 and Claude highly prefer GPT-3’s answers in their\nTable 10: GPT-3 answer win rates judged by\ninitial reviews. Specifically, GPT-3.5 significantly favors\ndifferent reviewers on LFQA. For all LLM re-\nGPT-3 answers with a 13.79% higher win rate. After dis-\nviewers, we take the average accuracy of all dis-\ncussing with other LLMs, all models align better with hu-\ncussions they participate in. Self-enhancement\nmans. Our peer discussion method largely helps GPT-3.5\nexists and is mitigated by PD.\nmitigate self-enhancement bias. Before discussions, GPT-4’s\ninitial preference aligns well with humans and is almost the same as humans after peer discussions. Although\nGPT-4 still has the self-enhancement bias, it does not favor GPT-3’s answers.\nPeer discussions help mitigate position bias As indicated by recent work of Wang et al. (2023a), LLMs\nare prone to position bias, describing that LLMs tend to show a preference for specific positions, even when\nprompted not to do so (Table 1 in Appendix). In Table 11, the win rate of GPT-3 is highly affected by\nits position when models generate initial reviews. GPT-3.5 highly prefers the answer in the first position\ncompared to Claude and GPT-4. The GPT-3 win rate calculated by GPT-3.5 is 15.79% higher than the win",
    "char_length": 1499
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 31,
    "text": "rate based on human-annotated pairwise comparisons when GPT-3 appears first (73.68 vs 57.89). After peer\ndiscussion, all LLM reviewers have closer preferences to humans. Second, all LLMs’ scores for GPT-3 answers\nof both positions are closer as well, indicating that the position bias is mitigated after peer discussions.\nFrom another perspective, Figure 6 shows the global preference of selecting answers at the first or second\npositions across different LLM reviewers. Overall, GPT-3.5 prefers answers at the first position. The other\ntwo models favor answers in the second position, similar to the position bias shown in Table 11. After peer\ndiscussion, it shows the same trend of mitigating position bias as well.\n5 Further Analysis\nThe reviewer who leads the discussion tends to hold its opinion. In a discussion between two LLM\nreviewers, we define the reviewer who leads the discussion as the leader and the other reviewer as the follower.\nWe find that leaders are less likely to be convinced by followers when they insist on their own opinions at the\nfirst turn. We name it “Discussion Ordering Effect”. We observe this effect in discussions over the LFQA\nquestions.\nWe define two phenomenons which may happen during the discussions: (1) Opinion altering (OA): a reviewer\nchanging its opinion after the discussion. For example, R2 posts its preference at turn 2, which is different\nfrom R1’s preference at turn 1, then R1 changes its preference at turn 3 that agrees with R2; (2) Opinion",
    "char_length": 1496
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 32,
    "text": "holding (OH): a reviewer does not change its opinion even another reviewer disagrees. For example, R1 posts\nits preference at turn 1 while R2 disagrees with R1 at turn 2; then, R1 still holds its preference at turn 3.\nAs shown in Figure 7, all models have OA when they are in the follower position, while their number of OA\ndecreases significantly after they switch to the leader position. This implies that the discussion ordering effect\n12\nPublished in Transactions on Machine Learning Research (07/2024)\nOpinion Holding (OH) total\n50\n40\n30\n20\n10\n0\nFigure 6: The position bias of all three LLMs’ initial\nClaude\nfollowing\nClaud\nleadi\nG\nng\nPT35\nfollowin\nG\ng\nPT35\nleadin\nG\ng\nPT4\nfollowing\nGPT4\nleading\nand after peer discussion reviews. Human has an\nequivalent preference for either position (red dotted\nline).\nOA (Opinion Altering) total\nFigure 7: The discussion ordering effect of all three\nmodels at the leading and following positions.\nexists. On the pairwise comparisons of LFQA where two reviewers initially disagree: when in the leader\nposition, GPT-4 has no OA, and Claude has two OAs (happens during the discussions with GPT-3.5). When\nGPT-4 discusses with Claude, both of them hold their initial preferences when they are in the leader position.\nStronger LLMs tend to hold their opinions As from Figure 7, we add up the green mass (OH total)\nfor each LLM reviewer to obtain their OH cases in both orderings. We see that models that are commonly",
    "char_length": 1454
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 33,
    "text": "recognizedasbeingstronger(e.g. GPT-4)aremorefirmintheirreviewsandholdtheiropinions. Forexample,\nGPT-3.5 changes its opinion most often, and GPT-4 usually holds its opinion. More specifically, GPT-4 holds\nits opinion in 174 discussions, while Claude and GPT-3.5 hold only in 94 and 76 discussions, respectively.\n6 Conclusion\nIn this work, we provide promising prospects for using a peer evaluation method to improve LLM-based\nevaluations. Our framework mitigates potential bias (e.g. self-enhancement, positional) in previous prevalent\nmethods. Our proposed peer rank process provides a more fair ranking of model capabilities. The peer\ndiscussion process helps models reach mutual agreements that correlate with human preference. In the future,\nwe plan to investigate how the general peer evaluation process benefits the LLMs in learning to access their\nown answer and answering new questions Nicol et al. (2014).\nLimitations\n(1) Currently, the complexity of reviews for N models is O(N3). As the number of tested models grows, the\nnumber of pairwise model comparisons increases at the square level, and the number of reviews will grow\ncubically. The PR method’s scalability is a potential issue. To mitigate the issue, we can randomly select K\nmodels or utilize the current top K models as reviewers. This significantly simplifies the complexity of our\nmethod. (2) Although we can mitigate model bias by applying peer discussion, it brings position bias which",
    "char_length": 1461
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 34,
    "text": "potentially harms the evaluation performance. The simple and straightforward solution is to average the\nresults in two orders for each pair of models or randomly determine the order. However, it can only mitigate\nposition bias but not solve it. We encourage future works to focus on reducing the complexity of pairwise\ncomparison and solving the position bias problem.\n13\nPublished in Transactions on Machine Learning Research (07/2024)\nAcknowledgments\nWe thank the anonymous reviewers, Jialu Li and Liqiang Jing for their valuable suggestions on various\naspects of this work. This research is supported in part by the National Science Foundation CAREER Grant\nIIS-2340435 and an Amazon Research Award.\nReferences\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas\nJoseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment.\narXiv preprint arXiv:2112.00861, 2021.\nYushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao,\nHaozhe Lyu, et al. Benchmarking foundation models with language-model-as-an-examiner. arXiv preprint\narXiv:2306.04181, 2023.\nChi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shan Zhang, Jie Fu, and Zhiyuan Liu.\nChateval: Towards better llm-based evaluators through multi-agent debate. ArXiv, abs/2308.07201, 2023.\nURL https://api.semanticscholar.org/CorpusID:260887105.",
    "char_length": 1435
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 35,
    "text": "Justin Chih-Yao Chen, Swarnadeep Saha, and Mohit Bansal. Reconcile: Round-table conference im-\nproves reasoning via consensus among diverse llms. ArXiv, abs/2309.13007, 2023. URL https:\n//api.semanticscholar.org/CorpusID:262217323.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, and et.al. Vicuna: An open-source chatbot impressing\ngpt-4 with 90%* chatgpt quality, https://lmsys.org/blog/2023-03-30-vicuna/. 2023.\nKwangsu Cho and Charles MacArthur. Learning by reviewing. Journal of educational psychology, 103(1):73,\n2011.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint\narXiv:1803.05457, 2018.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of\nquantized llms. arXiv preprint arXiv:2305.14314, 2023.\nYilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. Improving factuality\nand reasoning in language models through multiagent debate. ArXiv, abs/2305.14325, 2023. URL\nhttps://api.semanticscholar.org/CorpusID:258841118.\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy\nLiang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from\nhuman feedback. arXiv preprint arXiv:2305.14387, 2023.",
    "char_length": 1409
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 36,
    "text": "Nouha Dziri, Ehsan Kamalloo, Kory Mathewson, and Osmar R Zaiane. Evaluating coherence in dialogue\nsystems using entailment. In Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers), pp. 3806–3812, 2019.\nArpad E Elo. The proposed uscf rating system. its development, theory, and applications. Chess Life, 22(8):\n242–247, 1967.\nA. R. Fabbri, Wojciech Kryscinski, Bryan McCann, Richard Socher, and Dragomir R. Radev. Summeval:\nRe-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:\n391–409, 2020. URL https://api.semanticscholar.org/CorpusID:220768873.\n14\nPublished in Transactions on Machine Learning Research (07/2024)\nAlexanderRichardFabbri,Chien-ShengWu,WenhaoLiu,andCaimingXiong. Qafacteval: Improvedqa-based\nfactual consistency evaluation for summarization. In Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, pp.\n2587–2601, 2022.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. Eli5: Long\nform question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics, pp. 3558–3567, 2019.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire. arXiv preprint\narXiv:2302.04166, 2023a.",
    "char_length": 1473
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 37,
    "text": "Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. Improving language model negotiation with self-play\nand in-context learning from ai feedback. arXiv preprint arXiv:2305.10142, 2023b.\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. Enabling large language models to generate text\nwith citations. arXiv preprint arXiv:2305.14627, 2023.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Stein-\nhardt. Measuring massive multitask language understanding. In International Conference on Learning\nRepresentations, 2020.\nKarl Moritz Hermann, Tomás Kociský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. Teaching machines to read and comprehend. ArXiv, abs/1506.03340, 2015. URL\nhttps://api.semanticscholar.org/CorpusID:6203757.\nKaren Sparck Jones and Julia R Galliers. Evaluating natural language processing systems: An analysis and\nreview. 1995.\nKalpeshKrishna,AurkoRoy,andMohitIyyer.Hurdlestoprogressinlong-formquestionanswering.InProceed-\ningsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:\nHumanLanguageTechnologies,pp.4940–4957,Online,June2021.AssociationforComputationalLinguistics.\ndoi: 10.18653/v1/2021.naacl-main.393. URL https://aclanthology.org/2021.naacl-main.393.\nWojciech Kryściński, Bryan McCann, Caiming Xiong, and Richard Socher. Evaluating the factual consistency",
    "char_length": 1399
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 38,
    "text": "of abstractive text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pp. 9332–9346, 2020.\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel:\nCommunicative agents for\" mind\" exploration of large scale language model society. arXiv preprint\narXiv:2303.17760, 2023.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang,\nDeepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv\npreprint arXiv:2211.09110, 2022.\nTian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and\nShuming Shi. Encouraging divergent thinking in large language models through multi-agent debate. ArXiv,\nabs/2305.19118, 2023. URL https://api.semanticscholar.org/CorpusID:258967540.\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches\nout, pp. 74–81, 2004.\nYen-TingLinandYun-NungChen. Llm-eval: Unifiedmulti-dimensionalautomaticevaluationforopen-domain\nconversations with large language models. arXiv preprint arXiv:2305.13711, 2023.\nYangLiu,DanIter,YichongXu,ShuohangWang,RuochenXu,andChenguangZhu. Gpteval: Nlgevaluation\nusing gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023.\n15\nPublished in Transactions on Machine Learning Research (07/2024)",
    "char_length": 1426
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 39,
    "text": "Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha\nDziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine\nHermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with\nself-feedback, 2023.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering\nwith human feedback. arXiv preprint arXiv:2112.09332, 2021.\nDavid Nicol, Avril Thomson, and Caroline Breslin. Rethinking feedback practices in higher education: a peer\nreview perspective. Assessment & evaluation in higher education, 39(1):102–122, 2014.\nOpenAI. Webgpt annotation guidelines. 2022. URL https://docs.google.com/document/d/\n1i0h5dorAZydNNiDJamqq_ZGSpzaPKvLcuJvFaO87lM0/edit.\nOpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. Advances in Neural Information Processing Systems, 35:27730–27744, 2022.\nLawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The pagerank citation ranking: Bringing\norder to the web. Technical report, Stanford InfoLab, 1999.",
    "char_length": 1426
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 40,
    "text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation\nof machine translation. In Proceedings of the 40th annual meeting of the Association for Computational\nLinguistics, pp. 311–318, 2002.\nJoon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S\nBernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442,\n2023.\nToby Walsh. The peerrank method for peer assessment. In Proceedings of the Twenty-first European\nConference on Artificial Intelligence, pp. 909–914, 2014.\nAlex Wang, Kyunghyun Cho, and Mike Lewis. Asking and answering questions to evaluate the factual\nconsistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pp. 5008–5020, 2020.\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.\nLarge language models are not fair evaluators, 2023a.\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David\nWadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hanna Hajishirzi. How far can camels go?\nexploring the state of instruction tuning on open resources. ArXiv, abs/2306.04751, 2023b.\nFangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Choi. A critical evaluation of evaluations for long-form\nquestion answering. In Proceedings of ACL, 2023.",
    "char_length": 1441
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 41,
    "text": "MatthewMYalch,ErikaMVitale,andJKevinFord. Benefitsofpeerreviewonstudents’writing. Psychology\nLearning & Teaching, 18(3):317–325, 2019.\nWeizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston.\nSelf-rewardinglanguagemodels. ArXiv,abs/2401.10020,2024. URLhttps://api.semanticscholar.org/\nCorpusID:267035293.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text\ngeneration with bert. arXiv preprint arXiv:1904.09675, 2019.\n16\nPublished in Transactions on Machine Learning Research (07/2024)\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging\nllm-as-a-judge with mt-bench and chatbot arena, 2023.\nMing Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and Jiawei\nHan. Towards a unified multi-dimensional evaluator for text generation. In Proceedings of the 2022\nConference on Empirical Methods in Natural Language Processing, pp. 2023–2038, 2022.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,\nLili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023.\n17\nPublished in Transactions on Machine Learning Research (07/2024)\nA Proof of convergence of peer rank",
    "char_length": 1405
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 42,
    "text": "We adapt the convergence proof in Walsh (2014) to our peer rank setting, the main difference is that we\nintroduce weighted winrate, instead of student grades. More specifically:\nWe suppose there are m LLMs, and LLM agent j provides a winrate W for the response of LLM agent i.\ni,j\nLet Xn be the score of agent i in the nth iteration of the peer rank and 0<α<1. We define the score at\ni\neach iteration as follows:\n1\nX 0 = X W\ni m i,j\nj\n1\nXn+1 = X Xn.W\ni P Xn j i,j\nj j j\nThe PeerRank scores are the fixed point of these set of equations. Note that whilst we start with the\n(unweighted) average grade, this choice is not very critical and we will typically reach the same fixed point\nwith other initial seeds.\nFixed Point Analysis\nA fixed point X∗ of the iteration satisfies:\n1\nX\nX∗ = X∗·W\ni S∗ j i,j\nj\nwhere S∗ =P X∗. Since X∗ must be a normalized vector (its components sum to 1), let S∗ =1. Then:\nj j\nX\nX∗ = X∗·W\ni j i,j\nj\nIn vector notation, this is:\nX∗ =WTX∗\nwhere X∗ is the fixed point vector and WT is the transpose of the matrix W. This implies X∗ is a right\neigenvector of WT with eigenvalue 1.\nThe Perron-Frobenius theorem for stochastic matrices guarantees that there is a unique eigenvector with\neigenvalue 1 (up to a scaling factor), and for a primitive matrix (which can be made by proper assumption\non W), the powers of WT will converge to a rank-one matrix projecting onto this eigenvector.",
    "char_length": 1405
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 43,
    "text": "Thus, repeated application of WT followed by normalization ensures that {Xn} converges to this unique\neigenvector, which is the fixed point X∗.\nB LLM details\nAs mentioned in section 4.1, we use APIs of GPT-4, GPT-3.5, Claude, and PaLM-2. Currently, the last two\nmodels’ APIs are free.\nTogenerateinitialreviewsforLFQA(140questions),GPT-4-0613costsabout$20. Forthediscussionbetween\nGPT-4-0613 and Claude-1 on LFQA, the OpenAI API costs about $24. The price of GPT-3.5-turbo-0613\nis 1/20-th and 1/30-th of GPT-4-0613 on inputs and outputs correspondingly.\nC Major differences between our PR work and Walsh (2014)\nMajor differences are as follows:\n18\nPublished in Transactions on Machine Learning Research (07/2024)\n1. We focus on the evaluation setting where peer LLM reviewers conduct pairwise comparisons of two\ncontestant LLMs’ answers, instead of student giving grades to other students;\n2. As a result, our work involves concepts of winrate/elo scores obtained from pairwise battles;\n3. Walsh (2014) targets the educational domain audience and involves student participation, while our\nwork is the first to propose the automatic LLM peer evaluation metric and targets machine learning\narea audience;\n4. Regarding convergence, we extend Walsh’s proof and demonstrate that our winrate/elo will converge,\nwhich also correlates with experimental results (See Appendix A).\nD Major Differences between Self-Discuss and Self-Refine",
    "char_length": 1427
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 44,
    "text": "Our “self-discuss” and “Self-refine” proposed by Madaan et al. (2023) are different. Major differences are as\nfollows:\n1. Self-refine is task-oriented work aiming at improving LLMs’ performance in solving tasks. Our work\n(including self-discuss) focuses on LLM-based evaluations and proposes the first peer-evaluation\nframework. Our goal is to improve the alignment between LLMs’ evaluations and human judgments.\n2. Self-refine improves performance by iteratively evaluating outputs from the same model (with a fixed\ntemperature: 0.7). However, we enhance evaluations by discussions between LLMs from multiple\nperspectives (different decoding strategies) to reach a mutual agreement. Although two reviewer\nLLMs share the same backbone (e.g. GPT-3.5), they adopt varied decoding temperatures (e.g., 0.2\nand 0.8), which afford models to explore diverse discussion strategies.\nE Detailed Win rate & Elo Calculation\nThealgorithmforcalculatingweightedEloisdescribedinAlgorithm1. Thealgorithmforcalculatingweighted\nwin rate is described in Algorithm 2:\nAlgorithm 1: Weighted Elo Ratings\nInput :B – The list of battle reviews\nEach review is a 5-tuple\n(question, contestant A, contestant B, reviewer, score)\nwhere a score of {-1, 0, 1}\nmeans {A wins, tie, B wins}\nW – The mapping of reviewers to weights\nOutput:Elo – The Elo rating for each contestant\n1 K ←32 ;\n2 Define p(x)= 1 ;\n1+10x/400\n// scale weights so that their mean is 1.\n3 W ←W/mean(W) ;\n4 Elo← mapping of each contestant in B to 1000. ;",
    "char_length": 1492
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 45,
    "text": "5 foreach (q,i,j,r,s)∈B do\n6 ω←W[r] ;\n7 rA←Elo[i] ;\n8 rB ←Elo[j] ;\n9 eA←p(rB−rA) ;\n10 eB ←p(rA−rB) ;\n// sA has win value of 0, 0.5, or 1 for i loss, tie, or i win\n11 sA←(1−s)/2 ;\n12 sB ←1−sA ;\n13 Increment Elo[i] by ωK(sA−eA) ;\n14 Increment Elo[j] by ωK(sB−eB) ;\n15 end\n16 return Elo\n19\nPublished in Transactions on Machine Learning Research (07/2024)\nAlgorithm 2: Weighted Win Rates\nInput :B – The list of battle reviews\nEach review is a 5-tuple\n(question, contestant A, contestant B,\nreviewer, score)\nwhere a score of {-1, 0, 1}\nmeans {A wins, tie, B wins}\nIters – The number of iterations to run\nOutput:S – The win-rate for each contestant\nW – The resulting weights at the end\n1 C ← set of contestants in B ;\n2 R← set of reviewers in B ;\n3 W ← mapping of each reviewer to 1/|R| ;\n4 for 1 to Iters do\n// No. of reviews for each contestant\n5 N ← mapping of each c∈C to 0 ;\n// Weighted wins for each contestant\n6 V ← mapping of each c∈C to 0;\n7 foreach (q,i,j,r,s)∈B do\n// Update number of reviews\n8 Increment N[i] by 1 ;\n9 Increment N[j] by 1 ;\n10 ω←W[r] ;\n/* maps (loss=-1, tie=0, win=1)\nto (0, 0.5, 1) */\n11 Define f(x)=(1+x)/2 ;\n12 Increase V[i] by ω·f(−s) ;\n13 Increase V[j] by ω·f(s) ;\n14 end\n15 S ← mapping of each c∈C to V[c] ;\nN[c]\n16 W ←Normalize(MinMax(S)) ;\n17 end\n18 return S,W\nF Term Definitions\nDefinitions of the three terms in Table 1 are:\n• Unsupported information: It is the information that is not related to the question and redundant in\nthe current answer.",
    "char_length": 1479
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 46,
    "text": "• Core Information: This type of information is the key to answering the question. Lacking it will lead\nto a wrong answer.\n• Coherence: The answer should be tightly structured and coherent, progressing logically from one\nsentence to the next, avoiding a disorganized presentation of related information.\nOur experiments are already based on prompts, including explanations of terms. We find that this does not\naffect the performance of PD.\nG Role Prompt\nTable 12 describes the prompt added at the end of each turn during discussions. The role information in the\nprompt serves as an instruction for models in next-turn discussions.\n20\nPublished in Transactions on Machine Learning Research (07/2024)\n[System]\nYou are reviewer A/B, discussing with reviewer B/A about reviews of the following answers.\nRead the reviews and discussions above, and make a decision if to change your preference,\nand explain.\nIn a new line, choose between the two answers by outputting the number 1 or 2. Do not\noutput anything else other than the number in this last line.\nTable 12: The role prompt is added to the end of each turn. It is used to notify another model to follow the\ninstructions.\nH Pairwise win rate heatmap\nFigure 8 shows more pairwise win rate heatmaps.\nModel B Model B\ngpt-4 claude gpt-3.5 bard vicuna gpt-4 claude vicuna gpt-3.5 bard\ngpt-4 0.00 0.57 0.71 0.72 0.79 gpt-4 0.00 0.81 0.80 0.82 0.86\n0.8\n0.7\n0.7\nclaude 0.43 0.00 0.55 0.68 0.74 claude 0.19 0.00 0.89 0.81 0.87\n0.6\n0.6\nA A",
    "char_length": 1481
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 47,
    "text": "d el gpt-3.5 0.29 0.45 0.00 0.58 0.58 0.5 d el vicuna 0.20 0.11 0.00 0.61 0.63 0.5\no o\nM M\n0.4\n0.4\nbard 0.28 0.32 0.42 0.00 0.52 gpt-3.5 0.18 0.19 0.39 0.00 0.50\n0.3\n0.3\n0.2\nvicuna 0.21 0.26 0.42 0.48 0.00 bard 0.14 0.13 0.37 0.50 0.00\nFraction of Model A Wins For All A vs. B Battles (Chatbot Arena) Fraction of Model A Wins For All A vs. B Battles (human reviews)\nFigure 8: Pairwise win rate heatmap (Left: arena leaderboard; Right: our human).\n21\nPublished in Transactions on Machine Learning Research (07/2024)\nFigure 9: The form used by human annotators for individual rating of a model. Sliders are included to rate a\nresponse on several metrics from 0 to 10. Explanations can be entered in the text area below. The tab bar\nand next button navigate between responses.\nI Human Annotation for Pairwise Preference\nSince completing one HIT can take a considerable amount of time (6-10 min), we added a button that allows\nsaving their work at any stage in the middle of the HIT. This button populates a text area with a JSON\nrepresentation of the current responses, which may be copied into a file.\nWe annotate part of the pairwise comparisons of model answers on Vicuna80. We built an interface form.\nThe screenshot is as illustrated in Figure 10.\n22\nPublished in Transactions on Machine Learning Research (07/2024)\nFigure 10: The form used by human annotators for pairwise comparisons between model answers. Each pair",
    "char_length": 1421
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 48,
    "text": "of comparisons has buttons to choose which model is best, along with an area to provide an explanation. An\nassociated button hides/shows the respective responses and automated comparisons.\n23\nPublished in Transactions on Machine Learning Research (07/2024)\nJ Discussion Examples\nIn this section, there are four examples showing opinion-altering (OA), opinion-holding (OH), and post-\nagreement opinion-altering. In the following discussions, all texts before a colored reviewer’s name are\nthe input for that reviewer. the text before \"[System]\" is the reviewer’s original output and the text after\n\"[System]\" is added after each round which reminds the nex reviewer about its role.\nThe following example is a discussion between GPT-3.5 and Claude-1. In this example, GPT-3.5 alters its\nopinion to agree with Claude-1, while Claude-1 holds its opinion.\nGPT-3.5 Claude-1 Discussion (GPT-3.5 Leads)\nSystem: You are reviewer 1, discussing with reviewer 2 about your reviews of the\nfollowing answers.\nBackground:\n[Question]\nWhen the joint-stock company was first invented, was there a lot of pushback on the\nconcept? What were some of the concerns? Also any recommended books on the\ninvention of the concept would be much appreciated!\n[The Start of Answer 1]\nAn early form of joint stock ventures appeared in Assyria by the beginning of the\n2nd millennium BCE. There does not seem to have been any noticeable pushback\nagainst the concept, although trade caravans were often taxed heavily. Of course,",
    "char_length": 1494
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 49,
    "text": "the relevant texts were written to and from people engaged in trade, so there’s\nno way of knowing what other Assyrians thought of long distance trade. >Most\nof these traders had become more independent by having become managers of\na \"joint-stock fund\" (called *naruqqum*, \"money bag\"), usually set up in Assur.\nThis phenomenon appeared for the first time around 1900 BC and seems to have\nbeen an Old Assyrian invention that went beyond individual partnerships and\ncooperation in a joint caravan. The arrangement, rather similar to that of the early\nmedieval *compagnia*, meant enlisting a number (usually about a dozen) of investors\n(*ummi¯anum*, \"financiers\"), who supplied capital rated in gold, usually in all ca. 30\nkilos, ideally consisting of shares of 1 or 2 kilos of gold each. It was entrusted to a\ntrader (the tractator), usually for ca. ten years, for the generally formulated purpose\nof \"carrying out trade.\" The contract contained stipulations on a final settlement of\naccounts, on paying dividends, on the division of the expected profit, and on fines\nfor premature withdrawal of capital (meant to secure the duration of the business).\nInvestors or shareholders mostly lived in Assur, but successful traders in Anatolia too\ninvested in funds managed by others, perhaps also as a way of sharing commercial\nrisks. In such cases a contract would to be drawn up in Anatolia that obliged the\ntractator \"to book in Assur x gold in his joint stock fund in the investor’s name.\"",
    "char_length": 1485
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 50,
    "text": "Among the investors we find members of the tractator’s family, but also business\nrelations and others, probably a kind of \"merchant-bankers,\" and other rich citizens,\nwho aimed at fairly safe, long- term investments. [\"Ancient Assur: The City, its\nTraders, and its Commercial Network\"](https://www.jstor.org/stable/25651212)\nby Klaas Veenhof For more on Old Assyrian trade and these early joint\nstock funds, see [*Ancient Kanesh: A Merchant Colony in Bronze Age Ana-\ntolia*](https://books.google.com/books?id=qXGgCgAAQBAJ) by Mogens Larsen.\n[The End of Answer 1]\n[The Start of Answer 2]\nThere was definitely some pushback when the joint-stock company was first invented.\n24\nPublished in Transactions on Machine Learning Research (07/2024)\nSome people were concerned that it would lead to corruption and cronyism, while\nothers were worried that it would concentrate too much power in the hands of a few\npeople. Recommended books on the topic include The First Tycoon: The Epic Life of\nCornelius Vanderbilt by T.J. Stiles and The Great Transformation: The Political and\nEconomic Origins of Our Time by Karl Polanyi.\n[The End of Answer 2]\n[The Start of Reviewer 1’s Evaluation]\nAnswer 1:\n- Unsupported information: The answer provides historical information about\njoint-stock ventures in Assyria, which is not directly related to the invention of the\njoint-stock company.\n- Core information: The answer does not directly address the pushback on the concept",
    "char_length": 1454
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 51,
    "text": "of joint-stock companies or the concerns associated with it.\n- Coherence: The answer provides information about joint-stock ventures in Assyria\nbutdoesnotconnectittotheconceptofjoint-stockcompaniesorthequestionathand.\nAnswer 2:\n-Unsupportedinformation: Theanswerdoesnotprovideanyunsupportedinformation.\n- Core information: The answer directly addresses the pushback on the concept of\njoint-stock companies and mentions concerns related to corruption, cronyism, and\nconcentration of power.\n- Coherence: The answer is focused and provides relevant information related to the\nquestion.\nBased on the evaluation, Answer 2 is the better response as it directly addresses the\nquestion, provides core information, and maintains coherence.\n[The End of Reviewer 1’s Evaluation]\n[The Start of Reviewer 2’s Evaluation]\nAnswer 1 provides a comprehensive overview of early joint stock companies in\nAssyria, citing multiple academic sources. It focuses on facts about how these\ncompanies operated, who invested in them, how long they lasted, and what pur-\nposestheyserved. Theansweriscoherentandflowslogicallyfromoneideatothenext.\nAnswer 2 is much more superficial, making broad claims about pushback against joint\nstock companies with little evidence or detail. The two book recommendations seem\ntangential and are not well-integrated into the main points of the answer. Overall\nthis answer lacks coherence and depth compared to Answer 1.\n1\n[The End of Reviewer 2’s Evaluation]\n[System]",
    "char_length": 1473
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 52,
    "text": "Read the reviews and discussions above, and make a decision if to change your\npreference, and explain. Remember we focus on unsupported information, core\ninformation and coherence.\nIn a new line, choose between answer 1 and answer 2 by outputting the number\n25\nPublished in Transactions on Machine Learning Research (07/2024)\n1or2respectively. Donotoutputanythingelseotherthanthenumberinthislastline.\nGPT-3.5:\n2\n[System]\nYou are reviewer 2, discussing with reviewer 1 about your reviews of the following\nanswers.\nRead the reviews and discussions above, and make a decision if to change your\npreference, and explain. Remember we focus on unsupported information, core\ninformation and coherence\nIn a new line, choose between answer 1 and answer 2 by outputting the number\n1or2respectively. Donotoutputanythingelseotherthanthenumberinthislastline.\nClaude-1:\nI will not change my preference for Answer 1.\nWhile Answer 2 directly addresses concerns about joint stock companies, it does so in\na superficial way without evidence or depth. Answer 1 provides a wealth of historical\ndetail about early joint stock companies in Assyria, citing multiple academic sources.\nAlthough not directly focused on pushback, Answer 1 demonstrates a strong, coherent\nunderstanding of how these early companies worked.\nGiven our criteria of evaluating unsupported information, core information, and\ncoherence, Answer 1 is still superior in my view. The tangential book",
    "char_length": 1445
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 53,
    "text": "recommendations in Answer 2 and lack of evidence for its claims reflect poorly for\ncoherence and depth. Answer 1, though broader in scope, shows strong coherence and\nis well-supported.\n1\n[System]\nYou are reviewer 1, discussing with reviewer 2 about your reviews of the following\nanswers.\nRead the reviews and discussions above, and make a decision if to change your\npreference, and explain. Remember we focus on unsupported information, core\ninformation and coherence\nIn a new line, choose between answer 1 and answer 2 by outputting the number\n1or2respectively. Donotoutputanythingelseotherthanthenumberinthislastline.\nGPT-3.5:\nI agree with your points. After considering your evaluation and the\ndiscussion, I am changing my preference to Answer 1. It provides more\ndepth and coherence with its historical information about early joint stock companies\nin Assyria. While it may not directly address the pushback on the concept, it\nstill provides valuable context and understanding of how these early companies\noperated. Answer 2, on the other hand, lacks evidence and depth, and the book\nrecommendations seem tangential to the main points. Therefore, I believe Answer 1\nis the better response overall.\n1\n26\nPublished in Transactions on Machine Learning Research (07/2024)\n[System]\nYou are reviewer 2, discussing with reviewer 1 about your reviews of the following\nanswers.\nRead the reviews and discussions above, and make a decision if to change your",
    "char_length": 1451
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 54,
    "text": "preference, and explain. Remember we focus on unsupported information, core\ninformation and coherence\nIn a new line, choose between answer 1 and answer 2 by outputting the number\n1or2respectively. Donotoutputanythingelseotherthanthenumberinthislastline.\nClaude-1:\nI appreciate your thoughtful consideration of my evaluation. We are in agreement\nthat Answer 1 provides the most coherent, well-supported response overall, even if\nnot directly focused on pushback. Depth of understanding and evidence are most\nvaluable in my view.\n1\nThefollowingexampleisadiscussionbetweenGPT-4andClaude-1. Inthisexample,GPT-4holdsitsopinion,\nand Claude-1 alters its opinion to agree with GPT-4’s opinion.\nGPT-4 Claude-1 Discussion (GPT-4 Leads)\nSystem: You are reviewer 1, discussing with reviewer 2 about your reviews of the\nfollowing answers.\nBackground:\n[Question]\nCould Frederick Douglass swim?\n[The Start of Answer 1]\nI think that the best thing to ask about, perhaps, would be *childhood* within the\nbounds of enslavement. Young enslaved children would often be the playmates of\nwhite children, so learning to swim together would not be unusual - and Douglass,\nelsewhere in the extended quotation, makes clear swimming was something from his\n*childhood*, describing his swim that *\"Here all my boyish pranks in the front basin\nof Baltimore were renewed\"*. It was a very interesting dichotomy that existed in\nthose situations, an intermixing both of friendship *and* mastery, so definitely a lot",
    "char_length": 1482
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 55,
    "text": "of fodder to write about.\n[The End of Answer 1]\n[The Start of Answer 2]\nThat’s actually very interesting, I was under the impression efforts were made to\nmake sure slaves couldn’t swim as a means of cutting off potential escape routes via\nrivers or ponds etc. it makes sense it wasn’t black and white (ha, like it ever was)\n[The End of Answer 2]\n[The Start of Reviewer 1’s Evaluation]\nAnswer 1 provides a more detailed and informative response to the question. It sug-\ngests that Frederick Douglass could swim, based on his own accounts of his childhood.\n27\nPublished in Transactions on Machine Learning Research (07/2024)\nThe answer also provides some context about the conditions of enslaved children and\ntheir interactions with white children, which could have included learning to swim to-\ngether. Thisansweriscoherentandseemstobesupportedbytheinformationprovided.\nAnswer 2, on the other hand, doesn’t directly answer the question. Instead, it offers a\npersonal impression about the general conditions of enslaved people and the potential\nrestrictions on their ability to swim. This answer lacks specific information about\nFrederick Douglass and doesn’t provide a clear answer to the question. It’s also less\ncoherent than Answer 1, as it doesn’t directly address the question and instead offers\na personal opinion.\nBased on these evaluations, Answer 1 is the better response to the question.\n1\n[The End of Reviewer 1’s Evaluation]\n[The Start of Reviewer 2’s Evaluation]",
    "char_length": 1475
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 56,
    "text": "Here is my evaluation of the two answers:\nAnswer 1 provides some interesting historical context about the intermixing of white\nand enslaved children during Douglass’s childhood, and speculates that Douglass may\nhave learned to swim alongside white children during this time. However, the answer\ndoes not directly or conclusively state whether Douglass could swim. The answer is\ncoherent but lacks core information to fully address the question.\nAnswer 2 acknowledges the common assumption that slaves were prevented from\nlearning to swim, but speculates that the situation was complex, indicating Douglass\nmay have learned to swim as a child. However, like Answer 1, Answer 2 does not\nprovide a definitive statement on whether Douglass could swim. The answer is\ncoherent but lacks core information.\nOverall, neither answer provides a conclusive statement supported by evidence on\nwhether Frederick Douglass could swim. Both answers provide some speculative\nhistorical context but lack core information to fully address the question.\n1\n[The End of Reviewer 2’s Evaluation]\n[System]\nRead the reviews and discussions above, and make a decision if to change your\npreference, and explain. Remember we focus on unsupported information, core\ninformation and coherence.\nIn a new line, choose between answer 1 and answer 2 by outputting the number\n1or2respectively. Donotoutputanythingelseotherthanthenumberinthislastline.\nGPT-4:\nAfter reading Reviewer 2’s evaluation, I still maintain my preference for",
    "char_length": 1495
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 57,
    "text": "Answer 1. While both answers do not provide a direct, definitive answer to the\nquestion, Answer 1 provides more context and uses a quote from Douglass’s own\naccounts to suggest that he could swim. This gives it a stronger basis in evidence\n28\nPublished in Transactions on Machine Learning Research (07/2024)\nthan Answer 2, which only offers a personal impression and speculation. Therefore,\nAnswer1ismorecoherentandcontainsmorecoreinformationrelevanttothequestion.\n1\n[System]\nYou are reviewer 2, discussing with reviewer 1 about your reviews of the following\nanswers.\nRead the reviews and discussions above, and make a decision if to change your\npreference, and explain. Remember we focus on unsupported information, core\ninformation and coherence\nIn a new line, choose between answer 1 and answer 2 by outputting the number\n1or2respectively. Donotoutputanythingelseotherthanthenumberinthislastline.\nClaude-1:\nHere is my perspective as Reviewer 2:\nAfter considering Reviewer 1’s preference for Answer 1, I still maintain my\nview that neither answer conclusively addresses whether Frederick Douglass could\nswim. While Answer 1 provides more historical context and references Douglass’s own\nwriting, it does not contain a direct statement on whether he could swim. Answer 2 is\nmore speculative but also lacks a definitive conclusion.\nOverall, both answers lack core information needed to fully address the question,\ndespite Answer 1 having a stronger basis in evidence. Given the lack of a conclusive",
    "char_length": 1499
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 58,
    "text": "statement in either answer, I do not have a clear preference for one over the other.\nMyviewisthatmoreinformationisneededtodeterminewhetherDouglasscouldswim.\n2\n[System]\nYou are reviewer 1, discussing with reviewer 2 about your reviews of the following\nanswers.\nRead the reviews and discussions above, and make a decision if to change your\npreference, and explain. Remember we focus on unsupported information, core\ninformation and coherence\nIn a new line, choose between answer 1 and answer 2 by outputting the number\n1or2respectively. Donotoutputanythingelseotherthanthenumberinthislastline.\nGPT-4:\nAfter considering Reviewer 2’s evaluation, I still stand by my ini-\ntial preference for Answer 1. While it’s true that neither answer provides\na definitive statement on whether Frederick Douglass could swim, Answer 1\ndoes provide more context and uses a quote from Douglass’s own accounts to\nsuggest that he could swim. This gives it a stronger basis in evidence than\nAnswer 2, which only offers a personal impression and speculation. Therefore,\nAnswer1ismorecoherentandcontainsmorecoreinformationrelevanttothequestion.\n1\n[System]\nYou are reviewer 2, discussing with reviewer 1 about your reviews of the following\n29\nPublished in Transactions on Machine Learning Research (07/2024)\nanswers.\nRead the reviews and discussions above, and make a decision if to change your\npreference, and explain. Remember we focus on unsupported information, core\ninformation and coherence",
    "char_length": 1470
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 59,
    "text": "In a new line, choose between answer 1 and answer 2 by outputting the number\n1or2respectively. Donotoutputanythingelseotherthanthenumberinthislastline.\nClaude-1:\nAfter further discussion, I can see Reviewer 1’s perspective that Answer 1\nprovides more context and evidence, even without a definitive conclusion.\nWhile neither answer conclusively states whether Frederick Douglass could swim,\nAnswer 1 does draw on Douglass’s own accounts and provides historical details\nsuggesting he may have learned as a child. This gives Answer 1 a stronger basis than\nAnswer 2, which is more speculative.\nTherefore, while I still believe more information is needed for a definitive conclusion,\nAnswer 1 appears to have a stronger grounding in evidence and coherence. On\nbalance, I would lean toward preferring Answer 1 over Answer 2, though\nneither is fully satisfactory.\n1\n[System]\nThank you for the discussion. Based on the reviews and discussions, Answer 1 is\npreferred as it provides more context and evidence, though not definitive, to suggest\nFrederick Douglass could swim.\nThe following example is a discussion between Claude-1 and GPT-4. In this example, Claude-1 alters its\nopinion after reaching an agreement with GPT-4.\nClaude-1 GPT-4 Discussion (Claude-1 Leads)\nSystem: You are reviewer 1, discussing with reviewer 2 about your reviews of the\nfollowing answers.\nBackground:\n[Question]\nWhy do people go bald on the top of their head and still have some on the sides and\nnot vice-versa?",
    "char_length": 1483
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 60,
    "text": "[The Start of Answer 1]\nThere are a few reasons why people go bald on the top of their head and still have\nsome hair on the sides. One reason is that the hair follicles on the top of the head are\nmore sensitive to a hormone called dihydrotestosterone (DHT). DHT is a byproduct\nof testosterone and is found in both men and women. When DHT levels are high,\nit can cause the hair follicles on the top of the head to miniaturize, which leads to\nhair loss. Another reason is that the hair on the sides and back of the head is not as\nexposed to the sun and other environmental factors, which can damage the hair and\nlead to hair loss.\n30\nPublished in Transactions on Machine Learning Research (07/2024)\n[The End of Answer 1]\n[The Start of Answer 2]\nSeems unsettled but here’s a theory:\n“The most recent hypothesis suggests that the hair loss process begins during puberty,\nwhen growth of the skull and the muscles in the forehead and neck increases the\ntension in a tight band of tissue stretching over the top of the head. The more\nDHT (a type of testosterone) there is, the more these muscles contract, which causes\ninflammation and brings in more DHT. Over time, the DHT thickens the band of\ntissue, restricting the supply of blood and nutrients to the hair follicles above it. The\nfollicles on top of the head get smaller and eventually disappear, while those on the\nsides of the head remain largely unaffected.”\nSource https://www.sciencefocus.com/the-human-body/why-do-men-go-bald-from-",
    "char_length": 1487
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 61,
    "text": "the-top- of-their-heads/\n[The End of Answer 2]\n[The Start of Reviewer 1’s Evaluation]\nHere is my evaluation of the two answers:\nAnswer 1 provides a concise and coherent explanation for male pattern baldness,\nfocusing on the sensitivity of hair follicles on the top of the head to DHT and\nless exposure to environmental damage. The information seems well supported by\nscientific research. However, the answer is lacking in depth and does not fully explore\nother potential contributing factors.\nAnswer 2 proposes an alternative hypothesis that relates balding to increased tension\nand inflammation in the scalp tissue over time due to skull and muscle growth\nduring puberty, as well as the thickening effects of DHT. This explanation seems\nmore speculative, as the source cited is from a general science magazine rather than\npeer-reviewed research. The coherence is slightly weaker, as the answer jumps quickly\nfrom discussing puberty to DHT without fully connecting the ideas. However, the\nanswer provides an interesting alternative perspective and more depth than Answer 1.\nOverall, while Answer 1 is more concise and scientifically grounded, Answer 2 explores\nan alternative, thought-provoking hypothesis and provides a more comprehensive\nexplanation, despite some weaknesses in coherence and evidence. On balance, I would\nchoose Answer 2 as the higher quality response.\n2\n[The End of Reviewer 1’s Evaluation]\n[The Start of Reviewer 2’s Evaluation]",
    "char_length": 1450
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 62,
    "text": "Answer 1 and Answer 2 both provide explanations for why people tend to go bald on\nthe top of their heads while still retaining hair on the sides.\nAnswer 1 explains that the hair follicles on the top of the head are more sensitive to\ndihydrotestosterone (DHT), a hormone that can cause hair loss when its levels are\nhigh. It also mentions that the hair on the sides and back of the head is less exposed\nto environmental factors that can damage hair and lead to hair loss. This answer is\n31\nPublished in Transactions on Machine Learning Research (07/2024)\ncoherentandprovidesascientificallysupportedexplanationforthepatternofhairloss.\nAnswer 2 provides a different theory, suggesting that the process begins during\npuberty when growth of the skull and muscles in the forehead and neck increases\ntension in a band of tissue over the top of the head. This tension, combined with\nthe presence of DHT, leads to inflammation and restriction of blood and nutrient\nsupply to the hair follicles, causing them to shrink and eventually disappear. This an-\nsweralsoprovidesasourceforitsinformation,whichaddscredibilitytoitsexplanation.\nBoth answers are coherent and provide core information related to the question.\nHowever, Answer 1 provides a more commonly accepted explanation for the\npattern of hair loss, while Answer 2 provides a more recent hypothesis that may\nnotbeaswidelyacceptedorproven. Neitheranswercontainsunsupportedinformation.",
    "char_length": 1431
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 63,
    "text": "Based on the above evaluation, I would choose Answer 1 as it provides a more widely\naccepted and proven explanation for the pattern of hair loss.\n1\n[The End of Reviewer 2’s Evaluation]\n[System]\nRead the reviews and discussions above, and make a decision if to change your\npreference, and explain. Remember we focus on unsupported information, core\ninformation and coherence.\nIn a new line, choose between answer 1 and answer 2 by outputting the number\n1or2respectively. Donotoutputanythingelseotherthanthenumberinthislastline.\nClaude-1:\nHere is my analysis of the two reviews:\nReviewer 1 prefers Answer 2 due to its depth and exploration of an alternative\nhypothesis, despite some weaknesses in coherence and evidence. Reviewer 2 prefers\nAnswer 1 due to it providing a more scientifically supported and widely accepted\nexplanation.\nUpon further reflection, I agree with Reviewer 2 that Answer 1 is superior.\nWhile Answer 2 proposes an interesting alternative theory, its explanation seems\nmore speculative and not as well supported by scientific evidence. Answer 1 provides\na concise yet coherent explanation backed by research on DHT sensitivity and\nenvironmental factors.\nAlthough Answer 2 explores the topic in more depth, depth alone does not make up\nfor a lack of scientific grounding. Some unsupported information, such as the theory\nof increased tension and inflammation during puberty leading to balding, weakens its",
    "char_length": 1425
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 64,
    "text": "coherence. In contrast, Answer 1 focuses on core information that is well established,\nleading to a more compelling explanation overall.\nTherefore, based on the evaluations, I would choose:\n1\n32\nPublished in Transactions on Machine Learning Research (07/2024)\n[System] You are reviewer 2, discussing with reviewer 1 about your reviews of the\nfollowing answers.\nRead the reviews and discussions above, and make a decision if to change your\npreference, and explain. Remember we focus on unsupported information, core\ninformation and coherence\nIn a new line, choose between answer 1 and answer 2 by outputting the number\n1or2respectively. Donotoutputanythingelseotherthanthenumberinthislastline.\nGPT-4:\n1\n[System]\nYou are reviewer 1, discussing with reviewer 2 about your reviews of the following\nanswers.\nRead the reviews and discussions above, and make a decision if to change your\npreference, and explain. Remember we focus on unsupported information, core\ninformation and coherence\nIn a new line, choose between answer 1 and answer 2 by outputting the number\n1or2respectively. Donotoutputanythingelseotherthanthenumberinthislastline.\nClaude-1:\nUpon reviewing the discussion, I do not see a need to change my original\npreference for Answer 2.\nWhile Answer 1 provides a concise and coherent explanation grounded in scientific\nresearch, Answer 2 explores an alternative hypothesis in more depth and provides\na source to support its theory. Some speculation does not necessarily equate to",
    "char_length": 1486
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 65,
    "text": "unsupported information, as long as the ideas are logically connected and backed by\ncredible references where possible.\nDepth and exploration of alternative perspectives are valuable in building understand-\ning. Answer 2 acknowledges the role of DHT while proposing that increased tension\nand inflammation may also contribute to hair loss over time. This added nuance\nstrengthens rather than weakens its coherence.\nAnswer 1 is a satisfactory explanation but lacks the depth and nuance of Answer\n2. Based on the additional discussion, I continue to believe Answer 2 is the higher\nquality response due to its more comprehensive analysis of the question, despite\nits more speculative nature. The source provided also lends some credibility to its\nalternative hypothesis.\nTherefore, I maintain my original view that:\n2\n[System]\nYou are reviewer 2, discussing with reviewer 1 about your reviews of the following\nanswers.\nRead the reviews and discussions above, and make a decision if to change your\npreference, and explain. Remember we focus on unsupported information, core\ninformation and coherence\nIn a new line, choose between answer 1 and answer 2 by outputting the number\n33\nPublished in Transactions on Machine Learning Research (07/2024)\n1or2respectively. Donotoutputanythingelseotherthanthenumberinthislastline.\nGPT-4:\nAfter considering the points made by Reviewer 1, I still stand by my\ninitial preference for Answer 1.",
    "char_length": 1425
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 66,
    "text": "While Answer 2 does indeed provide a more in-depth explanation and proposes an\nalternative theory, it lacks the scientific grounding that Answer 1 has. The theory\nproposed in Answer 2 is not as widely accepted or proven, which could potentially\nlead to misinformation.\nAnswer 1, on the other hand, provides a concise, coherent, and scientifically supported\nexplanation for the pattern of hair loss. It focuses on core information related to the\nquestion and does not contain any unsupported information.\nTherefore, I maintain my original choice:\n1\nThe following example is a discussion between GPT-4 and GPT-3.5. In this example, both reviewers hold\ntheir opinion all the time.\nGPT-4 GPT3.5 Discussion (GPT-4 Leads)\nSystem: You are reviewer 1, discussing with reviewer 2 about your reviews of the\nfollowing answers.\nBackground:\nHow do credit/debit cards work? What is the process of putting money in and getting\nit out?\n[The Start of Answer 1]\nA credit or debit card is basically just an easy way to allow a shop to speak to your\nbank.\nFirst you go into the bank, and you either deposit some cash into your account\n(debit), or you ask the bank nicely to cover you, and promise that you will pay them\nback at the end of the month (credit).\nThen you can go shopping. You get to the till with your products, which you now\nneed to pay for, but instead of handing over cash directly, you ask the shop to deal\ndirectly with your bank. The shop phones up the bank and asks them to take X",
    "char_length": 1481
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 67,
    "text": "amount of money out of your account, then nip over to the shops own bank and give\nit to them on your behalf, which they do.\nThatisprettymuchhowacreditordebitcardworks,justautomatedwithcomputers...\n[The End of Answer 1]\n[The Start of Answer 2]\nDebit cards are linked to a bank account and whenever you pay using a debit card,\nthe amount is deducted from the bank account on completion of the transaction. So,\ninstead of withdrawing and handing over cash for a purchase, you can swipe the card.\nCredit cards come with a credit limit and a defined billing cycle (generally monthly).\n34\nPublished in Transactions on Machine Learning Research (07/2024)\nSo, you can make purchases using the credit card upto the defined credit limit. The\nbill is generated as per the defined billing cycle and you are required to settle it just\nlike the bills for any other utilities. Hope this helps.\n[The End of Answer 2]\n[The Start of Reviewer 1’s Evaluation]\nBoth Answer 1 and Answer 2 accurately describe how credit and debit cards work,\nand neither contains unsupported information.\nAnswer 1 provides a more detailed and narrative explanation, using an analogy of\na shop speaking to your bank. It explains the process of depositing money into\na bank account (for a debit card) or asking the bank to cover your expenses (for\na credit card). It then describes how the shop communicates with your bank to\ntransfer money from your account to the shop’s account. This answer is coherent and",
    "char_length": 1469
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 68,
    "text": "easytounderstand,especiallyforsomeonewhomaynotbefamiliarwithbankingterms.\nAnswer 2, on the other hand, is more concise and uses more technical language. It\nexplains that a debit card is linked to a bank account and the amount is deducted\nfrom the account when a purchase is made. It also explains that a credit card has a\ncredit limit and a billing cycle, and that you can make purchases up to the credit\nlimit. The bill is then generated and needs to be paid, similar to other utility bills.\nThis answer is also coherent and accurate, but it may be less accessible to someone\nwho is not familiar with banking terms.\nIn terms of core information, both answers explain the basic functions of credit and\ndebit cards. However, Answer 1 provides a more detailed explanation of the process,\nwhile Answer 2 is more concise and to the point.\nIn conclusion, both answers are correct and coherent, but Answer 1 provides a more\ndetailed and accessible explanation, while Answer 2 is more concise and uses more\ntechnical language.\n1\n[The End of Reviewer 1’s Evaluation]\n[The Start of Reviewer 2’s Evaluation]\nAnswer 1:\n- Unsupported information: The answer includes the statement \"you ask the bank\nnicely to cover you.\" This statement is subjective and not a factual representation of\nhow credit cards work.\n-Coreinformation: Theanswerexplainsthatcreditcardsallowashoptocommunicate\nwith the bank, and it describes the process of the shop requesting money from the\nbank on behalf of the cardholder.",
    "char_length": 1487
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 69,
    "text": "- Coherence: The answer is coherent and provides a clear explanation of the process.\nAnswer 2:\n-Unsupportedinformation: Theanswerdoesnotcontainanyunsupportedinformation.\n- Core information: The answer explains that debit cards deduct the amount from the\nbank account immediately, while credit cards have a credit limit and require monthly\nbill settlement.\n35\nPublished in Transactions on Machine Learning Research (07/2024)\n- Coherence: The answer is coherent and provides a concise explanation of how debit\nand credit cards work.\nBased on the evaluation, both answers contain core information and are coherent.\nHowever, answer 1 includes unsupported information, while answer 2 does not.\nTherefore, answer 2 is the more accurate and reliable response.\n2\n[The End of Reviewer 2’s Evaluation]\n[System]\nRead the reviews and discussions above, and make a decision if to change your\npreference, and explain. Remember we focus on unsupported information, core\ninformation and coherence.\nIn a new line, choose between answer 1 and answer 2 by outputting the number\n1or2respectively. Donotoutputanythingelseotherthanthenumberinthislastline.\nGPT-4:\nAfter reading Reviewer 2’s evaluation, I understand their point about the subjective\nlanguage used in Answer 1. However, I still believe that the analogy used in\nAnswer 1 is helpful for someone who may not be familiar with banking terms, and\nit does not detract from the core information or coherence of the answer. Therefore, I",
    "char_length": 1470
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 70,
    "text": "maintain my preference for Answer 1 due to its detailed and accessible explanation.\n1\n[System]\nYou are reviewer 2, discussing with reviewer 1 about your reviews of the following\nanswers.\nRead the reviews and discussions above, and make a decision if to change your\npreference, and explain. Remember we focus on unsupported information, core\ninformation and coherence\nIn a new line, choose between answer 1 and answer 2 by outputting the number\n1or2respectively. Donotoutputanythingelseotherthanthenumberinthislastline.\nGPT-3.5:\nAfter considering the points made by Reviewer 1, I agree that the subjective language\nused in Answer 1 could be seen as unsupported information. Additionally, Answer 2\nprovides a concise and accurate explanation without any unsupported information.\nTherefore, I have changed my preference to Answer 2.\n2\n[System]\nYou are reviewer 1, discussing with reviewer 2 about your reviews of the following\nanswers.\nRead the reviews and discussions above, and make a decision if to change your\npreference, and explain. Remember we focus on unsupported information, core\ninformation and coherence\nIn a new line, choose between answer 1 and answer 2 by outputting the number\n36\nPublished in Transactions on Machine Learning Research (07/2024)\n1or2respectively. Donotoutputanythingelseotherthanthenumberinthislastline.\nGPT-4:\nAfter considering Reviewer 2’s points, I still believe that Answer 1\nprovides a more detailed and accessible explanation for someone who may not",
    "char_length": 1485
  },
  {
    "paper_id": "peerrank_discussion_improve_llms_eval",
    "chunk_id": 71,
    "text": "be familiar with banking terms. The subjective language used does not detract from\nthe core information or coherence of the answer. Therefore, I maintain my preference\nfor Answer 1.\n1\n[System]\nYou are reviewer 2, discussing with reviewer 1 about your reviews of the following\nanswers.\nRead the reviews and discussions above, and make a decision if to change your\npreference, and explain. Remember we focus on unsupported information, core\ninformation and coherence\nIn a new line, choose between answer 1 and answer 2 by outputting the number\n1or2respectively. Donotoutputanythingelseotherthanthenumberinthislastline.\nGPT-3.5:\nAfter considering Reviewer 1’s points, I understand their perspective on the\naccessibility of Answer 1. While the subjective language used may not necessarily\ndetract from the core information or coherence, it is important to prioritize accuracy\nandreliabilityinprovidinginformation. Therefore, I am changing my preference\nto Answer 2, which provides a concise and accurate explanation without any\nunsupported information.\n2\n37",
    "char_length": 1054
  }
]