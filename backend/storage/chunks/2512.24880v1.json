[
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 0,
    "text": "mHC: Manifold-Constrained Hyper-Connections\nZhendaXie*â€ ,YixuanWei*,HuanqiCao*,\nChenggangZhao,ChengqiDeng,JiashiLi,DamaiDai,HuazuoGao,JiangChang,\nLiangZhao,ShangyanZhou,ZheanXu,ZhengyanZhang,WangdingZeng,\nShengdingHu,YuqingWang,JingyangYuan,LeanWang,WenfengLiang\nDeepSeek-AI\nAbstract\nRecently,studiesexemplifiedbyHyper-Connections(HC)haveextendedtheubiquitousresid-\nualconnectionparadigmestablishedoverthepastdecadebyexpandingtheresidualstream\nwidthanddiversifyingconnectivitypatterns. Whileyieldingsubstantialperformancegains,\nthis diversification fundamentally compromises the identity mapping property intrinsic to\ntheresidualconnection,whichcausesseveretraininginstabilityandrestrictedscalability,and\nadditionally incurs notable memory access overhead. To address these challenges, we pro-\npose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects\ntheresidualconnectionspaceofHContoaspecificmanifoldtorestoretheidentitymapping\nproperty, while incorporating rigorous infrastructure optimization to ensure efficiency. Em-\npiricalexperimentsdemonstratethatmHCiseffectivefortrainingatscale, offeringtangible\nperformanceimprovementsandsuperiorscalability. WeanticipatethatmHC,asaflexibleand\npracticalextensionofHC,willcontributetoadeeperunderstandingoftopologicalarchitecture\ndesignandsuggestpromisingdirectionsfortheevolutionoffoundationalmodels.\nx x x\n!\"# !\"# !\"#\nh'(&) h'(&)\n! !\nPost Mapping Post Mapping\nâ„‹\n!\n'(&) ğ’« â„³$%#&(â„‹\n!\n'(&))\nh(,) h(,)\n! !\nLayerâ„± Layer â„± Layer â„±",
    "char_length": 1497
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 1,
    "text": "h$%& h*+ h$%& h*+\n! ! ! !\nRes Mapping Pre Mapping Res Mapping Pre Mapping\nâ„‹\n!\n$%& â„‹\n!\n'$% ğ’« â„³!\"#(â„‹\n!\n$%&) ğ’« â„³$!\"(â„‹\n!\n'$%)\nx\n! x x\n! !\n(a) Residual Connection (b) Hyper-Connections(HC) (c) Manifold-Constrained HC (mHC)\nFigure1 | IllustrationsofResidualConnectionParadigms. Thisfigurecomparesthestructural\ndesignof(a)standardResidualConnection,(b)Hyper-Connections(HC),and(c)ourproposed\nManifold-ConstrainedHyper-Connections(mHC).UnliketheunconstrainedHC,mHCfocuses\non optimizing the residual connection space by projecting the matrices onto a constrained\nmanifoldtoensurestability.\n*Corecontributors. â€ Correspondingauthor: xie.zhenda@deepseek.com\n5202\nceD\n13\n]LC.sc[\n1v08842.2152:viXra\nContents\n1 Introduction 3\n2 RelatedWorks 4\n2.1 MicroDesign . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n2.2 MacroDesign . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n3 Preliminary 5\n3.1 NumericalInstability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n3.2 SystemOverhead . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n4 Method 8\n4.1 Manifold-ConstrainedHyper-Connections . . . . . . . . . . . . . . . . . . . . . . 8\n4.2 ParameterizationandManifoldProjection . . . . . . . . . . . . . . . . . . . . . . . 9\n4.3 EfficientInfrastructureDesign . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9",
    "char_length": 1413
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 2,
    "text": "4.3.1 KernelFusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n4.3.2 Recomputing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n4.3.3 OverlappingCommunicationinDualPipe . . . . . . . . . . . . . . . . . . 11\n5 Experiments 12\n5.1 ExperimentalSetup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n5.2 MainResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n5.3 ScalingExperiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n5.4 StabilityAnalysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n6 ConclusionandOutlook 15\nA Appendix 19\nA.1 DetailedModelSpecificationsandHyper-parameters. . . . . . . . . . . . . . . . . 19\n2\n1. Introduction\nDeepneuralnetworkarchitectureshaveundergonerapidevolutionsincetheintroductionof\nResNets (He et al., 2016a). As illustrated in Fig. 1(a), the structure of a single-layer can be\nformulatedasfollows:\nxğ‘™+1 =xğ‘™ +F(xğ‘™,W ğ‘™ ), (1)\nwhere xğ‘™ and xğ‘™+1 denote the ğ¶-dimensional input and output of the ğ‘™-th layer, respectively,\nand F represents the residual function. Although the residual function F has evolved over\nthepastdecadetoincludevariousoperationssuchasconvolution,attentionmechanisms,and\nfeed forward networks, the paradigm of the residual connection has maintained its original\nform. AccompanyingtheprogressionofTransformer(Vaswanietal.,2017)architecture, this",
    "char_length": 1476
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 3,
    "text": "paradigmhascurrentlyestablisheditselfasafundamentaldesignelementinlargelanguage\nmodels(LLMs)(Brownetal.,2020;Liuetal.,2024b;Touvronetal.,2023).\nThis success is primarily attributed to the concise form of the residual connection. More\nimportantly,earlyresearch(Heetal.,2016b)revealedthattheidentitymappingpropertyofthe\nresidualconnectionmaintainsstabilityandefficiencyduringlarge-scaletraining. Byrecursively\nextendingtheresidualconnectionacrossmultiplelayers,Eq.(1)yields:\nğ¿âˆ’1\nâˆ‘ï¸\nxğ¿ =xğ‘™ + F(xğ‘–,W ğ‘– ), (2)\nğ‘–=ğ‘™\nwhere ğ¿ and ğ‘™ correspond to deeper and shallower layers, respectively. The term identity\nmappingreferstothecomponentxğ‘™ itself,whichemphasizesthepropertythatthesignalfrom\ntheshallowerlayermapsdirectlytothedeeperlayerwithoutanymodification.\nRecently,studiesexemplifiedbyHyper-Connections(HC)(Zhuetal.,2024)haveintroduced\na new dimension to the residual connection and empirically demonstrated its performance\npotential. Thesingle-layerarchitectureofHCisillustratedinFig.1(b). Byexpandingthewidthof\ntheresidualstreamandenhancingconnectioncomplexity,HCsignificantlyincreasestopological\ncomplexitywithoutalteringthecomputationaloverheadofindividualunitsregardingFLOPs.\nFormally,single-layerpropagationinHCisdefinedas:\nxğ‘™+1 =H ğ‘™ resxğ‘™ +H ğ‘™ postâŠ¤ F(H ğ‘™ pre xğ‘™,W ğ‘™ ), (3)\nwherexğ‘™ andxğ‘™+1 denotetheinputandoutputoftheğ‘™-thlayer,respectively. Unliketheformu-\nlation in Eq. (1), the feature dimension of xğ‘™ and xğ‘™+1 is expanded from ğ¶ to ğ‘›Ã—ğ¶, where ğ‘› is",
    "char_length": 1451
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 4,
    "text": "theexpansionrate. ThetermHres âˆˆ Rğ‘›Ã—ğ‘› representsalearnablemappingthatmixesfeatures\nğ‘™\nwithintheresidualstream. Alsoasalearnablemapping,Hpre âˆˆ R1Ã—ğ‘› aggregatesfeaturesfrom\nğ‘™\ntheğ‘›ğ¶-dimstreamintoağ¶-dimlayerinput,andconversely,Hpost âˆˆ R1Ã—ğ‘› mapsthelayeroutput\nğ‘™\nbackontothestream.\nHowever, asthetrainingscaleincreases, HCintroducespotentialrisksofinstability. The\nprimary concern is that the unconstrained nature of HC compromises the identity mapping\nproperty when the architecture extends across multiple layers. In architectures comprising\nmultiple parallel streams, an ideal identity mapping serves as a conservation mechanism. It\nensuresthattheaveragesignalintensityacrossstreamsremainsinvariantduringbothforward\nandbackwardpropagation. RecursivelyextendingHCtomultiplelayersviaEq.(3)yields:\n(cid:32)ğ¿âˆ’ğ‘™ (cid:33) ğ¿âˆ’1 ğ¿âˆ’1âˆ’ğ‘–\nxğ¿ = (cid:214) H ğ¿ r âˆ’ es ğ‘– xğ‘™ + âˆ‘ï¸ (cid:169) (cid:173) (cid:214) H ğ¿ r âˆ’ es ğ‘— (cid:170) (cid:174) H ğ‘– postâŠ¤ F(H ğ‘– pre xğ‘–,W ğ‘– ), (4)\nğ‘–=1 ğ‘–=ğ‘™ ğ‘—=1\n(cid:171) (cid:172)\n3\nwhere ğ¿andğ‘™ representadeeperlayerandashallowerlayer,respectively. IncontrasttoEq.(2),\nthecompositemapping\n(cid:206)ğ¿âˆ’ğ‘™Hres\ninHCfailstopreservetheglobalmeanofthefeatures. This\nğ‘–=1 ğ¿âˆ’ğ‘–\ndiscrepancy leads to unbounded signal amplification or attenuation, resulting in instability\nduringlarge-scaletraining. Afurtherconsiderationisthat,whileHCpreservescomputational\nefficiencyintermsofFLOPs,thehardwareefficiencyconcerningmemoryaccesscostsforthe",
    "char_length": 1428
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 5,
    "text": "widenedresidualstreamremainsunaddressedintheoriginaldesign. Thesefactorscollectively\nrestrictthepracticalscalabilityofHCandhinderitsapplicationinlarge-scaletraining.\nToaddressthesechallenges,weproposeManifold-ConstrainedHyper-Connections(mHC),\nasshowninFig.1(c),ageneralframeworkthatprojectstheresidualconnectionspaceofHC\nontoaspecificmanifoldtorestoretheidentitymappingproperty,whileincorporatingrigorous\ninfrastructureoptimizationtoensureefficiency. Specifically,mHCutilizestheSinkhorn-Knopp\nalgorithm(SinkhornandKnopp,1967)toentropicallyprojectHres ontotheBirkhoffpolytope.\nğ‘™\nThis operation effectively constrains the residual connection matrices within the manifold\nthat is constituted by doubly stochastic matrices. Since the row and column sums of these\nmatricesequalto1,theoperationH\nğ‘™\nresxğ‘™ functionsasaconvexcombinationoftheinputfeatures.\nThis characteristic facilitates a well-conditioned signal propagation where the feature mean\nis conserved, and the signal norm is strictly regularized, effectively mitigating the risk of\nvanishing or exploding signals. Furthermore, due to the closure of matrix multiplication for\ndoublystochasticmatrices,thecompositemapping\n(cid:206)ğ¿âˆ’ğ‘™Hres\nretainsthisconservationproperty.\nğ‘–=1 ğ¿âˆ’ğ‘–\nConsequently,mHCeffectivelymaintainsthestabilityofidentitymappingsbetweenarbitrary\ndepths. To ensure efficiency, we employ kernel fusion and develop mixed precision kernels\nutilizingTileLang(Wangetal.,2025). Furthermore,wemitigatethememoryfootprintthrough",
    "char_length": 1487
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 6,
    "text": "selectiverecomputingandcarefullyoverlapcommunicationwithintheDualPipeschedule(Liu\netal.,2024b).\nExtensive experiments on language model pretraining demonstrate that mHC exhibits\nexceptionalstabilityandscalabilitywhilemaintainingtheperformanceadvantagesofHC.In-\nhouselarge-scaletrainingindicatesthatmHCsupportstrainingatscaleandintroducesonlya\n6.7%additionaltimeoverheadwhenexpansionrateğ‘› =4.\n2. Related Works\nArchitecturaladvancementsindeeplearningcanbeprimarilyclassifiedintomicro-designand\nmacro-design. Micro-designconcernstheinternalarchitectureofcomputationalblocks,specifying\nhow features are processed across spatial, temporal, and channel dimensions. In contrast,\nmacro-designestablishestheinter-blocktopologicalstructure,therebydictatinghowfeature\nrepresentationsarepropagated,routed,andmergedacrossdistinctlayers.\n2.1. MicroDesign\nDrivenbyparametersharingandtranslationinvariance,convolutioninitiallydominatedthepro-\ncessingofstructuredsignals. Whilesubsequentvariationssuchasdepthwiseseparable(Chollet,\n2017) and grouped convolutions (Xie et al., 2017) optimized efficiency, the advent of Trans-\nformers (Vaswani et al., 2017) established Attention and Feed-Forward Networks (FFNs) as\nthe fundamental building blocks of modern architecture. Attention mechanisms facilitate\nglobalinformationpropagation,whileFFNsenhancetherepresentationalcapacityofindividual\nfeatures. TobalanceperformancewiththecomputationaldemandsofLLMs,attentionmecha-",
    "char_length": 1449
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 7,
    "text": "nismshaveevolvedtowardsefficientvariantssuchasMulti-QueryAttention(MQA)(Shazeer,\n2019),Grouped-QueryAttention(GQA)(Ainslieetal.,2023),andMulti-HeadLatentAttention\n4\n(MLA)(Liuetal.,2024a). Simultaneously,FFNshavebeengeneralizedintosparsecomputing\nparadigmsviaMixture-of-Experts(MoE)(Fedusetal.,2022;Lepikhinetal.,2020;Shazeeretal.,\n2017),allowingformassiveparameterscalingwithoutproportionalcomputationalcosts.\n2.2. MacroDesign\nMacro-designgovernstheglobaltopologyofthenetwork(Srivastavaetal.,2015). Following\nResNet (He et al., 2016a), architectures such as DenseNet (Huang et al., 2017) and Fractal-\nNet(Larssonetal.,2016)aimedtoenhanceperformancebyincreasingtopologicalcomplexity\nthroughdenseconnectivityandmulti-pathstructures,respectively. DeepLayerAggregation\n(DLA)(Yuetal.,2018)furtherextendedthisparadigmbyrecursivelyaggregatingfeaturesacross\nvariousdepthsandresolutions.\nMore recently, the focus of macro-design has shifted toward expanding the width of the\nresidual stream (Chai et al., 2020; Fang et al., 2023; Heddes et al., 2025; Mak and Flanigan,\n2025;Menghanietal.,2025;Pagliardinietal.,2024;Xiaoetal.,2025;Xieetal.,2023;Zhuetal.,\n2024). Hyper-Connections(HC)(Zhuetal.,2024)introducedlearnablematricestomodulate\nconnectionstrengthsamongfeaturesatvaryingdepths,whiletheResidualMatrixTransformer\n(RMT)(MakandFlanigan,2025)replacedthestandardresidualstreamwithanouter-product\nmemorymatrixtofacilitatefeaturestorage. Similarly,MUDDFormer(Xiaoetal.,2025)employs",
    "char_length": 1471
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 8,
    "text": "multiwaydynamicdenseconnectionstooptimizecross-layerinformationflow. Despitetheir\npotential,theseapproachescompromisetheinherentidentitymappingpropertyoftheresidual\nconnection,therebyintroducinginstabilityandhinderingscalability. Furthermore,theyincur\nsignificant memory access overhead due to expanded feature widths. Building upon HC,\ntheproposedmHCrestrictstheresidualconnectionspaceontoaspecificmanifoldtorestore\ntheidentitymappingproperty,whilealsoincorporatingrigorousinfrastructureoptimizations\nto ensure efficiency. This approach enhances stability and scalability while maintaining the\ntopologicalbenefitsofexpandedconnections.\n3. Preliminary\nWefirstestablishthenotationusedinthiswork. IntheHCformulation,theinputtotheğ‘™-thlayer,\nxğ‘™ âˆˆ R1Ã—ğ¶ ,isexpandedbyafactorofğ‘›toconstructahiddenmatrixxğ‘™ = (xâŠ¤\nğ‘™,0\n,...,xâŠ¤\nğ‘™,ğ‘›âˆ’1\n)âŠ¤ âˆˆ Rğ‘›Ã—ğ¶\nwhich can be viewed as ğ‘›-stream residual. This operation effectively broadens the width of\ntheresidualstream. Togoverntheread-out, write-in, andupdatingprocessesofthisstream,\nHCintroducesthreelearnablelinearmappingsâ€”Hpre ,Hpost âˆˆ R1Ã—ğ‘› ,andHres âˆˆ Rğ‘›Ã—ğ‘› . These\nğ‘™ ğ‘™ ğ‘™\nmappingsmodifythestandardresidualconnectionshowninEq.(1),resultingintheformulation\ngiveninEq.(3).\nIntheHCformulation,learnablemappingsarecomposedoftwopartsofcoefficients: the\ninput-dependentoneandtheglobalone,referredtoasdynamicmappingsandstaticmappings,\nrespectively. Formally,HCcomputesthecoefficientsasfollows:\nï£± ï£´\nï£´\nxËœğ‘™ =RMSNorm(xğ‘™ )\nï£´\nï£´ ï£´\nï£´ï£²\nH\nğ‘™\npre =ğ›¼p\nğ‘™\nreÂ·tanh(ğœƒ\nğ‘™\npre xËœâŠ¤\nğ‘™\n)+b p\nğ‘™\nre\n(5)\nï£´ ï£´\nH",
    "char_length": 1501
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 9,
    "text": "ğ‘™\npost =ğ›¼p\nğ‘™\nostÂ·tanh(ğœƒ\nğ‘™\npost xËœâŠ¤\nğ‘™\n)+b p\nğ‘™\nost\nï£´\nï£´ ï£´\nï£´\nH\nğ‘™\nres =ğ›¼r\nğ‘™\nesÂ·tanh(ğœƒ\nğ‘™\nresxËœâŠ¤\nğ‘™\n)+br\nğ‘™\nes,\nï£³\nwhereRMSNorm(Â·) (ZhangandSennrich,2019)isappliedtothelastdimension,andthescalars\nğ›¼pre ,ğ›¼post and ğ›¼res âˆˆ R are learnable gating factors initialized to small values. The dynamic\nğ‘™ ğ‘™ ğ‘™\n5\nmappingsarederivedvialinearprojectionsparameterizedbyğœƒpre ,ğœƒpost âˆˆ R1Ã—ğ¶ andğœƒres âˆˆ Rğ‘›Ã—ğ¶ ,\nğ‘™ ğ‘™ ğ‘™\nwhilethestaticmappingsarerepresentedbylearnablebiasesb pre ,b post âˆˆ R1Ã—ğ‘› andbres âˆˆ Rğ‘›Ã—ğ‘› .\nğ‘™ ğ‘™ ğ‘™\nItisworthnotingthattheintroductionofthesemappingsâ€”Hpre ,Hpost ,andHresâ€”incurs\nğ‘™ ğ‘™ ğ‘™\nnegligiblecomputationaloverhead,asthetypicalexpansionrateğ‘›,e.g. 4,ismuchsmallerthan\nthe input dimension ğ¶. With this design, HC effectively decouples the information capacity\noftheresidualstreamfromthelayerâ€™sinputdimension,whichisstronglycorrelatedwiththe\nmodelâ€™scomputationalcomplexity(FLOPs). Consequently,HCoffersanewavenueforscaling\nbyadjustingtheresidualstreamwidth,complementingthetraditionalscalingdimensionsof\nmodel FLOPs and training data size discussed in pre-training scaling laws (Hoffmann et al.,\n2022).\nAlthoughHCnecessitatesthreemappingstomanagethedimensionalmismatchbetween\ntheresidualstreamandthelayerinput,preliminaryexperimentspresentedinTab.1indicate\nthat the residual mapping Hres yields the most significant performance gain. This finding\nğ‘™\nunderscoresthecriticalimportanceofeffectiveinformationexchangewithintheresidualstream.\nTable1 | AblationStudyofHCComponents. Whenaspecificmapping(Hpre ,Hpost ,orHres)is",
    "char_length": 1497
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 10,
    "text": "ğ‘™ ğ‘™ ğ‘™\ndisabled,weemployafixedmappingtomaintaindimensionalconsistency: uniformweightsof\n1/ğ‘›forHpre ,uniformweightsofonesforHpost ,andtheidentitymatrixforHres.\nğ‘™ ğ‘™ ğ‘™\nHres Hpre Hpost AbsoluteLossGap\nğ‘™ ğ‘™ ğ‘™\n0.0\nâœ“ âˆ’0.022\nâœ“ âœ“ âˆ’0.025\nâœ“ âœ“ âœ“ âˆ’0.027\n3.1. NumericalInstability\nWhile the residual mapping Hres is instrumental for performance, its sequential application\nğ‘™\nposesasignificantrisktonumericalstability. AsdetailedinEq.(4),whenHCisextendedacross\nmultiplelayers,theeffectivesignalpropagationfromlayerğ‘™ to ğ¿isgovernedbythecomposite\nmapping (cid:206)ğ¿âˆ’ğ‘™Hres. SincethelearnablemappingHresisunconstrained,thiscompositemapping\nğ‘–=1 ğ¿âˆ’ğ‘– ğ‘™\ninevitablydeviatesfromtheidentitymapping. Consequently,thesignalmagnitudeisproneto\nexplosionorvanishingduringboththeforwardpassandbackpropagation. Thisphenomenon\nunderminesthefundamentalpremiseofresiduallearning,whichreliesonunimpededsignal\nflow,therebydestabilizingthetrainingprocessindeeperorlarger-scalemodels.\nEmpiricalevidencesupportsthisanalysis. Weobserveunstablelossbehaviorinlarge-scale\nexperiments,asillustratedinFig.2. TakingmHCasthebaseline,HCexhibitsanunexpected\nlosssurgearoundthe12kstep,whichishighlycorrelatedwiththeinstabilityinthegradient\nnorm. Furthermore,theanalysisonHres validatesthemechanismofthisinstability. Toquantify\nğ‘™\nhowthecompositemapping\n(cid:206)ğ¿âˆ’ğ‘™Hres\namplifiessignalsalongtheresidualstream,weutilize\nğ‘–=1 ğ¿âˆ’ğ‘–\ntwometrics. Thefirst,basedonthemaximumabsolutevalueoftherowsumsofthecomposite",
    "char_length": 1450
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 11,
    "text": "mapping, captures the worst-case expansion in the forward pass. The second, based on the\nmaximumabsolutecolumnsum,correspondstothebackwardpass. Werefertothesemetrics\nastheAmaxGainMagnitudeofthecompositemapping. AsshowninFig.3(b),theAmaxGain\nMagnitudeyieldsextremevalueswithpeaksof3000,astarkdivergencefrom1thatconfirms\nthepresenceofexplodingresidualstreams.\n6\n0.012\n0.010\n0.008\n0.006\n0.004\n0.002\n0.000\n-0.002\n0 10000 20000 30000 40000 50000\nSteps\npaG\nssoL\netulosbA\n0.25\nmHC\nHC 0.20\n0.15\n0.10\n0.05\n0.00\n0 10000 20000 30000 40000 50000\nSteps\n(a) Absolute Training Loss Gap vs. Training Steps\nmroN\ndarG\nmHC\nHC\n(b) Gradient Norm vs. Training Steps\nFigure2|TrainingInstabilityofHyper-Connections(HC).Thisfigureillustrates(a)theabsolute\nlossgapofHCrelativetomHC,and(b)thecomparisonsofgradientnorms. Allresultsarebased\non27Bmodels.\n101\n100\n0 10 20 30 40 50 60\nLayer Index l\nedutingaM\nniaG\nxamA\n105\nres Forward Signal Gain Hl\nHl res Backward Gradient Gain 104\n103\n102\n101\n0 10 20 30 40 50 60\nLayer Index l\n(a) Single-Layer Mapping\nedutingaM\nniaG\nxamA\nl res Forward Signal Gain i=1Hl+1\nâˆ’\ni\nY6 i 1 =âˆ’1 l H6 re 1 s\nâˆ’\ni Backward Gradient Gain\nY\n(b) Composite Mapping\nFigure 3 | Propagation Instability of Hyper-Connections (HC). This figure illustrates the\npropagation dynamics of (a) the single-layer mapping Hres and (b) the composite mapping\nğ‘™\n(cid:206)ğ¿âˆ’ğ‘™Hres withinthe27Bmodel. Thelayerindexğ‘™ (x-axis)unrollseachstandardTransformer\nğ‘–=1 ğ¿âˆ’ğ‘–",
    "char_length": 1433
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 12,
    "text": "blockintotwoindependentlayers(AttentionandFFN).TheAmaxGainMagnitude(y-axis)is\ncalculatedasthemaximumabsoluterowsum(fortheforwardsignal)andcolumnsum(forthe\nbackwardgradient),averagedoveralltokensinaselectedsequence.\n3.2. SystemOverhead\nWhile the computational complexity of HC remains manageable due to the linearity of the\nadditionalmappings,thesystem-leveloverheadpreventsanon-negligiblechallenge. Specifically,\nmemoryaccess(I/O)costsoftenconstituteoneoftheprimarybottlenecksinmodernmodel\narchitectures,whichiswidelyreferredtoastheâ€œmemorywallâ€(Daoetal.,2022). Thisbottleneck\nisfrequentlyoverlookedinarchitecturaldesign,yetitdecisivelyimpactsruntimeefficiency.\nFocusingonthewidelyadoptedpre-normTransformer(Vaswanietal.,2017)architecture,\nweanalyzetheI/OpatternsinherenttoHC.Tab.2summarizesthepertokenmemoryaccess\noverhead in a singleresidual layerintroducedby the ğ‘›-stream residual design. The analysis\nrevealsthatHCincreasesthememoryaccesscostbyafactorapproximatelyproportionaltoğ‘›.\nThisexcessiveI/Odemandsignificantlydegradestrainingthroughputwithoutthemitigationof\nfusedkernels. Besides,sinceHpre ,Hpost ,andHres involvelearnableparameters,theirinterme-\nğ‘™ ğ‘™ ğ‘™\ndiateactivationsarerequiredforbackpropagation. Thisresultsinasubstantialincreaseinthe\nGPUmemoryfootprint,oftennecessitatinggradientcheckpointingtomaintainfeasiblememory\nusage. Furthermore,HCrequiresğ‘›-foldmorecommunicationcostinpipelineparallelism(Qi\netal.,2024),leadingtolargerbubblesanddecreasingthetrainingthroughput.\n7",
    "char_length": 1485
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 13,
    "text": "Table 2 | Comparison of Memory Access Costs Per Token. This analysis accounts for the\noverheadintroducedbytheresidualstreammaintenanceintheforwardpass, excludingthe\ninternalI/Oofthelayerfunction F.\nMethod Operation Read(Elements) Write(Elements)\nResidual ResidualMerge 2ğ¶ ğ¶\nConnection\nTotalI/O 2C C\nCalculateHpre ,Hpost ,Hres ğ‘›ğ¶ ğ‘›2+2ğ‘›\nğ‘™ ğ‘™ ğ‘™\nHpre ğ‘›ğ¶+ğ‘› ğ¶\nğ‘™\nHyper- Hpost ğ¶+ğ‘› ğ‘›ğ¶\nğ‘™\nConnections Hres ğ‘›ğ¶+ğ‘›2 ğ‘›ğ¶\nğ‘™\nResidualMerge 2ğ‘›ğ¶ ğ‘›ğ¶\nTotalI/O (5n+1)C+n2+2n (3n+1)C+n2+2n\n4. Method\n4.1. Manifold-ConstrainedHyper-Connections\nDrawinginspirationfromtheidentitymappingprinciple(Heetal.,2016b),thecorepremise\nofmHCistoconstraintheresidualmappingHres ontoaspecificmanifold. Whiletheoriginal\nğ‘™\nidentitymappingensuresstabilitybyenforcingHres =I,itfundamentallyprecludesinformation\nğ‘™\nexchange within the residual stream, which is critical for maximizing the potential of multi-\nstreamarchitectures. Therefore,weproposeprojectingtheresidualmappingontoamanifold\nthatsimultaneouslymaintainsthestabilityofsignalpropagationacrosslayersandfacilitates\nmutualinteractionamongresidualstreamstopreservethemodelâ€™sexpressivity. Tothisend,\nwerestrictHres tobeadoublystochasticmatrix,whichhasnon-negativeentrieswhereboth\nğ‘™\ntherowsandcolumnssumto1. Formally,letMres denotethemanifoldofdoublystochastic\nmatrices(alsoknownastheBirkhoffpolytope). WeconstrainH ğ‘™ res to P Mres (H ğ‘™ res),definedas:\nP Mres (H ğ‘™ res) â‰” (cid:8) H ğ‘™ res âˆˆ Rğ‘›Ã—ğ‘› | H ğ‘™ res1ğ‘› =1ğ‘›, 1 âŠ¤ ğ‘› H ğ‘™ res =1 âŠ¤ ğ‘› , H ğ‘™ res â©¾ 0 (cid:9) , (6)",
    "char_length": 1470
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 14,
    "text": "where1ğ‘› representstheğ‘›-dimensionalvectorofallones.\nItisworthnotingthatwhenğ‘› =1,thedoublystochasticconditiondegeneratestothescalar\n1,therebyrecoveringtheoriginalidentitymapping. Thechoiceofdoublestochasticityconfers\nseveralrigoroustheoreticalpropertiesbeneficialforlarge-scalemodeltraining:\n1. Norm Preservation: The spectral norm of a doubly stochastic matrix is bounded by 1\n(i.e., âˆ¥Hresâˆ¥ â‰¤ 1). Thisimpliesthatthelearnablemappingisnon-expansive,effectively\nğ‘™ 2\nmitigatingthegradientexplosionproblem.\n2. Compositional Closure: The set of doubly stochastic matrices is closed under matrix\nmultiplication. Thisensuresthatthecompositeresidualmappingacrossmultiplelayers,\n(cid:206)ğ¿âˆ’ğ‘™Hres,remainsdoublystochastic,therebypreservingstabilitythroughouttheentire\nğ‘–=1 ğ¿âˆ’ğ‘–\ndepthofthemodel.\n3. Geometric Interpretation via the Birkhoff Polytope: The set Mres forms the Birkhoff\npolytope, which is the convex hull of the set of permutation matrices. This provides a\nclear geometric interpretation: the residual mapping acts as a convex combination of\npermutations. Mathematically,therepeatedapplicationofsuchmatricestendstoincrease\n8\nthemixingofinformationacrossstreamsmonotonically,effectivelyfunctioningasarobust\nfeaturefusionmechanism.\nAdditionally,weimposenon-negativityconstraintsontheinputmappingsHpre\nandoutput\nğ‘™\nmappingsHpost\n. Thisconstrainpreventssignalcancellationarisingfromthecompositionof\nğ‘™\npositiveandnegativecoefficients,whichcanalsobeconsideredasaspecialmanifoldprojection.",
    "char_length": 1479
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 15,
    "text": "4.2. ParameterizationandManifoldProjection\nIn this section, we detail the calculation process of Hpre ,Hpost ,andHres in mHC. Given the\nğ‘™ ğ‘™ ğ‘™\ninputhiddenmatrixxğ‘™ âˆˆ Rğ‘›Ã—ğ¶ attheğ‘™-thlayer,wefirstflattenitintoavectorx(cid:174) ğ‘™ =vec(xğ‘™ ) âˆˆ R1Ã—ğ‘›ğ¶\ntopreservefullcontextinformation. Then, wefollowtheoriginalHCformulationtogetthe\ndynamicmappingsandthestaticmappingsasfollows:\nï£±\nï£´ ï£´\nx(cid:174)â€²\nğ‘™\n=RMSNorm(x(cid:174)\nğ‘™\n)\nï£´\nï£´ ï£´\nï£´ï£²\nHËœ\nğ‘™\npre =ğ›¼p\nğ‘™\nreÂ·(x(cid:174)â€²\nğ‘™\nğœ‘p\nğ‘™\nre)+b p\nğ‘™\nre\n(7)\nï£´ ï£´\nHËœ\nğ‘™\npost =ğ›¼p\nğ‘™\nostÂ·(x(cid:174)â€²\nğ‘™\nğœ‘p\nğ‘™\nost)+b p\nğ‘™\nost\nï£´\nï£´ ï£´\nï£´\nHËœ\nğ‘™\nres =ğ›¼r\nğ‘™\nesÂ·mat(x(cid:174)â€²\nğ‘™\nğœ‘r\nğ‘™\nes)+br\nğ‘™\nes,\nï£³\nwhere ğœ‘pre ,ğœ‘post âˆˆ Rğ‘›ğ¶Ã—ğ‘› and ğœ‘res âˆˆ Rğ‘›ğ¶Ã—ğ‘›2 arelinearprojectionsfordynamicmappingsand\nğ‘™ ğ‘™ ğ‘™\nmat(Â·) isareshapefunctionfrom R1Ã—ğ‘›2 to Rğ‘›Ã—ğ‘› .\nThen,thefinalconstrainedmappingsareobtainedvia:\nï£±Hpre =ğœ(HËœpre)\nï£´ ï£´ ğ‘™ ğ‘™\nï£´ï£² Hpost =2ğœ(HËœpost)\n(8)\nğ‘™ ğ‘™\nï£´\nï£´ ï£´Hres =Sinkhorn-Knopp(HËœres),\nï£³ ğ‘™ ğ‘™\nwhere ğœ(Â·) denotes the Sigmoid function. The Sinkhorn-Knopp(Â·) operator firstly makes all\nelements to be positive via an exponent operator and then conducts iterative normalization\nprocess that alternately rescales rows and columns to sum to 1. Specifically, given a positive\nmatrixM(0) =exp(HËœres) asthestartpoint,thenormalizationiterationproceedsas:\nğ‘™\n(cid:16) (cid:17)\nM (ğ‘¡) =T ğ‘Ÿ T ğ‘ (M (ğ‘¡âˆ’1)) , (9)\nwhereT ğ‘Ÿ andT ğ‘ denoterowandcolumnnormalization,respectively. Thisprocessconvergestoa\ndoublystochasticmatrixHres =M(ğ‘¡ max ) asğ‘¡ â†’ âˆ. Wechooseğ‘¡ =20asapracticalvaluein\nğ‘™ max max\nourexperiments.",
    "char_length": 1474
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 16,
    "text": "4.3. EfficientInfrastructureDesign\nInthissection,wedetailtheinfrastructuredesigntailoredformHC.Throughrigorousoptimiza-\ntion,weimplementmHC(withğ‘› =4)inlarge-scalemodelswithamarginaltrainingoverhead\nofonly6.7%.\n4.3.1. KernelFusion\nObserving that RMSNorm in mHC imposes significant latency when operating on the high-\ndimensionalhiddenstatex(cid:174) ğ‘™ âˆˆ R1Ã—ğ‘›ğ¶ ,wereorderthedividing-by-normoperationtofollowthe\n9\nmatrixmultiplication. Thisoptimizationmaintainsmathematicalequivalencewhileimproving\nefficiency. Furthermore,weemploymixed-precisionstrategiestomaximizenumericalaccuracy\nwithoutcompromisingspeed,andfusemultipleoperationswithsharedmemoryaccessinto\nunifiedcomputekernelstoreducememorybandwidthbottlenecks. Basedontheinputsand\nparametersdetailedinEq.(10)to (13),weimplementthreespecializedmHCkernelstocompute\nH\nğ‘™\npre ,H\nğ‘™\npost ,andH\nğ‘™\nres. Inthesekernels,thebiasesandlinearprojectionsareconsolidatedintobğ‘™\nand ğœ‘ ğ‘™,andtheRMSNormweightisalsoabsorbedin ğœ‘ ğ‘™.\nâ€¢ Eq. (14) to (15): We develop a unified kernel that fuses two scans on x(cid:174) ğ‘™, leveraging ma-\ntrix multiplication units to maximize memory bandwidth utilization. The backward\npassâ€”comprisingtwomatrixmultiplicationsâ€”issimilarlyconsolidatedintoasingleker-\nnel,eliminatingredundantreloadingofx(cid:174) ğ‘™. Bothkernelsfeatureafinelytunedpipeline\n(load,cast,compute,store)toefficientlyhandlemixed-precisionprocessing.\nâ€¢ Eq.(16)to (18): Theselightweightoperationsonsmallcoefficientsareopportunistically",
    "char_length": 1467
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 17,
    "text": "fusedintoasinglekernel,significantlyreducingkernellaunchoverhead.\nâ€¢ Eq. (19): We implement the Sinkhorn-Knopp iteration within a single kernel. For the\nbackwardpass,wederiveacustombackwardkernelthatrecomputestheintermediate\nresultson-chipandtraversestheentireiteration.\nğœ‘ ğ‘™ : tfloat32 [ğ‘›ğ¶,ğ‘›2+2ğ‘›] (10)\nx(cid:174) ğ‘™ : bfloat16 [1,ğ‘›ğ¶] (11)\nğ›¼pre ,ğ›¼post ,ğ›¼res : float32 Scalars (12)\nğ‘™ ğ‘™ ğ‘™\nbğ‘™ : float32 [1,ğ‘›2+2ğ‘›] (13)\n(cid:104) (cid:105)\nH ËœËœ ğ‘™ pre ,H ËœËœ ğ‘™ post ,H ËœËœ ğ‘™ res : float32 =x(cid:174) ğ‘™ ğœ‘ ğ‘™ (14)\nâˆš\n(cid:13) (cid:13)\nğ‘Ÿ : float32 =(cid:13)x(cid:174) ğ‘™(cid:13) / ğ‘›ğ¶ (15)\n2\n(cid:104) (cid:105) (cid:104) (cid:105)\nHËœ\nğ‘™\npre ,HËœ\nğ‘™\npost ,HËœ\nğ‘™\nres : float32 =1/ğ‘Ÿ ğ›¼p\nğ‘™\nreH ËœËœ\nğ‘™\npre ,ğ›¼p\nğ‘™\nostH ËœËœ\nğ‘™\npost ,ğ›¼r\nğ‘™\nesH ËœËœ\nğ‘™\nres +bğ‘™ (16)\n(cid:16) (cid:17)\nHpre : float32 =ğœ HËœpre (17)\nğ‘™ ğ‘™\n(cid:16) (cid:17)\nHpost : float32 =2ğœ HËœpost (18)\nğ‘™ ğ‘™\nHres : float32 =Sinkhorn-Knopp (cid:0) HËœres(cid:1) (19)\nğ‘™ ğ‘™\nUsing the coefficients derived from the aforementioned kernels, we introduce two addi-\ntional kernels to apply these mappings: one for F\npre\nâ‰” H\nğ‘™\npre xğ‘™ and another for F\npost,res\nâ‰”\nH\nğ‘™\nresxğ‘™ +H\nğ‘™\npostâŠ¤ F(Â·,Â·). ThroughfusingtheapplicationofH\nğ‘™\npost andH\nğ‘™\nres withresidualmerging,\nwereducethenumberofelementsreadfrom (3ğ‘›+1)ğ¶ to (ğ‘›+1)ğ¶ andthenumberofelements\nwritten from 3ğ‘›ğ¶ to ğ‘›ğ¶ for this kernel. We efficiently implement the majority of kernels (ex-\ncluding Eq. (14) to (15)) using TileLang (Wang et al., 2025). This framework streamlines the",
    "char_length": 1439
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 18,
    "text": "implementationofkernelswithcomplexcalculationprocessandallowsustofullyutilizethe\nmemorybandwidthwithminimalengineeringeffort.\n4.3.2. Recomputing\nThe ğ‘›-stream residual design introduces substantial memory overhead during training. To\nmitigatethis,wediscardtheintermediateactivationsofthemHCkernelsaftertheforwardpass\nandrecomputethemon-the-flyinthebackwardpass,throughre-executingthemHCkernels\n10\nwithouttheheavylayerfunction F. Consequently,forablockof ğ¿ ğ‘Ÿ consecutivelayers,weneed\nonlystoretheinputxğ‘™ tothefirstlayer. Excludinglightweightcoefficientswhileaccounting\n0\nforthepre-normwithinF,Tab. 3summarizestheintermediateactivationspreservedforthe\nbackwardpass.\nTable3 | StoredandRecomputedIntermediateActivationsWelistpertokenactivationpre-\nservedforthebackwardpassandthetransientactivationrecomputedin ğ¿ ğ‘Ÿ consecutivelayers.\nLayerğ‘™ 0 representsthefirstlayerin ğ¿ ğ‘Ÿ layersandlayerğ‘™ isin [ğ‘™ 0 ,ğ‘™ 0 +ğ¿ ğ‘Ÿ âˆ’1].\nActivations xğ‘™ 0 F(H ğ‘™ pre xğ‘™,W ğ‘™ ) xğ‘™ H ğ‘™ pre xğ‘™ RMSNorm(H ğ‘™ pre xğ‘™ )\nSize(Elements) ğ‘›ğ¶ ğ¶ ğ‘›ğ¶ ğ¶ ğ¶\nStoredMethod Every ğ¿ ğ‘Ÿ layers Everylayer Transientinside ğ¿ ğ‘Ÿ layers\nSincemHCkernelsrecomputationisperformedforblocksof ğ¿ ğ‘Ÿ consecutivelayers,given\natotalof ğ¿layers,wemustpersistentlystorethefirstlayerinputxğ‘™\n0\nforall âŒˆ\nğ¿\nğ¿\nğ‘Ÿ\nâŒ‰ blocksforthe\nbackwardpass. Inadditiontothisresidentmemory,therecomputationprocessintroducesa\ntransientmemoryoverheadof (ğ‘›+2)ğ¶Ã—ğ¿ ğ‘Ÿ elementsfortheactiveblock,whichdeterminesthe\npeakmemoryusageduringbackpropagation. Consequently,wedeterminetheoptimalblock\nsize ğ¿âˆ—\nğ‘Ÿ",
    "char_length": 1496
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 19,
    "text": "byminimizingthetotalmemoryfootprintcorrespondedto ğ¿ ğ‘Ÿ:\n(cid:20) (cid:24) ğ¿ (cid:25) (cid:21) âˆšï¸‚ ğ‘›ğ¿\nğ¿âˆ— ğ‘Ÿ =argm ğ¿ i ğ‘Ÿ n ğ‘›ğ¶Ã— ğ¿ ğ‘Ÿ +(ğ‘›+2)ğ¶Ã—ğ¿ ğ‘Ÿ â‰ˆ ğ‘›+2 . (20)\nFurthermore,pipelineparallelisminlarge-scaletrainingimposesaconstraint: recomputation\nblocks must not cross pipeline stage boundaries. Observing that the theoretical optimum ğ¿âˆ—\nğ‘Ÿ\ntypically aligns with the number of layers per pipeline stage, we choose to synchronize the\nrecomputationboundarieswiththepipelinestages.\n4.3.3. OverlappingCommunicationinDualPipe\nInlarge-scaletraining,pipelineparallelismisthestandardpracticeformitigatingparameterand\ngradientmemoryfootprints. Specifically,weadopttheDualPipeschedule(Liuetal.,2024b),\nwhich effectively overlaps scale-out interconnected communication traffic, such as those in\nexpertandpipelineparallelism. However,comparedtothesingle-streamdesign,theproposed\nğ‘›-stream residual in mHC incurs substantial communication latency across pipeline stages.\nFurthermore,atstageboundaries,therecomputationofmHCkernelsforall ğ¿ ğ‘Ÿ layersintroduces\nnon-negligiblecomputationaloverhead. Toaddressthesebottlenecks,weextendtheDualPipe\nschedule(seeFig.4)tofacilitateimprovedoverlappingofcommunicationandcomputationat\npipelinestageboundaries.\nNotably, to prevent blocking the communication stream, we execute the F kernels\npost,res\nof MLP (i.e. FFN) layers on a dedicated high-priority compute stream. We further refrain\nfrom employing persistent kernels for long-running operations in attention layers, thereby",
    "char_length": 1490
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 20,
    "text": "preventingextendedstalls. Thisdesignenablesthepreemptionofoverlappedattentioncom-\nputations,allowingforflexibleschedulingwhilemaintaininghighutilizationofthecompute\ndeviceâ€™sprocessingunits. Furthermore,therecomputationprocessisdecoupledfrompipeline\ncommunicationdependencies,astheinitialactivationofeachstagexğ‘™ isalreadycachedlocally.\n0\n11\nâ„±! ) '((B) â„±! * \"#$, '(#(B) â„±! * '((B) â„±! * '((F) â„±! * \"#$, '(#(F) â„±! ) '((F)\nWhole Stage\nNormal Compute Stream MLP (B) MLP (W) MLP (F) ATTN (B) ATTN (W) Recompute (B) ATTN (F)\nCommunication Stream DISPATCH (F) DISPATCH (B) COMBINE (F) PP Send Recv (F) PP Send Recv(B) COMBINE (B)\nHigh Priority Compute Stream\nâ„±! ) \"#$, '(#(F) â„±! ) \"#$, '(#(B)\nFigure 4 | Communication-Computation Overlapping for mHC. We extend the DualPipe\nscheduletohandletheoverheadintroducedbymHC.Lengthsofeachblockareillustrativeonly\nanddonotrepresentactualduration. (F),(B),(W)referstoforwardpass,backwardpass,weight\ngradientcomputation,respectively. FA and FM representskernelscorrespondedtoAttention\nandMLP,respectively.\n5. Experiments\n5.1. ExperimentalSetup\nWevalidatetheproposedmethodvialanguagemodelpre-training,conductingacomparative\nanalysisbetweenthebaseline,HC,andourproposedmHC.UtilizingMoEarchitecturesinspired\nby DeepSeek-V3 (Liu et al., 2024b), we train four distinct model variants to cover different\nevaluation regimes. Specifically, the expansion rate ğ‘› for both HC and mHC is set to 4. Our\nprimaryfocusisa27Bmodeltrainedwithadatasetsizeproportionaltoitsparameters,which",
    "char_length": 1500
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 21,
    "text": "serves as the subject for our system-level main results. Expanding on this, we analyze the\ncomputescalingbehaviorbyincorporatingsmaller3Band9Bmodelstrainedwithproportional\ndata, which allows us to observe performance trends across varying compute. Additionally,\ntospecificallyinvestigatethetokenscalingbehavior,wetrainaseparate3Bmodelonafixed\ncorpusof1trilliontokens. Detailedmodelconfigurationsandtraininghyper-parametersare\nprovidedinAppendixA.1.\n5.2. MainResults\n0.00\n-0.02\n-0.04\n-0.06\n10000 20000 30000 40000 50000\nSteps\npaG\nssoL\netulosbA\n0.20\n0.15\n0.10\nBaseline 0.05\nHC\nmHC\n0.00\n10000 20000 30000 40000 50000\nSteps\n(a) Absolute Training Loss Gap vs. Training Steps\nmroN\ndarG\nBaseline\nHC\nmHC\n(b) Gradient Norm vs. Training Steps\nFigure5 | TrainingStabilityofManifold-ConstrainedHyper-Connections(mHC).Thisfigure\nillustrates (a) the absolute training loss gap of mHC and HC relative to the baseline, and (b)\nthe gradient norm of the three methods. All experiments utilize the 27B model. The results\ndemonstratethatmHCexhibitsimprovedstabilityintermsofbothlossandgradientnorm.\nWe begin by examining the training stability and convergence of the 27B models. As\nillustrated in Fig. 5 (a), mHC effectively mitigates the training instability observed in HC,\nachievingafinallossreductionof0.021comparedtothebaseline. Thisimprovedstabilityis\nfurthercorroboratedbythegradientnormanalysisinFig.5(b),wheremHCexhibitssignificantly\nbetterbehaviorthanHC,maintainingastableprofilecomparabletothebaseline.\n12",
    "char_length": 1497
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 22,
    "text": "Table 4 | System-level Benchmark Results for 27B Models. This table compares the zero-\nshot and few-shot performance of the Baseline, HC, and mHC across 8 diverse downstream\nbenchmarks. mHCconsistentlyoutperformstheBaselineandsurpassesHConthemajorityof\nbenchmarks,demonstratingitseffectivenessinlarge-scalepre-training.\nBenchmark BBH DROP GSM8K HellaSwag MATH MMLU PIQA TriviaQA\n(Metric) (EM) (F1) (EM) (Acc.) (EM) (Acc.) (Acc.) (EM)\n#Shots 3-shot 3-shot 8-shot 10-shot 4-shot 5-shot 0-shot 5-shot\n27BBaseline 43.8 47.0 46.7 73.7 22.0 59.0 78.5 54.3\n27Bw/HC 48.9 51.6 53.2 74.3 26.4 63.0 79.9 56.3\n27Bw/mHC 51.0 53.9 53.8 74.7 26.0 63.4 80.5 57.6\nTab.4presentsthedownstreamperformanceacrossadiversesetofbenchmarks(Bisketal.,\n2020;Cobbeetal.,2021;Hendrycksetal.,2020,2021;Joshietal.,2017;Zellersetal.,2019). mHC\nyieldscomprehensiveimprovements,consistentlyoutperformingthebaselineandsurpassing\nHC on the majority of tasks. Notably, compared to HC, mHC further enhances the modelâ€™s\nreasoningcapabilities,deliveringperformancegainsof2.1%onBBH(Suzgunetal.,2022)and\n2.3%onDROP(Duaetal.,2019).\n5.3. ScalingExperiments\n0.02\n0.01\n0.00\n-0.01\n-0.02\n-0.03\n-0.04\n1021 1022\nFLOPs\npaG\nssoL\netulosbA\n101.0%\nBaseline\nmHC\n100.0%\n99.0%\n98.0%\n1021 1022\nFLOPs\noitaR\nssoL\nevitaleR\nBaseline 0.01\nmHC\n0.00\n-0.01\n-0.02\n-0.03\n2 4\nFLOPs Ã—1021\npaG\nssoL\netulosbA\n101.0%\nBaseline\nmHC\n100.0%\n99.0%\n98.0%\n2 4\nFLOPs Ã—1021\noitaR\nssoL\nevitaleR\nBaseline\nmHC\n(a) Compute Scaling Curve (b) Token Scaling Curve",
    "char_length": 1473
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 23,
    "text": "Figure6 | ScalingpropertiesofmHCcomparedtotheBaseline. (a)ComputeScalingCurve.\nSolidlinesdepicttheperformancegapacrossdifferentcomputebudgets. Eachpointrepresents\naspecificcompute-optimalconfigurationofmodelsizeanddatasetsize,scalingfrom3Band9B\nto27Bparameters. (b)TokenScalingCurve. Trajectoryofthe3Bmodelduringtraining. Each\npointrepresentsthemodelâ€™sperformanceatdifferenttrainingtokens. Detailedarchitectures\nandtrainingconfigurationsareprovidedinAppendixA.1.\nToassessthescalabilityofourapproach,wereporttherelativelossimprovementofmHC\nagainst the baseline across different scales. In Fig. 6 (a), we plot the compute scaling curve\nspanning3B,9B,and27Bparameters. Thetrajectoryindicatesthattheperformanceadvantageis\nrobustlymaintainedevenathighercomputationalbudgets,showingonlymarginalattenuation.\nFurthermore, we examine the within-run dynamics in Fig. 6 (b), which presents the token\nscalingcurveforthe3Bmodel. Collectively,thesefindingsvalidatetheeffectivenessofmHC\nin large-scale scenarios. This conclusion is further corroborated by our in-house large-scale\ntrainingexperiments.\n13\n2.0\n1.5\n1.0\n0.5\n0.0\n0 10 20 30 40 50 60\nLayer Index l\nedutingaM\nniaG\nxamA\n2.0\n1.5\n1.0\n0.5 PM res( Hl res) Forward Signal Gain\nPM res( Hl res) Backward Gradient Gain\n0.0\n0 10 20 30 40 50 60\nLayer Index l\n(a) Single-Layer Mapping\nedutingaM\nniaG\nxamA\nl i=1PM res( Hl re + s 1 âˆ’ i ) Forward Signal Gain\nY6 i 1 =âˆ’1 l PM res( H6 re 1 s âˆ’ i ) Backward Gradient Gain\nY\n(b) Composite Mapping",
    "char_length": 1473
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 24,
    "text": "Figure7 | PropagationStabilityofManifold-ConstrainedHyper-Connections(mHC).This\nfigureillustratesthepropagationdynamicsof(a)thesingle-layermapping P Mres (H ğ‘™ res) and(b)\nthecompositemapping (cid:206) ğ‘– ğ¿ = âˆ’ 1 ğ‘™P Mres (H ğ¿ r âˆ’ es ğ‘– ) withinthe27Bmodel. Theresultsdemonstratethat\nmHCsignificantlyenhancespropagationstabilitycomparedtoHC.\nres res res 30 res 30 res 60 res\nH1 H30 H60 i=1H31 âˆ’ i i=1H61 âˆ’ i i=1H61 âˆ’ i\n18.735.43 4.43 4.43 4.43 0.840.94-0.07-0.050.02 -21.64-5.58-3.74-5.71-6.60 -1.35-0.3Y8-0.33-0.34-0.31 -251.4-69Y.9-68.3-255.3142.1 -475.3-132Y.8-112.2-117.0-113.3\n-15.29-4.07-3.07-4.07-4.07 0.67-0.080.89-0.07-0.07 -20.22-6.06-2.27-5.33-6.57 6.471.81 1.56 1.58 1.51 -243.0-69.1-66.1-247.4139.6 -462.8-129.3-109.3-113.9-110.3\nHC\n-14.79-3.95-3.95-2.95-3.95 0.49-0.10-0.140.81-0.07 22.506.08 3.12 6.53 6.77 0.030.01-0.000.01 0.01 264.674.8 72.7268.9-151.8 509.1142.3120.2125.3121.3\n-15.88-4.22-4.22-4.22-3.22 0.960.06 0.05-0.030.87 -21.59-6.41-3.97-5.72-5.49 -0.81-0.23-0.19-0.20-0.19 -254.8-71.2-71.8-255.2143.3 -498.5-139.3-117.8-122.6-118.7\n-6.81-6.81-6.81-6.81 0.83 0.73 0.66 0.75 -11.97-6.86-10.23-11.89 1.22 1.04 1.06 1.02 -135.4-133.4-489.0273.3 -259.2-219.1-228.2-221.0\nPM res( H r 1 es) PM res( H r 3 e 0 s) PM res( H r 6 e 0 s) 3 i 0 =1PM res( H r 3 e 1 s âˆ’ i ) 3 i 0 =1PM res( H r 6 e 1 s âˆ’ i ) 6 i 0 =1PM res( H r 6 e 1 s âˆ’ i )\n1.000.67 0.09 0.03 0.22 1.000.96 0.01 0.00 0.04 1.000.92 0.06 0.01 0.01 1.00Y0.30 0.25 0.22 0.24 1.00Y0.35 0.28 0.17 0.20 1.00Y0.24 0.26 0.23 0.27",
    "char_length": 1498
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 25,
    "text": "1.000.26 0.48 0.26 0.00 1.000.00 0.97 0.03 0.00 1.000.05 0.81 0.01 0.13 1.000.24 0.25 0.25 0.26 1.000.03 0.62 0.29 0.07 1.010.23 0.25 0.26 0.27\nmHC\n1.000.03 0.24 0.00 0.73 1.000.00 0.00 1.00 0.00 1.000.00 0.01 0.97 0.02 1.000.20 0.24 0.28 0.28 1.000.01 0.17 0.80 0.02 1.010.21 0.25 0.27 0.28\n1.000.03 0.20 0.69 0.09 1.000.00 0.04 0.01 0.95 1.000.03 0.13 0.00 0.84 1.000.17 0.32 0.18 0.34 1.000.02 0.42 0.25 0.31 1.000.21 0.27 0.24 0.29\n0.98 1.00 0.98 1.04 0.96 1.02 1.04 0.99 1.00 1.01 0.99 1.00 0.90 1.06 0.93 1.11 0.41 1.50 1.50 0.60 0.88 1.03 1.00 1.11\nFigure8 | VisualizationsofLearnableMappings. Thisfiguredisplaysrepresentativesingle-\nlayer and composite mappings for HC (first row) and mHC (second row). Each matrix is\ncomputedbyaveragingoveralltokenswithinaselectedsequence. Thelabelsannotatedalong\nthey-axisandx-axisindicatetheforwardsignalgain(rowsum)andthebackwardgradientgain\n(columnsum),respectively.\n5.4. StabilityAnalysis\nSimilar to Fig. 3, Fig. 7 illustrates the propagation stability of mHC. Ideally, the single-layer\nmappingsatisfiesthedoublystochasticconstraint,implyingthatboththeforwardsignalgain\nandthebackwardgradientgainshouldequalto1. However,practiceimplementationsutilizing\ntheSinkhorn-Knoppalgorithmmustlimitthenumberofiterationstoachievecomputational\nefficiency. Inoursettings,weuse20iterationstoobtainanapproximatesolution. Consequently,\nasshowninFig.7(a),thebackwardgradientgaindeviatesslightlyfrom1. Inthecompositecase",
    "char_length": 1451
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 26,
    "text": "showninFig.7(b),thedeviationincreasesbutremainsbounded,reachingamaximumvalue\nofapproximately1.6. Notably, comparedtothemaximumgainmagnitudeofnearly3000in\nHC,mHCsignificantlyreducesitbythreeordersofmagnitude. Theseresultsdemonstratethat\nmHC significantly enhances propagation stability compared to HC, ensuring stable forward\nsignalandbackwardgradientflows. Additionally,Fig.8displaysrepresentativemappings. We\nobservethatforHC,whenthemaximumgainislarge,othervaluesalsotendtobesignificant,\nwhichindicatesgeneralinstabilityacrossallpropagationpaths. Incontrast,mHCconsistently\nyieldsstableresults.\n14\n6. Conclusion and Outlook\nInthispaper,weidentifythatwhileexpandingthewidthofresidualstreamanddiversifying\nconnections yields performance gains as proposed in Hyper-Connections (HC), the uncon-\nstrainednatureoftheseconnectionsleadstosignaldivergence. Thisdisruptioncompromises\ntheconservationofsignalenergyacrosslayers,inducingtraininginstabilityandhinderingthe\nscalabilityofdeepnetworks. Toaddressthesechallenges,weintroduceManifold-Constrained\nHyper-Connections(mHC),ageneralizedframeworkthatprojectstheresidualconnectionspace\nonto a specific manifold. By employing the Sinkhorn-Knopp algorithm to enforce a doubly\nstochasticconstraintonresidualmappings,mHCtransformssignalpropagationintoaconvex\ncombinationoffeatures. EmpiricalresultsconfirmthatmHCeffectivelyrestorestheidentity\nmappingproperty,enablingstablelarge-scaletrainingwithsuperiorscalabilitycomparedto",
    "char_length": 1463
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 27,
    "text": "conventionalHC.Crucially,throughefficientinfrastructure-leveloptimizations,mHCdelivers\ntheseimprovementswithnegligiblecomputationaloverhead.\nAsageneralizedextensionoftheHCparadigm,mHCopensseveralpromisingavenuesfor\nfutureresearch. Althoughthisworkutilizesdoublystochasticmatricestoensurestability,the\nframeworkaccommodatestheexplorationofdiversemanifoldconstraintstailoredtospecific\nlearningobjectives. Weanticipatethatfurtherinvestigationintodistinctgeometricconstraints\ncould yield novel methods that better optimize the trade-off between plasticity and stability.\nFurthermore, we hope mHC rejuvenates community interest in macro-architecture design.\nBy deepening the understanding of how topological structures influence optimization and\nrepresentationlearning,mHCwillhelpaddresscurrentlimitationsandpotentiallyilluminate\nnewpathwaysfortheevolutionofnext-generationfoundationalarchitectures.\nReferences\nJ.Ainslie,J.Lee-Thorp,M.DeJong,Y.Zemlyanskiy,F.LebrÃ³n,andS.Sanghai. Gqa: Training\ngeneralizedmulti-querytransformermodelsfrommulti-headcheckpoints. arXivpreprint\narXiv:2305.13245,2023.\nY.Bisk,R.Zellers,R.L.Bras,J.Gao,andY.Choi. PIQA:reasoningaboutphysicalcommonsense\ninnaturallanguage. InTheThirty-FourthAAAIConferenceonArtificialIntelligence,AAAI\n2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI\n2020,TheTenthAAAISymposiumonEducationalAdvancesinArtificialIntelligence,EAAI",
    "char_length": 1423
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 28,
    "text": "2020, New York, NY, USA, February 7-12, 2020, pages 7432â€“7439. AAAI Press, 2020. doi:\n10.1609/aaai.v34i05.6239. URLhttps://doi.org/10.1609/aaai.v34i05.6239.\nT.Brown,B.Mann,N.Ryder,M.Subbiah,J.D.Kaplan,P.Dhariwal,A.Neelakantan,P.Shyam,\nG. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural\ninformationprocessingsystems,33:1877â€“1901,2020.\nY.Chai,S.Jin,andX.Hou. Highwaytransformer: Self-gatingenhancedself-attentivenetworks.\nInD.Jurafsky,J.Chai,N.Schluter,andJ.Tetreault,editors,Proceedingsofthe58thAnnual\nMeeting of the Association for Computational Linguistics, pages 6887â€“6900, Online, July\n2020.AssociationforComputationalLinguistics. doi: 10.18653/v1/2020.acl-main.616. URL\nhttps://aclanthology.org/2020.acl-main.616/.\nF.Chollet. Xception: Deeplearningwithdepthwiseseparableconvolutions. InProceedingsof\ntheIEEEconferenceoncomputervisionandpatternrecognition,pages1251â€“1258,2017.\n15\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,\nJ.Hilton, R.Nakano, etal. Trainingverifierstosolvemathwordproblems. arXivpreprint\narXiv:2110.14168,2021.\nT. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. RÃ©. FlashAttention: Fast and memory-efficient\nexactattentionwithIO-awareness. InAdvancesinNeuralInformationProcessingSystems\n(NeurIPS),2022.\nD.Dua,Y.Wang,P.Dasigi,G.Stanovsky,S.Singh,andM.Gardner. DROP:Areadingcompre-\nhensionbenchmarkrequiringdiscretereasoningoverparagraphs.InJ.Burstein,C.Doran,and",
    "char_length": 1457
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 29,
    "text": "T.Solorio,editors,Proceedingsofthe2019ConferenceoftheNorthAmericanChapterofthe\nAssociation for Computational Linguistics: Human Language Technologies, NAACL-HLT\n2019,Minneapolis,MN,USA,June2-7,2019,Volume1(LongandShortPapers),pages2368â€“\n2378.AssociationforComputationalLinguistics, 2019. doi: 10.18653/V1/N19-1246. URL\nhttps://doi.org/10.18653/v1/n19-1246.\nY.Fang,Y.CAI,J.Chen,J.Zhao,G.Tian,andG.Li. Cross-layerretrospectiveretrievingvialayer\nattention. InTheEleventhInternationalConferenceonLearningRepresentations,2023. URL\nhttps://openreview.net/forum?id=pvgEL1yS3Ql.\nW.Fedus,B.Zoph,andN.Shazeer. Switchtransformers: Scalingtotrillionparametermodels\nwithsimpleandefficientsparsity. JournalofMachineLearningResearch,23(120):1â€“39,2022.\nK.He,X.Zhang,S.Ren,andJ.Sun.Deepresiduallearningforimagerecognition.InProceedings\noftheIEEEconferenceoncomputervisionandpatternrecognition,pages770â€“778,2016a.\nK.He,X.Zhang,S.Ren,andJ.Sun. Identitymappingsindeepresidualnetworks. InEuropean\nconferenceoncomputervision,pages630â€“645.Springer,2016b.\nM.Heddes,A.Javanmard,K.Axiotis,G.Fu,M.Bateni,andV.Mirrokni. Deepcrossattention:\nSuperchargingtransformerresidualconnections. InForty-secondInternationalConference\nonMachineLearning,2025. URLhttps://openreview.net/forum?id=j3JBfFnGYh.\nD.Hendrycks,C.Burns,S.Basart,A.Zou,M.Mazeika,D.Song,andJ.Steinhardt. Measuring\nmassivemultitasklanguageunderstanding. arXivpreprintarXiv:2009.03300,2020.",
    "char_length": 1420
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 30,
    "text": "D.Hendrycks,C.Burns,S.Kadavath,A.Arora,S.Basart,E.Tang,D.Song,andJ.Steinhardt.Mea-\nsuringmathematicalproblemsolvingwiththemathdataset. arXivpreprintarXiv:2103.03874,\n2021.\nJ.Hoffmann,S.Borgeaud,A.Mensch,E.Buchatskaya,T.Cai,E.Rutherford,D.deLasCasas,\nL.A.Hendricks,J.Welbl,A.Clark,T.Hennigan,E.Noland,K.Millican,G.vandenDriessche,\nB. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, O. Vinyals, J. Rae, and L. Sifre.\nAn empirical analysis of compute-optimal large language model training. In S. Koyejo,\nS. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural\nInformationProcessingSystems,volume35,pages30016â€“30030.CurranAssociates,Inc.,2022.\nURLhttps://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faf\nf6f588870935f114ebe04a3e5-Paper-Conference.pdf.\nG.Huang,Z.Liu,L.VanDerMaaten,andK.Q.Weinberger. Denselyconnectedconvolutional\nnetworks. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,\npages4700â€“4708,2017.\n16\nM.Joshi,E.Choi,D.Weld,andL.Zettlemoyer. TriviaQA:Alargescaledistantlysupervisedchal-\nlengedatasetforreadingcomprehension.InR.BarzilayandM.-Y.Kan,editors,Proceedingsof\nthe55thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: Long\nPapers), pages 1601â€“1611, Vancouver, Canada, July 2017. Association for Computational\nLinguistics. doi: 10.18653/v1/P17-1147. URLhttps://aclanthology.org/P17-1147.\nG.Larsson,M.Maire,andG.Shakhnarovich. Fractalnet: Ultra-deepneuralnetworkswithout",
    "char_length": 1463
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 31,
    "text": "residuals. arXivpreprintarXiv:1605.07648,2016.\nD.Lepikhin,H.Lee,Y.Xu,D.Chen,O.Firat,Y.Huang,M.Krikun,N.Shazeer,andZ.Chen.\nGshard: Scalinggiantmodelswithconditionalcomputationandautomaticsharding. arXiv\npreprintarXiv:2006.16668,2020.\nA. Liu, B. Feng, B. Wang, B. Wang, B. Liu, C. Zhao, C. Dengr, C. Ruan, D. Dai, D. Guo, et al.\nDeepseek-v2: Astrong,economical,andefficientmixture-of-expertslanguagemodel. arXiv\npreprintarXiv:2405.04434,2024a.\nA. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, et al.\nDeepseek-v3technicalreport. arXivpreprintarXiv:2412.19437,2024b.\nI. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101,2017.\nB.MakandJ.Flanigan. Residualmatrixtransformers: Scalingthesizeoftheresidualstream.\narXivpreprintarXiv:2506.22696,2025.\nG. Menghani, R. Kumar, and S. Kumar. LAurel: Learned augmented residual layer. In\nForty-second International Conference on Machine Learning, 2025. URL https://open\nreview.net/forum?id=rUDRWP9WvZ.\nM.Pagliardini,A.Mohtashami,F.Fleuret,andM.Jaggi. Denseformer: Enhancinginformation\nflowintransformersviadepthweightedaveraging. InTheThirty-eighthAnnualConference\nonNeuralInformationProcessingSystems,2024. URLhttps://openreview.net/forum\n?id=kMnoh7CXrq.\nP.Qi,X.Wan,G.Huang,andM.Lin. Zerobubble(almost)pipelineparallelism. InTheTwelfth\nInternational Conference on Learning Representations, 2024. URL https://openreview\n.net/forum?id=tuzTN0eIO5.",
    "char_length": 1464
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 32,
    "text": "N. Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint\narXiv:1911.02150,2019.\nN. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outra-\ngeouslylargeneuralnetworks: Thesparsely-gatedmixture-of-expertslayer. arXivpreprint\narXiv:1701.06538,2017.\nR.SinkhornandP.Knopp. Concerningnonnegativematricesanddoublystochasticmatrices.\nPacificJournalofMathematics,21(2):343â€“348,1967.\nR. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep networks. In C. Cortes,\nN.Lawrence,D.Lee,M.Sugiyama,andR.Garnett,editors,AdvancesinNeuralInformation\nProcessingSystems,volume28.CurranAssociates,Inc.,2015. URLhttps://proceedings.\nneurips.cc/paper_files/paper/2015/file/215a71a12769b056c3c32e7299f1c5e\nd-Paper.pdf.\n17\nJ.Su,M.Ahmed,Y.Lu,S.Pan,W.Bo,andY.Liu. Roformer: Enhancedtransformerwithrotary\npositionembedding. Neurocomputing,568:127063,2024.\nM.Suzgun,N.Scales,N.SchÃ¤rli,S.Gehrmann,Y.Tay,H.W.Chung,A.Chowdhery,Q.V.Le,\nE.H.Chi,D.Zhou,etal. Challengingbig-benchtasksandwhetherchain-of-thoughtcansolve\nthem. arXivpreprintarXiv:2210.09261,2022.\nH.Touvron,T.Lavril,G.Izacard,X.Martinet,M.-A.Lachaux,T.Lacroix,B.RoziÃ¨re,N.Goyal,\nE. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv\npreprintarXiv:2302.13971,2023.\nA.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,Å.Kaiser,andI.Polo-\nsukhin. Attention is all you need. Advances in neural information processing systems, 30,\n2017.",
    "char_length": 1469
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 33,
    "text": "L.Wang,H.Gao,C.Zhao,X.Sun,andD.Dai. Auxiliary-loss-freeloadbalancingstrategyfor\nmixture-of-experts. arXivpreprintarXiv:2408.15664,2024.\nL.Wang,Y.Cheng,Y.Shi,Z.Tang,Z.Mo,W.Xie,L.Ma,Y.Xia,J.Xue,F.Yang,etal. Tilelang: A\ncomposabletiledprogrammingmodelforaisystems. arXivpreprintarXiv:2504.17577,2025.\nD.Xiao,Q.Meng,S.Li,andX.Yuan.Muddformer: Breakingresidualbottlenecksintransformers\nviamultiwaydynamicdenseconnections. arXivpreprintarXiv:2502.12170,2025.\nS.Xie,R.Girshick,P.DollÃ¡r,Z.Tu,andK.He. Aggregatedresidualtransformationsfordeep\nneural networks. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition,pages1492â€“1500,2017.\nS.Xie,H.Zhang,J.Guo,X.Tan,J.Bian,H.H.Awadalla,A.Menezes,T.Qin,andR.Yan.Residual:\nTransformerwithdualresidualconnections,2023. URLhttps://arxiv.org/abs/2304.1\n4802.\nF. Yu, D. Wang, E. Shelhamer, and T. Darrell. Deep layer aggregation. In Proceedings of the\nIEEEconferenceoncomputervisionandpatternrecognition,pages2403â€“2412,2018.\nR.Zellers,A.Holtzman,Y.Bisk,A.Farhadi,andY.Choi. HellaSwag: Canamachinereallyfinish\nyoursentence? InA.Korhonen,D.R.Traum,andL.MÃ rquez,editors,Proceedingsofthe57th\nConferenceoftheAssociationforComputationalLinguistics,ACL2019,Florence,Italy,July\n28-August2,2019,Volume1: LongPapers,pages4791â€“4800.AssociationforComputational\nLinguistics,2019. doi: 10.18653/v1/p19-1472. URLhttps://doi.org/10.18653/v1/p1\n9-1472.\nB. Zhang and R. Sennrich. Root mean square layer normalization. Advances in neural",
    "char_length": 1476
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 34,
    "text": "informationprocessingsystems,32,2019.\nD.Zhu,H.Huang,Z.Huang,Y.Zeng,Y.Mao,B.Wu,Q.Min,andX.Zhou. Hyper-connections.\narXivpreprintarXiv:2409.19606,2024.\n18\nA. Appendix\nA.1. DetailedModelSpecificationsandHyper-parameters.\nTable5 | DetailedModelSpecificationsandHyper-parameters. Thistablepresentsthearchitec-\nturalconfigurationsforthe3B,9B,and27BmodelsbasedontheDeepSeek-V3(Liuetal.,2024b)\narchitecture. Itoutlinesthespecifichyper-parametersformHCandHC,includingtheresidual\nstreamexpansionandSinkhorn-Knoppsettings,alongsidetheoptimizationandtrainingproto-\ncolsusedintheexperiments.\n3B\nAttribute 3B 9B 27B\n1TTokens\nVocabParams 331M 496M 662M 331M\nActiveParams 612M 1.66B 4.14B 612M\nTotalParams 2.97B 9.18B 27.0B 2.97B\nLayers 12 18 30 12\nLeadingDenseLayers 1 1\nRoutedExperts 64 64 72 64\nActiveExperts 6 6\nSharedExperts 2 2\nDimension 1280 1920 2560 1280\nFFNDimension 896 1280 1536 896\nLoadBalancingMethod Loss-Free(Wangetal.,2024) Loss-Free\nAttentionHeads 16 24 32 16\nAttentionDimension 128 128\nAttentionVariant MLA(Liuetal.,2024a) MLA\nKVRank 512 512\nPositionEmbedding RoPE(Suetal.,2024) RoPE\nRoPEDimension 64 64\nRoPEğœƒ 10000 10000\nLayerNormType RMSNorm(ZhangandSennrich,2019) RMSNorm\nLayerNormğœ€ 1e-20 1e-20\nmHC/HCExpansionRateğ‘› 4 4\nmHC/HCGatingFactorInitğ›¼ 0.01 0.01\nmHCSinkhorn-Knoppğ‘¡ 20 20\nmax\nSequenceLength 4096 4096\nVocabSize 129280 129280\nBatchSize 320 512 1280 2560\nTrainingSteps 30000 50000 50000 100000\nTrainingTokens 39.3B 105B 262B 1.05T\nWarmupSteps 2000 2000",
    "char_length": 1464
  },
  {
    "paper_id": "2512.24880v1",
    "chunk_id": 35,
    "text": "Optimizer AdamW(LoshchilovandHutter,2017) AdamW\nAdamWBetas (0.9,0.95) (0.9,0.95)\nAdamWğœ€ 1e-20 1e-20\nBaseLearningRate 8.6e-4 5.9e-4 4.0e-4 9.0e-4\nLrScheduler Step Step\nLrDecayStepRatio [0.8Ã—,0.9Ã—] [0.8Ã—,0.9Ã—]\nLrDecayRate [0.316,0.1] [0.316,0.1]\nWeightDecay 0.1 0.1\n19",
    "char_length": 267
  }
]