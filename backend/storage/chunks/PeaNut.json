[
  {
    "paper_id": "PeaNut",
    "chunk_id": 0,
    "text": "PEANuT: Parameter-Efficient Adaptation with\nWeight-aware Neural Tweakers\nYiboZhong1,* HaoxiangJiang2,* LincanLi3 RyumeiNakada4\nTianciLiu5 LinjunZhang4 HuaxiuYao6 HaoyuWang2\n1IndependentResearcher 2UniversityatAlbany 3FloridaStateUniversity 4RutgersUniversity\n5PurdueUniversity 6UniversityofNorthCarolinaatChapelHill\nFine-tuninglargepre-trainedfoundationmodelsoftenyieldsexcellentdownstreamperformancebutis\nprohibitivelyexpensivewhenupdatingallparameters. Parameter-efficientfine-tuning(PEFT)methods\nsuchasLoRAalleviatethisbyintroducinglightweightupdatemodules,yettheycommonlyrelyonweight-\nagnosticlinearapproximations,limitingtheirexpressiveness. Inthiswork,wepropose PEANuT,anovel\nPEFTframeworkthatintroducesweight-awareneuraltweakers,compactneuralmodulesthatgenerate\ntask-adaptiveupdatesconditionedonfrozenpre-trainedweights.PEANuTprovidesaflexibleyetefficient\nwaytocapturecomplexupdatepatternswithoutfullmodeltuning. Wetheoreticallyshowthat PEANuT\nachievesequivalentorgreaterexpressivitythanexistinglinearPEFTmethodswithcomparableorfewer\nparameters. Extensiveexperimentsacrossfourbenchmarkswithovertwentydatasetsdemonstratethat\nPEANuT consistentlyoutperformsstrongbaselinesinbothNLPandvisiontasks,whilemaintaininglow\ncomputationaloverhead.\nKeywords: parameter-efficientfine-tuning,foundationmodel\nDate: 2025-11-25\nCodeRepository: https://github.com/yibozhong/peanut\nContact: yibozhong657@gmail.com;hjiang2@albany.edu;hwang28@albany.edu\n1. Introduction",
    "char_length": 1455
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 1,
    "text": "Pre-trained models, trained on large and diverse general-domain corpora, have demonstrated strong\ngeneralization capabilities across a variety of tasks, including natural language understanding (Devlin,\n2018,Liu,2019,HowardandRuder,2018,WuandHe,2019,Sunetal.,2023,Wangetal.,2021,2022),\ngeneration (Touvron et al., 2023, AI@Meta, 2024, Xu et al., 2025, Liu et al., 2025, Wang et al., 2024b,\nYaoetal.,2022,Lewisetal.,2020),andvisiontaskssuchasimageclassification(Dosovitskiyetal.,2020,\nBhojanapalli et al., 2021, Chen et al., 2021). A common strategy for adapting these models to specific\ndownstream tasks is full fine-tuning. However, due to the massive number of parameters involved, full\nfine-tuning often leads to significant computational and memory costs (Qin et al., 2024).\nTo mitigate these challenges, various parameter-efficient fine-tuning (PEFT) methods (Ding et al., 2023,\nHan et al., 2024) have been developed, enabling pre-trained models to be fine-tuned in resource-\nconstrained environments (Lin et al., 2024). These methods retain most of the pre-trained weights\nin a frozen state and introduce a small set of trainable components, thereby significantly reducing\nmemory and compute overhead (Lin et al., 2024). Among them, Low-Rank Adaptation (LoRA) (Hu\n‚àóTheseauthorscontributedequallytothiswork,orderwasdeterminedrandomly(byrollingadie).\n5202\nvoN\n42\n]GL.sc[\n3v07810.0142:viXra\nPEANuT:Parameter-EfficientAdaptationwithWeight-awareNeuralTweakers",
    "char_length": 1461
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 2,
    "text": "et al., 2021b, Liu et al., 2024, Song et al., 2024, B√ºy√ºkaky√ºz, 2024, Zhao et al., 2024) is a popular and\nwidely adopted approach due to its simplicity, strong empirical performance, and compatibility with\nmodern architectures.\nInstead of updating pre-trained model weight directly, LoRA introduces two learnable low-rank matrices\nfor it, and approximate weight updates through their product. Since the numbers of parameters of these\nlow-rank matrices are much smaller than that of the original pre-trained weights, LoRA significantly\nreduces the memory overhead during fine-tuning.\nDespiteitswidespreadsuccess,LoRAhasinherentlimitations,particularlyinitsabilitytomodelcomplex\nweight adaptation behaviors. LoRA approximates the weight change with the product of two low-rank\nmatrices. While recent studies have observed that the cumulative weight updates during fine-tuning\noften exhibit approximately low-rank structure (Zhao et al., 2024), LoRA itself learns these updates\nfrom scratch using randomly initialized parameters, without leveraging any prior knowledge from\nthe pre-trained weights. As a result, the optimization process becomes more challenging, especially\nunder low-rank settings where the parameter space is highly constrained and prone to suboptimal local\nminima (Pan et al., 2024). Furthermore, due to its linear structure, LoRA may struggle to capture\nintricate adaptation patterns required by many downstream tasks. To compensate for this limited",
    "char_length": 1467
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 3,
    "text": "capacity, LoRA-based methods often resort to increasing the rank of the update matrices, which in turn\nreduces their parameter efficiency and undermines their original motivation.\nTo overcome these limitations, we propose a parameter-efficient adaptation method with weight-aware\nneuraltweakers, PEANuT,whichincorporatesalightweightneuralnetwork,whichtakesthepre-traiend\nweightastheinput,intotheadaptationprocess. UnlikeLoRA,whichapproximatesweightupdateslinearly\nthrough low-rank decomposition, PEANuT models cumulative weight updates as explicit functions of\nthe pre-trained model‚Äôs original weights. This enables PEANuT to capture complex, non-linear patterns\nin the weight space, improving adaptation performance without increasing the number of parameters.\nThe key innovation in PEANuT lies in introducing compact neural networks, neural tweakers, that\ntransforms the pre-trainedweights, approximating theupdates with minimal additional computation.\nThis nonlinear transformation enhances the expressiveness of the parameter updates while maintaining\nthe efficiency. Importantly, this architecture facilitates a more efficient exploration of the optimization\nlandscape,leadingtobettertaskadaptation,particularlyincaseswherelinearmethodslikeLoRAwould\nrequire much larger ranks to achieve competitive results. We theoretically demonstrate that PEANuT\ncan achieve the same or greater expressivity than LoRA with fewer parameters.\nThe contributions are summarized as follows:",
    "char_length": 1477
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 4,
    "text": "‚Ä¢ We propose PEANuT, a new PEFT method that introduces weight-aware neural tweakers to generate\nadaptiveupdatesignals. Themethodenablesefficientandflexibleadaptationbeyondlinearconstraints.\nTo the best of our knowledge, this is the first work to introduce nonlinear adaptation for LoRA-based\nPEFT methods.\n‚Ä¢ TheproposedPEANuTenhancesmodelperformancewhilemaintainingtheefficiency. Wetheoretically\nshow that PEANuT can achieve a possibly improved parameter efficiency compared to LoRA.\n‚Ä¢ Weconductextensiveexperimentsonfourbenchmarkscoveringovertwentydatasets. Theexperiments\nshow that the proposed PEANuT can outperform baselines on both vision and text tasks.\n2\nPEANuT:Parameter-EfficientAdaptationwithWeight-awareNeuralTweakers\n2. Related Work\nInthissection,weprovideaconciseoverviewofrelatedworkonParameter-EfficientFine-Tuning(PEFT)\nmethods. PEFTmethodsaimtoreducethememoryoverheadoffine-tuningpre-trainedmodels,enabling\nfine-tuninginresource-constrainedenvironments. AccordingtoHanetal.(2024),PEFTmethodscanbe\ncategorized into: 1) Additive PEFT methods (Chronopoulou et al., 2023, Edalati et al., 2022, Lester\net al., 2021, Wang et al., 2024d, Liu et al., 2022), 2) Selective PEFT methods (Guo et al., 2020, Das\net al., 2023, Sung et al., 2021, Ansell et al., 2021, Zaken et al., 2021, Vucetic et al., 2022, Chen et al.,\n2024, Miao et al., 2025, Chen et al., 2025), 3) Reparameterized PEFT methods (Hu et al., 2021a,",
    "char_length": 1421
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 5,
    "text": "Valipour et al., 2022, Zhang et al., 2023, Karimi Mahabadi et al., 2021, Liu et al., 2024, Kopiczko et al.,\n2023), and 4) Hybrid PEFT methods (Mao et al., 2021, Chen et al., 2023, He et al., 2021, Zhang\net al., 2022, Zhou et al., 2024). Additive PEFT methods (Chronopoulou et al., 2023, Edalati et al., 2022,\nLester et al., 2021, Wang et al., 2024d, Liu et al., 2022) introduces a small set of additional trainable\nparameters strategically placed within the model. One of the most prominent additive PEFT approaches\nis Adapter (Chronopoulou et al., 2023, Edalati et al., 2022, Zhao et al., 2022), which involves inserting\nsmall adapter layers between pre-trained weight blocks. Prompt Tuning (Wang et al., 2024d, Lester\net al., 2021, Vu et al., 2021, Li and Liang, 2021) is another technique, where learnable vectors, or \"soft\nprompts,\" are prepended to the input sequence without modifying the model‚Äôs weights. This method is\nparticularlyeffectiveforlarge-scalemodelsandhasinspiredvariantssuchasPrefixTuning(LiandLiang,\n2021). Selective PEFT focuses on optimizing the fine-tuning process by selectively adjusting a subset of\nthe model‚Äôs parameters rather than introducing additional ones. For instance, Diff Pruning (Guo et al.,\n2020)usesalearnablebinarymasktoselectparametersforfine-tuning. Similarly,FishMask(Sungetal.,\n2021) and Fish-Dip (Das et al., 2023) leverage Fisher information to determine parameter importance",
    "char_length": 1423
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 6,
    "text": "andidentifythemostcrucialonesforupdates. Additionally,BitFit(Zakenetal.,2021)fine-tunesonlythe\nbiastermsinthemodel,significantlyreducingthenumberoftrainableparameters. HybridPEFT methods\naim to combine the strengths of various existing PEFT techniques to enhance model performance across\ndiverse tasks. UniPELT (Mao et al., 2021) integrates LoRA, prefix-tuning, and adapters within each\nTransformer block, employing a gating mechanism to determine which module should be active during\nfine-tuning. S4 (Chen et al., 2023) further explores the design space by partitioning layers into groups\nand assigning different PEFT methods to each group. Additionally, NOAH (Zhang et al., 2022) and\nAUTOPEFT (Zhou et al., 2024) leverage neural architecture search (NAS) to automatically discover\noptimal combinations of PEFT techniques tailored to specific tasks.\nReparameterized PEFT methods are most close to our proposed method. Low-Rank Adaptation (LoRA)-\nbased methods, which are representative of reparameterized PEFT approaches, have gained significant\nattention due to their minimal architectural changes, no additional inference costs, and high efficiency.\nLoRA (Hu et al., 2021a) introduces two trainable low-rank matrices for each pre-trained model weight\ntoapproximatethedesiredupdatesoftheoriginalmodel. ExtensionsofLoRAincludeDyLoRA(Valipour\netal.,2022),whichdynamicallyadjuststherankofthelow-rankmatricesduringtrainingtooptimizefor",
    "char_length": 1434
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 7,
    "text": "specific tasks; AdaLoRA (Zhang et al., 2023), which adaptively allocates the parameter budget among\nweight matrices based on their importance scores; and DoRA (Liu et al., 2024), which decomposes\nthe pre-trained weight into magnitude and direction, applying LoRA only for direction updates. Other\nvariants include VeRA (Kopiczko et al., 2023), which introduces shared frozen random matrices across\nlayers to improve efficiency further, and RoseLoRA (Wang et al., 2024c), which employs a row- and\ncolumn-wise sparse low-rank adaptation mechanism to selectively update the most significant param-\neters. FourierFT (Gao et al.) replaces the matrix multiplication in LoRA with a Fourier transform,\n3\nPEANuT:Parameter-EfficientAdaptationwithWeight-awareNeuralTweakers\n‚àô\nùëæùüé ‚àÜùëæ\nùë©\nùëæùüé ùë®\n: Parameters are frozen\n: Parameters are trainable LoRA PEANut\nùëæùüé: pre-trained model weight\nùëì(ùëæùüé;ùúΩ)\nùë©, ùë®: Introduced low-rank matrices\nFigure1:Frameworkofproposed PEANuT.\nwhile PiSSA (Meng et al., 2024) and MiLoRA (Wang et al., 2024a) update the principal and minor\nsingular components of the weight matrix, respectively. However, existing PEFT methods rely on linear\ntransformations to approximate pre-trained weight updates, which struggle to capture the complex\nrelationships inherent in weight updates, leading to a significant performance gap compared to full\nfine-tuning. Meanwhile, existing research like (Teney et al., 2024) also demonstrates that nonlinear",
    "char_length": 1444
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 8,
    "text": "activation is an integral part of the neural network driving its success.\n3. Methodology\nIn this section, we start with a brief introduction of LoRA. Motivated by a key limitation in LoRA\nparameter efficiency that roots from LoRA parameterization form, we propose PEANuT, a novel PEFT\nmethod to solve the issue. Notably, PEANuT is able to achieves better parameter efficiency provably.\n3.1. Preliminary\nLoRA (Hu et al., 2021a) assumes that the updates to model weights during the fine-tuning exhibit low-\nrankproperties. Builtuponthis,LoRAmodelstheincrementalupdateofsomeweightmatrixW0 ‚àà Rd 1 √ód2\nin a pre-trained model approximately by the product of two learnable low-rank matrices\nW = W0+‚àÜW = W0+AB,\nwhere A ‚àà Rd 1 √ór and B ‚àà Rr√ód2 with r ‚â™ min(d 1 ,d 2 ). When conducting fine-tuning, only introduced\ntwo low-rank matrices A and B will be updated and the pre-trained weight W0 is frozen, as represented\nby the following optimization\nmin\nA,B\nL(D train;W0+AB), (1)\nwhere D train is the training set used for fine-tuning and L is the loss function. Since A and B are both\nlow-rank matrices that contain significantly fewer parameters compared with the original W0, the LoRA\ncosts much less memory space compared to the fully fine-tuning.\n4\nPEANuT:Parameter-EfficientAdaptationwithWeight-awareNeuralTweakers\n3.2. Inherent Limitation of LoRA Formulation\nWhileLoRAfamilyhavedemonstratedremarkableparameterefficiencyinfine-tuningpre-trainedmodels",
    "char_length": 1444
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 9,
    "text": "fordiversedownstreamtasks,wearguethattheirproduct-basedformulationaresuboptimalforcapturing\nthe full fine-tuning dynamics in an efficient way.\nSpecifically, when fully fine-tuning a pre-trained model, the update process of weight W is typically\nperformed through an iterative gradient descent:\nW0 = W0 ‚àíŒ∑‚àá L,\nt t‚àí1 W0\nt‚àí1\nwhereW0 = W0 istheinitialstate, Œ∑ isthelearningrate,andW0 representstheweightsaftertiterations.\n0 t\nThe cumulative change in the weights over time can be represented as:\n‚àÜW = W0‚àíW0.\nt 0\nThisweightchange‚àÜWcanbeinterpretedasafunctionoftheoriginalpre-trainedweightsW0,capturing\nthe model‚Äôs adaptation to the specific task during fine-tuning.\nNonetheless, LoRA matrices A and B are parameterized in a free way without any dependency on\nW0. While gradient ‚àá L and ‚àá L are implicit functions of W0, making final learned A ,B indirectly\nA B t t\ndepends on W0 as well, as will be proved shortly, the lack of explicit dependency still makes LoRA\ninherently suboptimal for fine-tuning pre-trained models.\n3.3. Parameter-Efficient Adaptation with Weight-aware Neural Tweakers\nMotivatedbytheaboveanalysisonLoRA‚Äôslimitation,weproposetoapproximate‚àÜWusingalightweight\nneuralnetworkthatexplicitlytakespre-trainedmodelweightW0 asinputandoutputstheweightupdate\ndirectly. By doing so, our approach captures more complex and richer transformation of the weights\nin a more efficient manner. We refer to our method as parameter-efficient adaptation method with\nweight-aware neural tweakers (PEANuT).",
    "char_length": 1500
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 10,
    "text": "Following LoRA‚Äôs updates paradigm, the proposed PEANuT also provides incremental update of pre-\ntrained models. However, PEANuT modifies the forward pass of the model by introducing a dynamic\nnonlinear weight transformation. Specifically, the modified model‚Äôs forward propagation is formulated\nas:\ny = (W0+ f(W0;Œ∏))x.\nHere x and y are the input and output with respect to the current layer, respectively, and f(¬∑;Œ∏) :\nRd 1 √ód2 ‚Üí Rd 1 √ód2 is a nonlinear neural network parameterized by learnable parameter Œ∏. The neural\nnetwork f(W0;Œ∏) generates the weight update as a function of W0.\nTo ensure the parameter efficiency of our PEANuT, the learnable neural network f(W0;Œ∏) should be\nlightweight, i.e., the number of parameters Œ∏should be much fewer than that of the original pre-trained\nweightW0. Therefore,weparametrize f(W0;Œ∏)asaneuralnetworkwithbottlenecklayers. Forexample,\na simple case is f(W0;Œ∏) = œÉ(W0Œò\n1\n)Œò\n2\n, where Œ∏ = (Œò\n1\n, Œò\n2\n) ‚àà Rd2 √ór√óRr√ód2 with r ‚â™ min(d\n1\n,d\n2\n),\nand œÉ(¬∑) is some non-linear activation function such as ReLU (Glorot et al., 2011). We can also increase\nthe layers or add activation function for the output of f(W0;Œ∏) to enhance the model expressiveness.\n5\nPEANuT:Parameter-EfficientAdaptationwithWeight-awareNeuralTweakers\nDuring fine-tuning, the optimization objective is to minimize the task-specific loss function, which can\nbe represented as\nmin\nŒ∏\nL(D train;W0+ f(W0;Œ∏)),",
    "char_length": 1409
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 11,
    "text": "where the original pre-trained weight W0 is frozen, and only neural network parameters Œ∏ are updated.\nThe overview of PEANuT is shown in Fig. 1.\nRemark3.1. Thebenefitofournewformulationliesintwofolds. First,ourincrementalupdate f(W0;Œ∏)\nis an explicit function of W0, allowing it to capture updates in a more effective way. Second, the neural\nnetwork-based f(W0;Œ∏) allows for dynamic, non-linear weight updates that can capture more complex\ninteractions. These two advantages make PEANuT a more effective and efficient PEFT method than\nexisting LoRA-based approaches.\n3.4. Theoretical Analysis\nIn this section, we show the theoretical analysis of the sub-optimality of LoRA in terms of parameter\nefficiency. We prove that PEANuT can achieve equivalent or even superior efficiency under certain\nconditions. Specifically, suppose PEANuT adopts the following lightweight architecture, as described in\nSection 3.3:\nf(W0;Œ∏) = œÉ(W0Œò )Œò .\n1 2\nThe following proposition demonstrates that PEANuT can match the expressivity of LoRA using fewer\nparameters under specific conditions. Here, expressivity is measured by the minimum attainable loss.\nProposition 3.2. Given pre-trained weight matrix W0. Let œÉ denote ReLU activation function, andU0 ‚àà\nRd\n1\n√órank(W0) betheleftsingularvectorsofW0. Supposethatthefine-tuninglossLisinvariantunderthethe\nprojection of the weight matrix to the left singular space of W0, i.e., L(D train;W) = L(D train;U0U0‚ä§W)\nfor any W ‚àà Rd 1 √ód2 . Then, for any r ‚â• 1 ,",
    "char_length": 1482
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 12,
    "text": "min L(D train;W0+ f(W0;(Œò\n1\n, Œò\n2\n)))\nŒò\n1\n‚ààRd2√ó2r,\nŒò\n2\n‚ààR2r√ód2\n‚â§ min L(D train;W0+AB)\nA‚ààRd1√ór,\nB‚ààRr√ód2\n‚â§ min L(D train;W0+ f(W0;(Œò\n1\n, Œò\n2\n))).\nŒò\n1\n‚ààRd2√ór,\nŒò\n2\n‚ààRr√ód2\nIn words, Prop 3.2 demonstrates the (approximate) equivalence of LoRA and PEANuT in terms of their\nexpressivity. Specifically, the minimum attainable loss using rank-r LoRA can be achieved by PEANuT\nwith 2r hidden units, and conversely, the minimum attainable loss using PEANuT with r hidden units\ncan be achieved rank-r LoRA, provided the invariance assumption holds. This equivalence further\nimplies that the function classes realized by PEANuT withO(r) hidden dimensions and rank-r LoRA\nare equivalent in expressivity, as the result holds for any loss functions.\nImportantly,thishighlightsapotentialimprovementinparameterefficiencyby PEANuT.Namely, PEANuT\nwithO(rd ) parameters maintains the expressivity of LoRA with r(d +d ) parameters. That it to say,\n2 1 2\n6\nPEANuT:Parameter-EfficientAdaptationwithWeight-awareNeuralTweakers\nPEANuT offers a significant improvement in parameter efficiency when d ‚â™ d ( a condition that\n2 1\nwidely holds for the down projection matrix of transformers fully-connected layers (Vaswani, 2017,\nDosovitskiy et al., 2021) ). In such cases, PEANuT provably achieves better parameter efficiency than\nLoRA.Theaddedparameterefficiencycanalsoimprovesampleefficiencybyallowingthemodeltolearn\nrepresentations with the same or fewer data points.",
    "char_length": 1439
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 13,
    "text": "The invariance assumption in Proposition 3.2 pertains to the pre-trained model, and asserts that the\nlater layers of the model depends solely on the task-relevant feature space. Given that we fine-tune\na pre-trained model, the later layers are expected to capture this task-relevant feature space, which\nis described by the left singular space of W0. In practice, since the later layers primarily rely on this\npre-trained feature space, the principal directions of the pre-trained weight matrix, represented by its\nsingular vectors, encode most of the useful features for downstream tasks. This makes the loss largely\ninvariant to changes outside this subspace. The proof is available in Appendix B.1.\nIf we consider a sinusoid activation function œÉp(x) = sin(2œÄx), then stronger result that PEANuT\nhas expressivity (almost) greater than or equal to a LoRA with possibly more parameters can be\nestablished without the invariance assumption. We defer the result to the Appendix B.2.\n4. Complexity Analysis\nIn this section, we compare the computational and space complexity of PEANuT and LoRA.\nSpace Complexity. BecausewesettheintroducedparametersofLoRAand PEANuT tobethesame,we\nonlydiscussthespacecomplexityofthetraininginthissection. BothLoRAand PEANuT requirestoring\nthe added parameters and their gradients. PEANuT may incur a slightly higher activation memory\nduring backpropagation due to the extra nonlinearity, but our empirical results (see Sec. 5.4) show that",
    "char_length": 1468
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 14,
    "text": "this overhead is minimal and does not affect scalability in practice.\nComputational complexity. In terms of per-step computation cost, LoRA computes the residual update\nas ABx, which costs O(d r+rd ) per input vector x. PEANuT requires computing f(W ;Œ∏)x. When\n1 2 0\nusing the aforementioned example with one-hidden layer and having the same latent dimension as\nLoRA, the main cost is O(d d r). Although this complexity is higher than LoRA‚Äôs, both methods benefit\n1 2\nfrom highly matrix-friendly implementations. In practice, our experiments (see Sec. 5.4) show that the\nempirical training time per step is comparable. Importantly, during inference, both PEANuT and LoRA\nallowtheirupdatemodulestobemergedintotheoriginalweightmatrixW ,ensuringthatnoadditional\n0\nforward-pass cost is incurred in deployment.\n5. Experiment\nIn the experiments, we evaluate the proposed PEANuT and answer the following questions:\nRQ1 How does PEANuT compare to state-of-the-art PEFT methods on NLP and vision tasks?\nRQ2 What is the role of nonlinear approximation in the proposed PEANuT?\nRQ3 What is the real runtime and memory consumption of proposed PEANuT?\nRQ4 How does the performance of PEANuT vary with different fine-tuned modules, depths of the\nlightweight neural network, or non-linear activation functions?\n7\nPEANuT:Parameter-EfficientAdaptationwithWeight-awareNeuralTweakers\n5.1. Benchmarks and Experiment Setups\nWeexperimentPEANuTondatasetsfromfourrepresentativebenchmarks: 1)CommonsenseReasoning",
    "char_length": 1487
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 15,
    "text": "coversdiversemulti-choiceproblemsfromBoolQ(Clarketal.,2019),PIQA(Bisketal.,2020),SIQA(Sap\net al., 2019), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2019), ARC-e and ARC-\nc(Clarketal.,2018),andOpenBookQA(Mihaylovetal.,2018)datasets. FollowingWangetal.(2024a),\nwe finetune LLaMA2-7B (Touvron et al., 2023), LLaMA3-8B (AI@Meta, 2024) and Qwen3-8B (Yang\net al., 2025) on Commonsense170K (Hu et al., 2023) benchmark which combines all previous training\nsets, and evaluate the accuracy on their testing sets separately. 2) Arithmetic Understanding consists\nof two math reasoning datasets: GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021).\nWe finetune LLaMA2-7B (Touvron et al., 2023) and Qwen3-8B (Yang et al., 2025) on MetaMath (Yu\net al., 2023) dataset following Wang et al. (2024a). Models need to generate correct answers, and\naccuracy is used as the evaluation metric. 3) Natural Language Understanding consists of eight\ndatasets from the GLUE benchmark (Wang et al., 2018). We follow the evaluation metrics and setups\nfrom Gao et al. (2024), Wu et al. (2024b). 4) Image Classification consists of Oxford-Pets (Parkhi\net al., 2012), CIFAR10 (Krizhevsky, 2009), DTD (Cimpoi et al., 2014), EuroSAT (Helber et al., 2019),\nRESISC45 (Cheng et al., 2017), StanfordCars (Krause et al., 2013), FGVC (Maji et al., 2013), and\nCIFAR100(Krizhevsky,2009)followingGaoetal.(2024). Thefirstfivedatasetshavesmalllabelspaces,\nwhile the last three have large label spaces.",
    "char_length": 1486
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 16,
    "text": "Baselines methods are constructed on a task basis. Specifically, for each task, the proposed PEANuT is\ncomparedwithrepresentativebaselinesfromthecorrespondingdomain. ForbothCommonsenseReason-\ningandArithmeticUnderstanding,followingWangetal.(2024a),LoRA(Huetal.,2021b),PiSSA(Meng\netal.,2024)andMiLoRA(Wangetal.,2024a)areemployedasbaselines. PEANuT isappliedtoquery,\nkey, value, MLP up and MLP down layers. For Natural Language Understanding, we follow the\nsetup from prior works (Gao et al., 2024, Wu et al., 2024b) that evaluate various representative PEFT\nmethods, including LoRA (Hu et al., 2021b), Adapter Houlsby et al. (2019), BitFit (Zaken et al., 2021),\nRED (Wu et al., 2024a), DoRA (Liu et al., 2024), ReFT Wu et al. (2024b), and FourierFT (Gao et al.,\n2024). For Image Classification, we follow the setting of Gao et al. (2024) and take linear probing (LP),\nLoRA (Hu et al., 2021b) and FourierFT (Gao et al., 2024) as baselines. PEANuT is applied to the query\nand value layers. See our appendix for details about the datasets (App D) and hyper-parameters (App\nC).\n5.2. Performance Comparison\nWe showcase PEANuT performance on different tasks.\nCommonsense Reasoning. We experiment PEANuT with eight commonsense reasoning datasets to\naddressRQ1,resultsareshowninTab1. Wecomparetheperformanceofthreestate-of-the-artbaselines\nwith the proposed PEANuT, and PEANuT consistently outperforms all of them, achieving the highest",
    "char_length": 1428
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 17,
    "text": "accuracy on all tasks. Specifically, PEANuT surpasses LoRA, PiSSA, and MiLoRA in terms of average\naccuracyby4.6%,10%,and2.5%,respectively,whenusingLLaMA2-7Basthebackbone. OnLLaMA3-8B\nas the backbone, PEANuT demonstrates average improvements of 4.9%, 11.8%, and 2.9% over LoRA,\nPiSSA,andMiLoRA,respectively. WithQwen3-8Basthebackbone, PEANuT improvesaverageaccuracy\nover LoRA and MiLoRA by 3.9% and 2.4%, respectively. These results highlight the effectiveness and\nsuperiority of PEANuT as a PEFT method.\n8\nPEANuT:Parameter-EfficientAdaptationwithWeight-awareNeuralTweakers\nArithmetic Reasoning. In this section, we present results on two arithmetic reasoning tasks in Tab 4\nto help address RQ1. From the table, while full fine-tuning (FFT) achieves highest accuracy across the\ntwo datasets, the performance gap between the proposed PEANuT and FFT is very small, despite that\nPEANuT reliesonsignificantlyfewertrainableparameters. Moreover,comparedtostate-of-the-artPEFT\nbaselines, PEANuT achieves remarkable performance improvements. In terms of average accuracy,\nPEANuT demonstrates improvements of 7.5%, 12.4%, and 2.4% over LoRA, PiSSA, and MiLoRA,\nrespectively, when using LLaMA2-7B as the backbone. With Qwen3-8B as the backbone, PEANuT\nimproves average accuracy over LoRA and MiLoRA by 7.1% and 3.7%. These results on clearly confirm\nthat PEANuT is highly effective and efficient for complex reasoning tasks.\nNaturalLanguageUnderstanding. WefurtherconductexperimentsontheGLUEtoanswerRQ1,results",
    "char_length": 1500
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 18,
    "text": "are shown in Tab 3. From the table, PEANuT significantly outperforms state-of-the-art PEFT methods.\nSpecifically, PEANuT-S, which uses a similar number of trainable parameters as FourierFT (Gao et al.,\n2024), DiReFT (Wu et al., 2024b), and LoReFT (Wu et al., 2024b), surpasses all PEFT baselines and\nexperiences only a small performance drop (0.2%) compared to FFT. Additionally, PEANuT-L exceeds\nthe performance of all baselines, including FFT, with roughly the same number of trainable parameters\nas in LoRA. These results demonstrate that PEANuT exhibits excellent generalization ability while\nmaintaining great parameter efficiency.\nImage Classification. In this section, we conduct experiments on image classification tasks to address\nRQ2, PEANuT uses depth of 6, and results are shown in Tab 2. From the table, PEANuT significantly\noutperforms LoRA and FourierFT using the same number of trainable parameters. Specifically, PEANuT\nachieves performance improvements of 11.05%, 7.30%, and 26.02% compared to LoRA, FourierFT, and\nLP, respectively. Furthermore, compared to FFT, the proposed PEANuT shows negligible performance\ndrop (86.49% v.s. 86.34%), while using only 0.3% of the trainable parameters required by FFT. This\ndemonstrates that PEANuT exhibits exceptional adaptation capability not only on NLP tasks, but also\non vision tasks as well. Additionally, it verifies the effectiveness of the nonlinear adaptation used in\nPEANuT.\n5.3. Ablation Study",
    "char_length": 1462
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 19,
    "text": "In this section, in order to answer RQ2, we present an ablation study with two variants of LoRA\nto validate the effectiveness of our proposed framework: 1) nonlinear LoRA y = (W +œÉ(A)B)x,\n0\nand 2) multiplicative LoRA y = (W +W AB)x. Experiments are conducted on image classification\n0 0\nbenchmarks, and results are reported in Tab 5. According to the table, both nonlinear LoRA and\nmultiplicative LoRA perform worse than PEANuT. This highlights the effectiveness of incorporating\nnonlinear approximations and explicitly using model weights as input to the nonlinear function in\nPEANuT.\n5.4. Runtime and Memory Cost\nTo answer RQ3, we evaluate the computational efficiency of our proposed method, PEANuT, by\nmeasuring its runtime and memory consumption across three representative datasets: MRPC, SST-2,\nand Commonsense Reasoning. Table 7 summarizes the results, comparing PEANuT against the LoRA\napproach under identical settings. For a fair comparison, we ensure that the number of trainable\nparameters is matched between PEANuT and LoRA. All experiments are conducted on the same\nhardware setup using a single NVIDIA A100 GPU. As shown in Table 7, PEANuT exhibits comparable\n9\nPEANuT:Parameter-EfficientAdaptationwithWeight-awareNeuralTweakers\nTable 1: Common Reasoning performance of PEANuT and PEFT baselines on LLaMA 2-7B, LLaMA 3-8B and\nQwen3-8B.Resultsmarkedwith‚Äú+‚ÄùaretakenfromLiuetal.(2024),andthosemarkedwith‚Äú‚àó‚Äùaretakenfrom",
    "char_length": 1432
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 20,
    "text": "Wangetal.(2024a). Bestresultsareinbold. ‚ÄúAVG‚Äùmeanstheaverageaccuracyofalldatasets.\nAccuracy(‚Üë)\nModel PEFT\nBoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA AVG\nLoRA+ 69.8 79.9 79.5 83.6 82.6 79.8 64.7 81.0 77.6\nPiSSA* 67.6 78.1 78.4 76.6 78.0 75.8 60.2 75.6 73.8\nLLaMA2-7B\nMiLoRA* 67.6 83.8 80.1 88.2 82.0 82.8 68.8 80.6 79.2\nPEANuT 71.9 84.0 80.4 88.9 84.6 86.5 71.6 83.0 81.4\nLoRA+ 70.8 85.2 79.9 91.7 84.3 84.2 71.2 79.0 80.8\nPiSSA* 67.1 81.1 77.2 83.6 78.9 77.7 63.2 74.6 75.4\nLLaMA3-8B\nMiLoRA* 68.8 86.7 77.2 92.9 85.6 86.8 75.5 81.8 81.9\nPEANuT 72.1 87.0 80.9 94.3 86.7 91.4 78.9 84.8 84.5\nLoRA 86.3 87.2 84.1 92.5 81.5 89.6 78.8 89.5 86.2\nQwen3-8B MiLoRA 85.2 89.3 84.2 94.6 82.2 92.3 82.7 89.5 87.5\nPEANuT 89.4 90.2 87.4 95.7 85.5 92.7 82.6 93.6 89.6\nTable2:ImageClassificationperformanceonViT-base. Bestresultsareinbold. ‚ÄúAVG‚Äùmeanstheaverageaccuracy\nofalldatasets. Resultsmarkedwith‚Äú‚àó‚ÄùaretakenfromGaoetal.(2024).\nMethod Params(M) OxfordPets StanfordCars CIFAR10 DTD EuroSAT FGVC RESISC45 CIFAR100 AVG\nFFT‚àó 85.8M 93.14 79.78 98.92 77.68 99.05 54.84 96.13 92.38 86.49\nLP‚àó - 90.28 25.76 96.41 69.77 88.72 17.44 74.22 84.28 68.36\nLoRA‚àó 581K 93.19 45.38 98.78 74.95 98.44 25.16 92.70 92.02 77.58\nFourierFT‚àó 239K 93.05 56.36 98.69 77.30 98.78 32.44 94.26 91.45 80.29\nPEANuT 263K 93.62 80.21 98.78 79.61 98.85 52.93 94.71 92.02 86.34\nruntime and memory usage to LoRA across all tasks. On the MRPC and SST-2 datasets, PEANuT",
    "char_length": 1430
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 21,
    "text": "incurs only marginal overhead in training time, with identical memory consumption. For the larger\nCommonsenseReasoningdataset, PEANuT takes5.7hoursand23.8GBofmemory,comparedtoLoRA‚Äôs\n5.6 hours and 22.7GB. The slightly higher memory usage is attributed to the additional nonlinear\ntransformationmodulein PEANuT,buttheincreaseremainsslight. Overall,theresultsdemonstratethat\nPEANuT achievesimprovedperformance(asdiscussedinearliersections)withnegligibleadditionalcost\nin training runtime and memory, highlighting its practicality and scalability for real-world deployment.\n5.5. Sensitivity w.r.t. Depth\nTo answer RQ4, we analyze the impact of depth on the performance of PEANuT. Deeper architectures\nare generally more expressive and can better model the complex, nonlinear relationships involved in\nideal weight updates (Raghu et al., 2017). We evaluate PEANuT with varying depth across NLU, vision,\ncommonsense reasoning, and arithmetic reasoning tasks.\n10\nPEANuT:Parameter-EfficientAdaptationwithWeight-awareNeuralTweakers\nTable 3: GLUE benchmark performance on RoBERTa-base. Results marked with ‚Äú‚àó‚Äù are taken from Wu et al.\n(2024a). Bestresultsareinbold. ‚ÄúAVG‚Äùmeanstheaverageaccuracyofalldatasets. PEANuT-Sappliestrainable\nmodules to layers starting from the 4th layer, with hidden dimensions set to 1. This matches the parameter\nnumbersofFourierFT. PEANuT-Lapplies PEANuT toalllayerswithhiddendimension8,aligningtheparameter\nbudgetofLoRA.\nAccuracy (‚Üë)\nPEFT Params(%)",
    "char_length": 1469
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 22,
    "text": "MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B AVG\nFFT 100% 87.3 94.4 87.9 62.4 92.5 91.7 78.3 90.6 85.6\nAdapter‚àó 0.318% 87.0 93.3 88.4 60.9 92.5 90.5 76.5 90.5 85.0\nLoRA‚àó 0.239% 86.6 93.9 88.7 59.7 92.6 90.4 75.3 90.3 84.7\nAdapterFNN‚àó 0.239% 87.1 93.0 88.8 58.5 92.0 90.2 77.7 90.4 84.7\nBitFit‚àó 0.080% 84.7 94.0 88.0 54.0 91.0 87.3 69.8 89.5 82.3\nRED‚àó 0.016% 83.9 93.9 89.2 61.0 90.7 87.2 78.0 90.4 84.3\nFourierFT 0.019% 84.7 94.2 90.0 63.8 92.2 88.0 79.1 90.8 85.3\nDiReFT‚àó 0.015% 82.5 92.6 88.3 58.6 91.3 86.4 76.4 89.3 83.2\nLoReFT‚àó 0.015% 83.1 93.4 89.2 60.4 91.2 87.4 79.0 90.0 84.2\nPEANuT-S 0.019% 84.9 94.3 90.2 64.6 92.0 88.3 78.3 90.5 85.4\nPEANuT-L 0.241% 86.9 95.2 90.0 64.8 92.3 90.3 82.7 90.7 86.6\nTable4:ArithmeticReasoningperformanceonLLaMA2-7BandQwen3-8B.Resultsmarkedwith‚Äú+‚Äùaretaken\nfromYuetal.(2023),andthosemarkedwith‚Äú‚àó‚ÄùaretakenfromWangetal.(2024a). Bestresultsareinbold.\n‚ÄúAVG‚Äùmeanstheaverageaccuracyofalldatasets.\nModel Method GSM8K MATH AVG\nFFT + 66.50 19.80 43.20\nLoRA* 60.58 16.88 38.73\nLLaMA2-7B\nPiSSA* 58.23 15.84 37.04\nMiLoRA* 63.53 17.76 40.65\nPEANuT 65.05 18.30 41.68\nLoRA 85.22 67.26 76.24\nQwen3-8B MiLoRA 89.01 68.58 78.80\nPEANuT 92.87 70.50 81.69\nTable 5: Ablation Study on image classification task. The parameters count is the same and ‚ÄúAVG‚Äù means the\naverageaccuracyofalldatasets. Forsimpleandfaircomparison, PEANuT usesdepthof2.\nMethod OxfordPets StanfordCars CIFAR10 DTD EuroSAT FGVC RESISC45 CIFAR100 AVG\nNonlinearLoRA 94.11 72.84 98.68 79.16 98.61 39.33 93.79 92.38 83.31",
    "char_length": 1501
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 23,
    "text": "MultiplicativeLoRA 93.57 77.32 98.68 77.57 98.81 46.79 94.34 91.86 84.81\nPEANuT 93.77 80.03 98.70 77.57 98.79 53.60 94.27 92.47 86.15\nWeincreasethenumberofintermediatelayersinsertedbetween PEANuT‚Äôsinputandoutputprojections.\nEach intermediate layer is a small feedforward block of shape Rr√ór with non-linear activations. These\n11\nPEANuT:Parameter-EfficientAdaptationwithWeight-awareNeuralTweakers\nTable6:Accuracycomparisonof PEANuT usingRoBERTa-basewithdifferentdepthconfigurationsontheGLUE\nbenchmark. Thehighestaccuracyofmethodspercategoryareinbold. ‚ÄúAVG‚Äùmeanstheaverageaccuracyofall\ndatasets.\nAccuracy (‚Üë)\ndepth Params(%)\nMNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B AVG\n2 0.239% 86.6 94.6 90.0 64.4 92.7 89.7 78.7 90.9 86.0\n4 0.239% 86.7 94.5 90.2 65.1 92.4 90.5 80.5 90.8 86.3\n6 0.241% 86.9 95.2 90.0 64.8 92.3 90.3 82.7 90.7 86.6\nTable7:Runtimeandmemoryconsumptionofproposed PEANuT.\nDataset Method Time Memory\nLoRA 77.7s 6916MB\nMRPC\nPEANuT 78.4s 6916MB\nLoRA 870.7s 2410MB\nSST-2\nPEANuT 911.4s 2410MB\nCommonsense LoRA 5.6h 22.7GB\nReasoning PEANuT 5.7h 23.8GB\nlayers are lightweight compared to the input/output projections (A ‚àà Rd2 √ór, B ‚àà Rr√ód2 ), and add\nminimal overhead since r ‚â™ d . The adaptation starts from the frozen base weight W0, which is\n2\ntransformedthroughmultiplelayerstopredict‚àÜW. Weadoptresidualconnectionsforstableoptimization\nand improved convergence. All other hyperparameters are kept fixed during this analysis. The layer\nstructure is illustrated in Fig. 2.",
    "char_length": 1478
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 24,
    "text": "The results in Table 6 (GLUE) and Fig. 3 (RTE, Cars, PIQA, MATH) indicate that increasing depth\nconsistently improves accuracy. For instance, average GLUE accuracy increases from 86.0 to 86.6 when\nmovingfrom2to6layers,withnosignificantchangeinparametercount. Onotherbenchmarks,deeper\nconfigurations yield steady gains up to 6 layers. Beyond this, performance may slightly drop (e.g., at\ndepth 10), likely due to optimization difficulties without fine-grained hyperparameter tuning.\nIn summary, depth enhances PEANuT‚Äôs effectiveness across tasks, offering better adaptation capability\nwith negligible cost in memory or parameters. However, very deep settings may require further tuning\nto maintain stability.\n5.6. Sensitivity w.r.t. Activations\nOne key innovation of PEANuT compared to LoRA and other PEFT methods, which rely solely on linear\ntransformations for modeling weight updates, is the introduction of non-linear activations within the\nadaptationneuralnetwork. Sincethechoiceofnon-linearactivationsdirectlyaffectsthelearningprocess\n12\nPEANuT:Parameter-EfficientAdaptationwithWeight-awareNeuralTweakers\nNEAT Output Layer\nŒîW\nIntermediate n\nIntermediate n-1\nW\nVarious\nL\nayers\nIntermediate 2\nW0\nIntermediate 1\nW0\nInput Layer\nAdaptation Process Target Modules\nFigure2:Implementationofintroducingmoredepthsto PEANuTt. Weinsertmultipleintermediatelayersinto\nthelayersfromvanilla PEANuT,withnon-linearactivationinbetween. Thedepthisdescribedasthenumberof",
    "char_length": 1455
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 25,
    "text": "layersin PEANuT,withvanilla PEANuT havingadepthof2(i.e. theinputandoutputlayers).\nand the dynamics of weight updates, we investigate how different non-linear activations affects the\nadaptation performance to address RQ4. To this end, we perform experiments on the StanfordCars\nbenchmarkusingvariousnon-linearactivations,includingReLU,LeakyReLU,GELU,Tanh,andsinusoidal\nactivation( œÉp(x) = sin(2œÄx)). CorrespondingresultsarepresentedinFig4. Toensureafaircomparison,\nthe number of trainable parameters is fixed. We optimize other hyperparameters such as learning rate\nfor better performance.\nFrom the figure, the best performance achieved by different activation functions is similar, indicating\nthattheadaptationpotentialofvariousactivationsiscomparable. Thisimpliesthat PEANuT canbenefit\nfrom various type of nonlinearity induced by different activations. However, it is also worth noting that\nsinusoidal activations encounters a performance drop at large learning rates. Consequently, tuning basic\nhyperparameterssuchaslearningratecanstillbebeneficial. Inconclusion,wesuggestReLUasadefault\nchoice in execution, given its practical simplicity (Teney et al., 2024).\n6. Sensitivity w.r.t. Fine-tuned Module\nWe end up this section with a study on applying PEANuT to different modules in a ViT, to help better\nunderstand RQ4.\nSpecifically, given the importance of MLP in Transformer architecture, we compare two settings: 1)",
    "char_length": 1420
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 26,
    "text": "Following Hu et al. (2021b), we apply PEANuT to the query and value layers (QV layers) in the multi-\nhead self-attention module (MHSA) in ViT. 2) Besides QV layers, we also apply PEANuT to MLP layers.\nWe tune the hidden dimension r to ensure the same parameter scale for fair comparison, and tune the\nhyperparameters to maximize performance. Corresponding results are shown in Fig. 5.\nFrom the figure, applying PEANuT to the QV layers yields results comparable to applying PEANuT\n13\nPEANuT:Parameter-EfficientAdaptationwithWeight-awareNeuralTweakers\n0.830\n0.820\n0.810\n0.800\n0.790\n0.780\n2 4 6\nDepth\nycaruccA\nRTE Cars PIQA MATH\n0.1830\n0.8400\n0.804\n0.8398 0.1828\n0.802 0.8396 0.1826\n0.8394 0.1824\n0.800 0.8392 0.1822\n0.8390 0.1820\n0.798\n2 4 6 2 4 6 2 4 6\nDepth Depth Depth\nFigure3:AccuracyontheRTE,StanfordCars,PIQAandMATHdatasetwithvaryingdepthsoftheneuralnetwork\nusedin PEANuT.Thedepthhererepresentsthetotalnumberoflayersintheneuralnetwork. Wechoosedepth\nequalsto2,4and6layersinthefigure.\n0.80\n0.79\n0.78\n0.77\n0.76\n0.75\n1 3 5 8 10\nLearning Rate (√ó10 3)\nycaruccA\nGELU Tanh Leaky ReLU ReLU Sinusoidal\n0.80 0.80 0.80 0.8\n0.79\n0.79 0.6\n0.79 0.78\n0.78\n0.78 0.77 0.4 0.77\n0.76\n0.77 0.76 0.2\n0.75\n0.76 0.75 0.0\n1 3 5 8 10 1 3 5 8 10 1 3 5 8 10 1 3 5 8 10\nLearning Rate (√ó10 3) Learning Rate (√ó10 3) Learning Rate (√ó10 3) Learning Rate (√ó10 3)\nFigure4:Influenceofdifferentnonlinearactivationschoicesfor PEANuT.ExperimentsareconductedonStan-",
    "char_length": 1431
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 27,
    "text": "fordCars,PEANuTdepthisfixedto2. Differentactivationsshareasimilarpatternofdependencyonlearningrate.\n100\n80\n60\n40\n20\n0\nPets Cars Cifar10 Dtd Eurosat Fgvc Resisc45 Cifar100 Average\n)%(\nycaruccA\nQV-MLP QV\nFigure 5: Accuracy of PEANuT with different targeted fine-tuning modules, including just QV layers and a\ncombinationofQVandMLPlayers,onimageclassificationdatasets.\nto both the QV and MLP layers. This indicates that PEANuT is robust to the selections of fine-tuning\ndifferentmodules. Thisfindingconfirmsanotherkeyadvantageof PEANuT:itdoesnotrequireextensive\nmanual tuning on which parts (modules, layers) of the foundation model PEANuT should be applied.\nConsequently, PEANuT can be easily incorporated to a wide range of scenarios.\n14\nPEANuT:Parameter-EfficientAdaptationwithWeight-awareNeuralTweakers\n7. Conclusion\nInthiswork,weproposePEANuT,anovelparameter-efficientfine-tuning(PEFT)methodthatintroduces\nnonlinear transformations to enhance model adaptation while maintaining efficiency. By incorporating\na lightweight neural network that models cumulative weight updates as functions of the pre-trained\nweights, PEANuT effectively captures complex, nonlinear structures in the weight space, allowing for\nmore expressive and accurate adaptation to downstream tasks. Our theoretical analysis supports the\nefficacyof PEANuT,demonstratingthatitcanachievegreaterorequivalentexpressivenesscomparedto\nexisting LoRA, a popular and state-of-the-art PEFT method, with fewer number of parameters. Through",
    "char_length": 1499
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 28,
    "text": "extensive experiments on four benchmarks encompassing over twenty datasets with various pre-trained\nbackbones, PEANuT demonstrated superior performance on both NLP and vision tasks compared to\nexisting state-of-the-art methods.\n15\nPEANuT:Parameter-EfficientAdaptationwithWeight-awareNeuralTweakers\nReferences\nAI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/blob/\nmain/MODEL_CARD.md.\nAlan Ansell, Edoardo Maria Ponti, Anna Korhonen, and Ivan Vuliƒá. Composable sparse fine-tuning for\ncross-lingual transfer. arXiv preprint arXiv:2110.07560, 2021.\nTom M Apostol. Modular functions and dirichlet series in number theory. 1990.\nSrinadh Bhojanapalli, Ayan Chakrabarti, Daniel Glasner, Daliang Li, Thomas Unterthiner, and Andreas\nVeit. Understandingrobustnessoftransformersforimageclassification. InProceedingsoftheIEEE/CVF\ninternational conference on computer vision, pages 10231‚Äì10241, 2021.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about\nphysical commonsense in natural language. arXiv preprint arXiv:1911.11641, 2020.\nKerim B√ºy√ºkaky√ºz. Olora: Orthonormal low-rank adaptation of large language models. arXiv preprint\narXiv:2406.01775, 2024.\nChun-Fu Richard Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-scale vision\ntransformerforimageclassification.InProceedingsoftheIEEE/CVFinternationalconferenceoncomputer\nvision, pages 357‚Äì366, 2021.",
    "char_length": 1436
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 29,
    "text": "JiaaoChen,AstonZhang,XingjianShi,MuLi,AlexSmola,andDiyiYang. Parameter-efficientfine-tuning\ndesign spaces. arXiv preprint arXiv:2301.01821, 2023.\nWei Chen, Zichen Miao, and Qiang Qiu. Large convolutional model tuning via filter subspace. arXiv\npreprint arXiv:2403.00269, 2024.\nWei Chen, Jingxi Yu, Zichen Miao, and Qiang Qiu. Sparse fine-tuning of transformers for generative\ntasks. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages18703‚Äì18713,\n2025.\nGong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark\nand state of the art. Proceedings of the IEEE, 105(10):1865‚Äì1883, 2017.\nAlexandra Chronopoulou, Matthew E Peters, Alexander Fraser, and Jesse Dodge. Adaptersoup: Weight\naveraging to improve generalization of pretrained language models. arXiv preprint arXiv:2302.07027,\n2023.\nMircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing\ntextures in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 3606‚Äì3613, 2014.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\nToutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint\narXiv:1905.10044, 2019.\nPeter Clark, Isaac Cowhey, OrenEtzioni, Tushar Khot, AshishSabharwal, Carissa Schoenick, and Oyvind\nTafjord. Thinkyouhavesolvedquestionanswering? tryarc,theai2reasoningchallenge. arXivpreprint\narXiv:1803.05457, 2018.\n16",
    "char_length": 1501
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 30,
    "text": "PEANuT:Parameter-EfficientAdaptationwithWeight-awareNeuralTweakers\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.\nTraining verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\nSarkar Snigdha Sarathi Das, Ranran Haoran Zhang, Peng Shi, Wenpeng Yin, and Rui Zhang. Uni-\nfied low-resource sequence labeling by sample-aware dynamic sparse finetuning. arXiv preprint\narXiv:2311.03748, 2023.\nJacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv\npreprint arXiv:1810.04805, 2018.\nNing Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen,\nChi-Min Chan, Weize Chen, et al. Parameter-efficient fine-tuning of large-scale pre-trained language\nmodels. Nature Machine Intelligence, 5(3):220‚Äì235, 2023.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-\nterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,etal. Animageisworth\n16x16 words: Transformers for image recognition at scale. In International Conference on Learning\nRepresentations, 2020.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-\nterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and",
    "char_length": 1450
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 31,
    "text": "Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In Inter-\nnational Conference on Learning Representations, 2021. URL https://openreview.net/forum?\nid=YicbFdNTTy.\nAli Edalati, Marzieh Tahaei, Ivan Kobyzev, Vahid Partovi Nia, James J Clark, and Mehdi Rezagholizadeh.\nKrona: Parameter efficient tuning with kronecker adapter. arXiv preprint arXiv:2212.10650, 2022.\nZiqi Gao, Qichao Wang, Aochuan Chen, Zijing Liu, Bingzhe Wu, Liang Chen, and Jia Li. Parameter-\nefficient fine-tuning with discrete fourier transform. In Forty-first International Conference on Machine\nLearning.\nZiqi Gao, Qichao Wang, Aochuan Chen, Zijing Liu, Bingzhe Wu, Liang Chen, and Jia Li. Parameter-\nefficient fine-tuning with discrete fourier transform. arXiv preprint arXiv:2405.03003, 2024.\nMichaelSGashlerandStephenCAshmore. Trainingdeepfourierneuralnetworkstofittime-seriesdata.\nIn Intelligent Computing in Bioinformatics: 10th International Conference, ICIC 2014, Taiyuan, China,\nAugust 3-6, 2014. Proceedings 10, pages 48‚Äì55. Springer, 2014.\nXavierGlorot,AntoineBordes,andYoshuaBengio. Deepsparserectifierneuralnetworks. InProceedings\nof the fourteenth international conference on artificial intelligence and statistics, pages 315‚Äì323. JMLR\nWorkshop and Conference Proceedings, 2011.\nDemi Guo, Alexander M Rush, and Yoon Kim. Parameter-efficient transfer learning with diff pruning.\narXiv preprint arXiv:2012.07463, 2020.",
    "char_length": 1441
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 32,
    "text": "ZeyuHan,ChaoGao,JinyangLiu,SaiQianZhang,etal. Parameter-efficientfine-tuningforlargemodels:\nA comprehensive survey. arXiv preprint arXiv:2403.14608, 2024.\n17\nPEANuT:Parameter-EfficientAdaptationwithWeight-awareNeuralTweakers\nJunxianHe,ChuntingZhou,XuezheMa,TaylorBerg-Kirkpatrick,andGrahamNeubig. Towardsaunified\nview of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366, 2021.\nPatrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and\ndeep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in\nApplied Earth Observations and Remote Sensing, 12(7):2217‚Äì2226, 2019.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint\narXiv:2103.03874, 2021.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In\nInternational conference on machine learning, pages 2790‚Äì2799. PMLR, 2019.\nJeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. arXiv\npreprint arXiv:1801.06146, 2018.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,\n2021a.",
    "char_length": 1494
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 33,
    "text": "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,\n2021b.\nZhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria,\nand Roy Lee. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language\nmodels. arXiv preprint arXiv:2304.01933, 2023.\nRabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient low-rank\nhypercomplex adapter layers. Advances in Neural Information Processing Systems, 34:1022‚Äì1035,\n2021.\nDawid Jan Kopiczko, Tijmen Blankevoort, and Yuki Markus Asano. Vera: Vector-based random matrix\nadaptation. arXiv preprint arXiv:2310.11454, 2023.\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained\ncategorization. In Proceedings of the IEEE international conference on computer vision workshops, pages\n554‚Äì561, 2013.\nA Krizhevsky. Learning multiple layers of features from tiny images. Master‚Äôs thesis, University of Tront,\n2009.\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\ntuning. arXiv preprint arXiv:2104.08691, 2021.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, et al. Retrieval-augmented generation for",
    "char_length": 1459
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 34,
    "text": "knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:9459‚Äì9474,\n2020.\n18\nPEANuT:Parameter-EfficientAdaptationwithWeight-awareNeuralTweakers\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv\npreprint arXiv:2101.00190, 2021.\nJi Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao,\nXingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for on-device\nllm compression and acceleration. Proceedings of Machine Learning and Systems, 6:87‚Äì100, 2024.\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A\nRaffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning.\nAdvances in Neural Information Processing Systems, 35:1950‚Äì1965, 2022.\nShih-YangLiu,Chien-YiWang,HongxuYin,PavloMolchanov,Yu-ChiangFrankWang,Kwang-TingCheng,\nandMin-HungChen. DoRA:Weight-decomposedlow-rankadaptation. arXiv:2402.09353, 2024. URL\nhttps://arxiv.org/abs/2402.09353.\nTianci Liu, Haoxiang Jiang, Tianze Wang, Ran Xu, Yue Yu, Linjun Zhang, Tuo Zhao, and Haoyu Wang.\nRoserag: Robust retrieval-augmented generation with small-scale llmsvia margin-aware preference\noptimization. arXiv preprint arXiv:2502.10993, 2025.\nYinhan Liu. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692,\n2019.",
    "char_length": 1410
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 35,
    "text": "Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual\nclassification of aircraft. arXiv preprint arXiv:1306.5151, 2013.\nYuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Wen-tau Yih, and\nMadian Khabsa. Unipelt: A unified framework for parameter-efficient language model tuning. arXiv\npreprint arXiv:2110.07577, 2021.\nFanxu Meng, Zhaohui Wang, and Muhan Zhang. Pissa: Principal singular values and singular vectors\nadaptation of large language models. arXiv preprint arXiv:2404.02948, 2024.\nZichen Miao, Wei Chen, and Qiang Qiu. Coeff-tuning: A graph filter subspace view for tuning attention-\nbased large models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages\n20146‚Äì20157, 2025.\nTodorMihaylov,PeterClark,TusharKhot,andAshishSabharwal. Canasuitofarmorconductelectricity?\na new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.\nRui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang, Chi Han, and Tong Zhang. Lisa: Layer-\nwise importance sampling for memory-efficient large language model fine-tuning. arXiv preprint\narXiv:2403.17919, 2024.\nOmkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012 IEEE\nconference on computer vision and pattern recognition, pages 3498‚Äì3505. IEEE, 2012.\nRuiyang Qin, Dancheng Liu, Zheyu Yan, Zhaoxuan Tan, Zixuan Pan, Zhenge Jia, Meng Jiang, Ahmed",
    "char_length": 1447
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 36,
    "text": "Abbasi, Jinjun Xiong, and Yiyu Shi. Empirical guidelines for deploying llms onto resource-constrained\nedge devices. arXiv preprint arXiv:2406.03777, 2024.\n19\nPEANuT:Parameter-EfficientAdaptationwithWeight-awareNeuralTweakers\nMaithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the expressive\npower of deep neural networks. In international conference on machine learning, pages 2847‚Äì2854.\nPMLR, 2017.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial\nwinograd schema challenge at scale. arXiv preprint arXiv:1907.10641, 2019.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense\nreasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019.\nLinSong,YukangChen,ShuaiYang,XiaohanDing,YixiaoGe,Ying-CongChen,andYingShan. Low-rank\napproximation for sparse attention in multi-modal llms. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 13763‚Äì13773, 2024.\nXiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei Guo, Tianwei Zhang, and Guoyin Wang. Text\nclassification via large language models. arXiv preprint arXiv:2305.08377, 2023.\nYi-LinSung,VarunNair,andColinARaffel. Trainingneuralnetworkswithfixedsparsemasks. Advances\nin Neural Information Processing Systems, 34:24193‚Äì24205, 2021.\nDamien Teney, Armand Mihai Nicolicioiu, Valentin Hartmann, and Ehsan Abbasnejad. Neural redshift:",
    "char_length": 1458
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 37,
    "text": "Random networks are not random functions. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 4786‚Äì4796, 2024.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, and et al. Llama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023.\nMojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, and Ali Ghodsi. Dylora: Parameter effi-\ncient tuning of pre-trained models using dynamic search-free low-rank adaptation. arXiv preprint\narXiv:2210.07558, 2022.\nA Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017.\nTuVu,BrianLester,NoahConstant,RamiAl-Rfou,andDanielCer. Spot: Betterfrozenmodeladaptation\nthrough soft prompt transfer. arXiv preprint arXiv:2110.07904, 2021.\nDanilo Vucetic, Mohammadreza Tayaranian, Maryam Ziaeefard, James J Clark, Brett H Meyer, and\nWarrenJGross. Efficientfine-tuningofbertmodelsontheedge. In2022IEEEInternationalSymposium\non Circuits and Systems (ISCAS), pages 1838‚Äì1842, 2022.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue:\nA multi-task benchmark and analysis platform for natural language understanding. arXiv preprint\narXiv:1804.07461, 2018.\nHanqing Wang, Zeguan Xiao, Yixia Li, Shuo Wang, Guanhua Chen, and Yun Chen. Milora: Harnessing\nminor singular components for parameter-efficient llm finetuning. arXiv preprint arXiv:2406.09044,\n2024a.",
    "char_length": 1498
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 38,
    "text": "Haoyu Wang, Fenglong Ma, Yaqing Wang, and Jing Gao. Knowledge-guided paraphrase identification.\nIn Findings of the Association for Computational Linguistics: EMNLP 2021, pages 843‚Äì853, 2021.\n20\nPEANuT:Parameter-EfficientAdaptationwithWeight-awareNeuralTweakers\nHaoyu Wang, Handong Zhao, Yaqing Wang, Tong Yu, Jiuxiang Gu, and Jing Gao. Fedkc: Federated\nknowledge composition for multilingual natural language understanding. In Proceedings of the ACM\nWeb Conference 2022, pages 1839‚Äì1850, 2022.\nHaoyu Wang, Ruirui Li, Haoming Jiang, Jinjin Tian, Zhengyang Wang, Chen Luo, Xianfeng Tang,\nMonica Xiao Cheng, Tuo Zhao, and Jing Gao. Blendfilter: Advancing retrieval-augmented large\nlanguage models via query generation blending and knowledge filtering. In Proceedings of the 2024\nConference on Empirical Methods in Natural Language Processing, pages 1009‚Äì1025, 2024b.\nHaoyu Wang, Tianci Liu, Tuo Zhao, and Jing Gao. Roselora: Row and column-wise sparse low-rank\nadaptation of pre-trained language model for knowledge editing and fine-tuning. arXiv preprint\narXiv:2406.10777, 2024c.\nYihan Wang, Jatin Chauhan, Wei Wang, and Cho-Jui Hsieh. Universality and limitations of prompt\ntuning. Advances in Neural Information Processing Systems, 36, 2024d.\nMuling Wu, Wenhao Liu, Xiaohua Wang, Tianlong Li, Changze Lv, Zixuan Ling, Jianhao Zhu, Cenyuan\nZhang, Xiaoqing Zheng, and Xuanjing Huang. Advancing parameter efficiency in fine-tuning via",
    "char_length": 1432
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 39,
    "text": "representation editing. arXiv:2402.15179, 2024a. URL https://arxiv.org/abs/2402.15179.\nShanchan Wu and Yifan He. Enriching pre-trained language model with entity information for relation\nclassification. In Proceedings of the 28th ACM international conference on information and knowledge\nmanagement, pages 2361‚Äì2364, 2019.\nZhengxuan Wu, Aryaman Arora, Zheng Wang, Atticus Geiger, Dan Jurafsky, Christopher D. Manning,\nand Christopher Potts. ReFT: Representation finetuning for language models. 2024b. URL arxiv.\norg/abs/2404.03592.\nRan Xu, Wenqi Shi, Yuchen Zhuang, Yue Yu, Joyce C Ho, Haoyu Wang, and Carl Yang. Collab-rag:\nBoostingretrieval-augmentedgenerationforcomplexquestionansweringviawhite-boxandblack-box\nllm collaboration. arXiv preprint arXiv:2504.04915, 2025.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,\nChengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge,\nHaoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi\nYang,JingZhou,JingrenZhou,JunyangLin,KaiDang,KeqinBao,KexinYang,LeYu,LianghaoDeng,\nMei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan\nLiu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang,\nXuanchengRen,YangFan,YangSu,YichangZhang,YingerZhang,YuWan,YuqiongLiu,ZekunWang,\nZeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. arXiv preprint",
    "char_length": 1495
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 40,
    "text": "arXiv:2505.09388, 2025.\nShunyuYao,JeffreyZhao,DianYu,NanDu,IzhakShafran,KarthikRNarasimhan,andYuanCao. React:\nSynergizing reasoning and acting in language models. In The eleventh international conference on\nlearning representations, 2022.\nLonghui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li,\nAdrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large\nlanguage models. arXiv preprint arXiv:2309.12284, 2023.\n21\nPEANuT:Parameter-EfficientAdaptationwithWeight-awareNeuralTweakers\nElad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for\ntransformer-based masked language-models. arXiv preprint arXiv:2106.10199, 2021.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine\nreally finish your sentence? arXiv preprint arXiv:1905.07830, 2019.\nQingru Zhang, Minshuo Chen, Alexander Bukharin, Nikos Karampatziakis, Pengcheng He, Yu Cheng,\nWeizhu Chen, and Tuo Zhao. Adalora: Adaptive budget allocation for parameter-efficient fine-tuning.\narXiv preprint arXiv:2303.10512, 2023.\nYuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Neural prompt search. arXiv preprint arXiv:2206.04673,\n2022.\nHongyu Zhao, Hao Tan, and Hongyuan Mei. Tiny-attention adapter: Contexts are more important than\nthe number of parameters. arXiv preprint arXiv:2211.01979, 2022.\nJiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian.",
    "char_length": 1500
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 41,
    "text": "Galore: Memory-efficientllmtrainingbygradientlow-rankprojection.arXivpreprintarXiv:2403.03507,\n2024.\nHan Zhou, Xingchen Wan, Ivan Vuliƒá, and Anna Korhonen. Autopeft: Automatic configuration search\nfor parameter-efficient fine-tuning. Transactions of the Association for Computational Linguistics, 12:\n525‚Äì542, 2024.\n22\nPEANuT:Parameter-EfficientAdaptationwithWeight-awareNeuralTweakers\nA. Appendix\nB. Details of Theoretical Results\nInthissection,weprovidetheproofofProposition3.2andintroduceadditionaltheoreticalresultswhen\nwe assume sinusoid activation.\nB.1. Proof of Proposition 3.2\nThe intuition behind the proof is that we can always restore an identity function using two ReLU\nactivation functions, i.e., x = œÉ(x)‚àíœÉ(‚àíx) for any x ‚àà R\nProof of Proposition 3.2. We first show that\nmin L(Dtrain;W0+ f(W0;(Œò\n1\n,Œò\n2\n)))\nŒò\n1\n‚ààRd2√ó2r,Œò\n2\n‚ààR2r√ód2\n‚â§ min L(Dtrain;W0+AB).\nA‚ààRd1√ór,B‚ààRr√ód2\nLet(A‚àó,B‚àó) = argmin\nA‚ààRd1√ór,B‚ààRr√ód2\nL(D train;W0+AB). TakeŒò#\n1\n:= [(W0)‚Ä†A‚àó;‚àí(W0)‚Ä†A‚àó] ‚àà Rd2 √ó2r\nand Œò# := [B‚àó‚ä§;‚àíB‚àó‚ä§]‚ä§ ‚àà R2r√ód2 , where (W0)‚Ä† ‚àà Rd2 √ód 1 is the Moore-Penrose inverse of W0. Then,\n2\nsince is a ReLU activation function,\nœÉ\nf(W0;(Œò#,Œò#))\n1 2\n=œÉ(W0Œò#)Œò#\n1 2\n=œÉ(W0(W0)‚Ä†A ‚àó)B ‚àó‚àíœÉ(‚àíW0(W0)‚Ä†A ‚àó)B ‚àó\n=W0(W0)‚Ä†A ‚àó B ‚àó .\nNote that W0(W0)‚Ä† = U0U0‚ä§ is the projection to the left singular space of W0. Hence\nL(Dtrain;W0+ f(W0;(Œò#\n1\n,Œò#\n2\n)))\n=L(Dtrain;U0U0‚ä§ W0+U0U0‚ä§\nA\n‚àó\nB\n‚àó)\n=L(Dtrain;W0+A ‚àó B ‚àó),\nwhere the last equality follows from the invariance assumption. This gives the first inequality:\nmin L(Dtrain;W0+ f(W0;(Œò",
    "char_length": 1501
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 42,
    "text": "1\n,Œò\n2\n)))\nŒò\n1\n‚ààRd2√ó2r,Œò\n2\n‚ààR2r√ód2\n‚â§ L(Dtrain;W0+ f(W0;(Œò#\n1\n,Œò#\n2\n)))\n= L(Dtrain;W0+A ‚àó B ‚àó)\n= min L(Dtrain;W0+AB).\nA‚ààRd1√ór,B‚ààRr√ód2\nWe next show the following inequality:\nmin L(Dtrain;W0+AB)\nA‚ààRd1√ór,B‚ààRr√ód2\n‚â§ min L(Dtrain;W0+ f(W0;(Œò\n1\n,Œò\n2\n))).\nŒò\n1\n‚ààRd2√ór,Œò\n2\n‚ààRr√ód2\n23\nPEANuT:Parameter-EfficientAdaptationwithWeight-awareNeuralTweakers\nTakeA# = œÉ(W0Œò 1 ‚àó) ‚àà Rd 1 √órandB# = Œò 2 ‚àó ‚àà Rr√ód2 ,where(Œò 1 ‚àó, Œò 2 ‚àó) = argmin Œò\n1\n‚ààRd2√ór,Œò\n2\n‚ààRr√ód1 L(D train;W0+\nf(W0;(Œò\n,\nŒò ))). The conclusion follows from\n1 2\nmin L(Dtrain;W0+AB)\nA‚ààRd1√ór,B‚ààRr√ód2\n‚â§ L(Dtrain;W0+A#B#)\n= L(Dtrain;W0+œÉ(W0Œò\n1\n‚àó)Œò\n2\n‚àó)\n= min L(Dtrain;W0+ f(W0;(Œò\n1\n,Œò\n2\n))).\nŒò\n1\n‚ààRd2√ór,Œò\n2\n‚ààRr√ód1\nB.2. Theoretical Analysis of PEANuT under sinusoid activation function\nHere we consider a sinusoid activation function œÉp(x) = sin(2œÄx) (Gashler and Ashmore, 2014) and\ndesign f(W0;Œ∏) = œÉp(W0Œò\n1\n)Œò\n2\nwithŒ∏= (Œò\n1\n, Œò\n2\n). Withthisperiodicactivationfunction,wecanshow\na stronger result that PEANuT has expressivity (almost) greater than or equal to a LoRA with more\nparameters when d ‚â´ d .\n1 2\nProposition B.1 (Expressivity of PEANuT with Sine Activation). Suppose that there exists a row of W0,\nwhose entries are linearly independent over the rationals. Then, for any r > 0 , A ‚àà Rd 1 √ór and B ‚àà Rr√ód2 ,\nand œµ > 0 , there exists some Œò‚àó ‚àà Rd2 √ór and Œò‚àó ‚àà Rr√ód2 such that\n1 2\n‚à•AB‚àíœÉp(W0Œò‚àó)Œò‚àó‚à•F ‚â§ œµ.\n1 2\nProposition B.1 shows that the class of updates ‚àÜW = œÉp(W0Œò\n1\n)Œò\n2\nby PEANuT with 2rd\n2\nparameters",
    "char_length": 1451
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 43,
    "text": "is dense in the class of updates ‚àÜW = AB by LoRA with r(d +d ) parameters. When d ‚â™ d , this\n1 2 2 1\nshows better parameter efficiency of PEANuT.\nExamining the proof of Proposition B.1, it is straightforward to show that the result holds for any\ncontinuous and periodic activation function whose range contains an open interval centered at 0.\nProof. This proof relies on Kronecker‚Äôs theorem (Theorem 7.9 in Apostol (1990)) from number theory,\nwhich shows that for all j ‚àà Rq, the fractional parts of (ct ,ct ,...,ct )‚ä§ is dense in [0,1]q over c ‚àà R,\n1 2 q\nas long as t ,...,t are linearly independent over the rationals.\n1 q\nLet W j‚àó be the j‚àó-th column of W0 whose entries are linearly independent over the rationals. Since AB\nhas a scale ambiguity, we can assume that A is a matrix whose entries are bounded by 1 without loss of\ngenerality. Write A = (A ,A ,...,A ).\n1 2 r\nTake œµ‚Ä≤ > 0 whose value will be determined later. From Kronecker‚Äôs theorem, for each A there exists\nj\nsome c ‚àà R such that\nj\n‚Éí (Ô∏Ä )Ô∏Ä‚Éí\n‚Éí arcsin A ‚Éí\n‚Éí ‚Éí {c j W j‚àó }‚àí j ‚Éí ‚Éí ‚â§ œµ ‚Ä≤ ,\n2œÄ\n‚Éí ‚Éí\nwhere {B} isavectorwhoseentriesarethefractionalpartofthecorrespondingentryofB,and arcsin is\napplied elementwisely.\n24\nPEANuT:Parameter-EfficientAdaptationwithWeight-awareNeuralTweakers\nLet Œò 1 ‚àó = (c 1 e j‚àó,c 2 e j‚àó,...,c r e j‚àó ), where e j‚àó is the j‚àó-th standard basis vector in Rd2 . Using the fact that\n2œÄ{c j W j‚àó } = 2œÄc j W j‚àó mod 2œÄ , we have\n‚É¶ ‚É¶œÉp(W0Œò\n1\n‚àó)‚àíA ‚É¶\n‚É¶\n2\nF\n= ‚É¶ ‚É¶œÉp((c 1 W j‚àó,c 2 W j‚àó,...c r W j‚àó ))‚àíA ‚É¶ ‚É¶ 2 F",
    "char_length": 1488
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 44,
    "text": "‚â§ ‚àëÔ∏Å‚É¶ ‚É¶sin (Ô∏Ä 2œÄc j W j‚àó )Ô∏Ä ‚àíA j ‚É¶ ‚É¶ 2 ‚â§ 4œÄ2rœµ ‚Ä≤2, (2)\nj\nwhere the last inequality follows from equation 2 and the fact that sin(x) is Lipschitz continuous with\nLipschitz constant 1. Hence by choosing Œò‚àó ‚Üê B, we have\n2\n‚É¶ ‚É¶AB‚àíœÉp(W0Œò\n1\n‚àó)Œò\n2\n‚àó‚É¶\n‚É¶\n2\nF\n‚â§‚à•B‚à•2‚É¶ ‚É¶œÉp(W0Œò\n1\n‚àó)‚àíA ‚É¶\n‚É¶\n2\nF\n‚â§4œÄ2‚à•B‚à•2rœµ ‚Ä≤2.\n‚àö\nChoose œµ‚Ä≤ = œµ/(2œÄ r‚à•B‚à•), then the proof is complete.\nC. Hyperparameters\nWe provide the specific hyperparameters used in our experiments to ensure reproducibility. For most\nof our experiments, we use the standard implementation of PEANuT, which we refer to as vanilla\nPEANuT.Theneuralnetworkarchitectureinvanilla PEANuT consistsofonlytwolayers: aninputlayer\nand an output layer. We select this approach because vanilla PEANuT offers the benefits of simplicity\nin implementation, a low parameter count, and sufficient adaptation power. Nonetheless, we dedicate\nSection 5.5 to exploring more complex adaptation networks and their effect on performance.\nC.1. Image Classification\nHyperparameters for PEANuT for Fig. 5 are provided in Table 8. We tune the classification head and\nthe backbone separately and provide detailed settings for each dataset. All weight decay values are not\ntuned and follow the settings from Gao et al. (2024). The scaling factor s is set to 1.0 . The hidden layer\ndimension r for MHSA is set to 7 in the QV-setting, while both hidden layer dimensions for MHSA and\nMLP are set to 2 in the QV-MLP-setting described in Section 6.\nC.2. Natural Language Understanding",
    "char_length": 1482
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 45,
    "text": "We provide used hyper-parameters for PEANuT in natural language understanding on the GLUE\nbenchmark in Table 11 and Table 12. The reported results are obtained when using a depth of 6 for\nPEANuT. The learning rates for the head and the backbone are tuned separately. The scaling factor s is\nsearched in {0.01,0.1,1.0}. For reproducibility, we fix the seed as 0. The hidden layer dimension r is set\nto8inPEANuT-Land1inPEANuT-S.Morespecifically,weapplyPEANuTtoalllayersinRoBERTa-base\nfor PEANuT-L, while only applying PEANuT to layers {4,5,6,7,8,9,10,11} for PEANuT-S to reduce\nthe number of trainable parameters. The seed is fixed for reproducibility.\n25\nPEANuT:Parameter-EfficientAdaptationwithWeight-awareNeuralTweakers\nTable8:Hyperparameterofimageclassificationfor PEANuT.\nHyperparameter OxfordPets StanfordCars CIFAR10 DTD EuroSAT FGVC RESISC45 CIFAR100\nEpochs 10\nOptimizer AdamW\nLRSchedule Linear\nWeightDecay 8E-4 4E-5 9E-5 7E-5 3E-4 7E-5 3E-4 1E-4\nQV\nLearningRate(PEANuT) 5E-3 1E-2 5E-3 1E-2 5E-3 1E-2 5E-3 5E-3\nLearningRate(Head) 5E-3 1E-2 5E-3 1E-2 5E-3 1E-2 1E-2 5E-3\nQV-MLP\nLearningRate(PEANuT) 5E-3 5E-3 5E-3 1E-2 5E-3 5E-3 1E-2 5E-3\nLearningRate(Head) 5E-3 1E-2 5E-3 1E-2 5E-3 1E-2 1E-2 5E-3\nTable9:Hyperparameterofcommonsensereasoningfor PEANuT.\nHyperparameter Commonsense Reasoning\nHidden Layer Dimension 32\n32\nŒ±\nDropout 0.05\nOptimizer Adam W\nLearning Rate 3e-4\nBatch Size 16\nWarmup Steps 100\nEpochs 1\nC.3. Commonsense Reasoning",
    "char_length": 1442
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 46,
    "text": "Weprovidehyperparameterssettingsof PEANuT forcommonsensereasoningtaskinTable9. Wefollow\nthe hyperparameters settings in MiLoRA (Wang et al., 2024a). We limit all samples to a maximum of\n256 tokens. For evaluation, we set a maximum token number of 32.\nC.4. Arithmetic Reasoning\nWe provide hyperparameters settings of PEANuT for arithmetic reasoning task in Table 10. We follow\nthe hyper-parameters settings in MiLoRA (Wang et al., 2024a). We limit all samples to a maximum of\n2048 tokens. For evaluation, we set a maximum token number of 256 on GSM8K (Cobbe et al., 2021)\ndataset. On MATH (Hendrycks et al., 2021), we set the maximum new token to 512.\nD. Datasets\nIn this section, we provide a detailed description of the datasets used in our experiments.\n26\nPEANuT:Parameter-EfficientAdaptationwithWeight-awareNeuralTweakers\nTable10:Hyperparameterofarithmeticreasoningfor PEANuT.\nHyperparameter ArithmeticReasoning\nHiddenLayerDimension 64\nŒ± 64\nDropout 0.05\nOptimizer AdamW\nLearningRate 3e-4\nBatchSize 16\nWarmupSteps 100\nEpochs 3\nTable11:HyperparameterofGLUEbenchmarkfor PEANuT-L.\nHyperparameter STS-B RTE MRPC CoLA SST-2 QNLI MNLI QQP\nOptimizer AdamW\nLR Schedule Linear\nLearning Rate (PEANuT) 5E-3 5E-3 5E-3 1E-3 5E-3 1E-3 5E-3 5E-3\nLearning Rate (Head) 5E-3 5E-3 5E-3 1E-3 5E-3 1E-3 5E-3 5E-3\nScaling 0.1 0.01 0.01 0.1 0.01 0.01 0.01 0.01\nMax Seq. Len 512 512 512 512 512 512 512 512\nBatch Size 64 32 64 64 32 32 32 64\nD.1. Image Classification",
    "char_length": 1446
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 47,
    "text": "For image classification, we provide detailed information about the used datasets in Table 8.\nD.2. Natural Language Understanding\nThe GLUE benchmark comprises 8 NLP datasets: MNLI, SST-2, MRPC, CoLA, QNLI, QQP, RTE, and\nSTS-B,coveringtaskssuchasinference,sentimentanalysis,paraphrasedetection,linguisticacceptability,\nquestion-answering, and textual similarity. We provide detailed information about them in Table 14.\nD.3. Commonsense Reasoning\nFor commonsense reasoning task, we use 8 datasets, including BoolQ, PIQA, SIQA, HellaSwag, Wino-\nGrande, ARC-e, ARC-c and OBQA. The detailed information is provided in Table 15.\nD.4. Arithmetic Reasoning\nDetailedinformationforarithmeticreasoningtaskisprovidedinTable16. GSM8Kconsistsofhighquality\ngrade school math problems, typically free-form answers. MATH includes classifications from multiple\nmathematical domains, such as algebra, counting_and_probability, geometry, intermediate_algebra,\nnumber_theory, prealgebra and precalculus.\n27\nPEANuT:Parameter-EfficientAdaptationwithWeight-awareNeuralTweakers\nTable12:HyperparameterofGLUEbenchmarkfor PEANuT-S.\nHyperparameter STS-B RTE MRPC CoLA SST-2 QNLI MNLI QQP\nOptimizer AdamW\nLR Schedule Linear\nLearning Rate (PEANuT) 5E-3 1E-3 5E-3 5E-3 5E-3 1E-3 5E-3 1E-3\nLearning Rate (Head) 1E-3 1E-3 5E-3 1E-3 5E-3 1E-3 5E-3 1E-3\nScaling 0.1 1.0 0.01 0.1 0.01 0.1 0.01 1.0\nMax Seq. Len 512 512 512 512 512 512 512 512\nBatch Size 64 32 64 64 32 32 32 64\nTable13:Detailedinformationofimageclassificationtasks.",
    "char_length": 1496
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 48,
    "text": "Dataset #Class #Train #Val #Test Rescaled resolution\nOxfordPets 37 3,312 368 3,669\nStandfordCars 196 7,329 815 8,041\nCIFAR10 10 45,000 5,000 10,000\nDTD 47 4,060 452 1,128\n224√ó224\nEuroSAT 10 16,200 5,400 5,400\nFGVC 100 3,000 334 3,333\nRESISC45 45 18,900 6,300 6,300\nCIFAR100 100 45,000 5,000 10,000\n28\nPEANuT:Parameter-EfficientAdaptationwithWeight-awareNeuralTweakers\nTable14:DetailedinformationoftheGLUEbenchmark. STS-Bisaregressiontask,whileallothertasksareeither\nsingle-sentenceorsentence-pairclassificationtasks.\nCorpus Task Metrics # Train # Val # Test # Labels\nSingle-Sentence Tasks\nCoLA Acceptability Matthews Corr. 8.55k 1.04k 1.06k 2\nSST-2 Sentiment Accuracy 67.3k 872 1.82k 2\nSimilarity and Paraphrase Tasks\nMRPC Paraphrase Accuracy/F1 3.67k 408 1.73k 2\nSTS-B Sentence similarity Pearson/Spearman Corr. 5.75k 1.5k 1.38k 1\nQQP Paraphrase Accuracy/F1 364k 40.4k 391k 2\nInference Tasks\nMNLI NLI Accuracy 393k 19.65k 19.65k 3\nQNLI QA/NLI Accuracy 105k 5.46k 5.46k 2\nRTE NLI Accuracy 2.49k 277 3k 2\nTable15:Detailedinformationofcommonsensereasoningtask.\nDataset #Class #Train #Dev #Test\nBoolQ Binary classification 9,427 3,270 3,245\nPIQA Binary classification 16,113 1,838 3,000\nSIQA Ternary classification 33,410 1,954 2,224\nHellaSwag Quaternary classification 39,905 10,042 10,003\nWinoGrande Binary classification 40,398 1,267 1,767\nARC-e Quaternary classification 2,251 570 2,376\nARC-c Quaternary classification 1,119 229 1,172\nOBQA Quaternary classification 4,957 500 500",
    "char_length": 1481
  },
  {
    "paper_id": "PeaNut",
    "chunk_id": 49,
    "text": "Table16:Detailedinformationofarithmeticreasoningtask.\nDataset #Train #Dev #Test\nGSM8K 7,473 1,319 1,319\nMATH 12,500 500 5,000\n29",
    "char_length": 129
  }
]