[
  {
    "paper_id": "plot_twist",
    "chunk_id": 0,
    "text": "Plot Twist: Multimodal Models Don’t Comprehend Simple Chart Details\nYasamanRazeghi IshitaDasgupta FangyuLiu\n♢ ♠ ♠\nVinayRamasesh SameerSingh\n♠ ♢\nUniversityofCalifornia,Irvine GoogleDeepmind\n♢ ♠\n{yrazeghi,sameer}@uci.edu\nAbstract Previous work has showed that real world\ndatasets–whileveryusefulforensuringtheprac-\nRecentadvancesinmultimodalmodelsshow\nticalapplication–canoftencontainstatisticalpat-\nremarkableperformanceinreal-worldbench-\nterns such that model can do well without fully\nmarksforchartandfigureunderstandinglike\nunderstandingtherelevantcapability(Goyaletal.,\nChartQAthatinvolveinterpretingtrends,com-\nparingdatapoints,andextractinginsightsfrom 2017;McCoyetal.,2019). Moreover,testingba-\nvisuals. In this paper, we investigate the ex- siccapabilitiescanhighlightimportantlimitations\ntenttowhichthesemodelstrulycomprehend of models that do not appear in complex bench-\nthe underlying information in charts by pos- marks(Ribeiroetal.,2020). Complexcapabilities\ningdirect,elementaryquestionsaboutsimple\nexamined in these real-world benchmarks, such\nfeaturessuchasaxesrangesandvaluestoex-\nasobtaininginsightsfromvisualizations,arealso\naminetheirfundamentalvisualunderstanding\nmadeupofmanysteps: understandingtheimage,\nabilitiesinthecontextofcharts. Ourquestions\ndomainknowledge,andreasoning. Thismakesit\nareappliedtotwosetsoffigures: syntheticand\nreal-world. Theempiricalevaluationof5pop- hardertodiagnosethecauseforfailures.",
    "char_length": 1435
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 1,
    "text": "ularmultimodalmodelsonourdatasetreveals Inthiswork,weprobemultimodalmodelstoun-\nshortfallsinunderstandingchartsandfigures, derstandwhethertheycananswerelementaryques-\ncontrarytowhattheirperformanceoncomplex tions about the specific visual content in charts.\nbenchmarksmightsuggest. Forinstance,Gem-\nThis is a core capability that is essential for any\niniProVisiononlyachieves57.9%accuracyon\nmodel claiming proficiency in chart comprehen-\nourelementarysetofquestionsonreal-world\nsion. Weevaluatethisunderstandingbyconstruct-\nplots,whileotherpopularmultimodalmodels\nshowedsimilarorlessperformance. Thiswork ingelementaryprobingquestions. Theseelemen-\nhighlights an important limitation of current tary questions include straightforward questions\nmultimodalmodels,andcautionsagainstoverly that measure fundamental skills like identifying\noptimisticinterpretationsoftheirabilitiesbased axis extremes and extracting plot values on syn-\nonresultsofcanonicalevaluations.\nthetic plots. We first pose these elementary ques-\ntions on basic, synthetic plots. Then, we select a\n1 Introduction\nsubsetofreal-worldChartQAtestplotsandpose\nAssessingchartunderstandingcapabilitiesoffersa our simple questions to them. This allows us to\ncrucialbenchmarkforevaluatingfoundationalmod- directlycomparemodelperformancesoncomplex\nels’reasoningskillsbeyondtext. Significantefforts ChartQA queries versus performance on our ele-\nhavebeenmadetodevelopbenchmarksforchart mentaryones(examplesinFigure1).",
    "char_length": 1475
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 2,
    "text": "understanding,suchasChartQA,thatfeaturescom- Ourfindingsuncovershortcomingsinthesemod-\nplex,human-writtenquestionsreflectingreal-world elsregardingfundamentalaspectsofchartunder-\napplications (Methani et al., 2019; Masry et al., standing. For example, PaLI-3 only gets 37.7%\n2022). Multimodalmodelshaverecentlymadesig- accuracyonourstraightforwardquestionsonthe\nnificantprogressontheseevaluationbenchmarks real-worldplots. Moreover,othermodelssuchas\n(Gemini-Teametal.,2023;Chenetal.,2023;Ope- GeminiProVisionandGPT-4Valsogetlessthan\nnAIetal.,2023). Whilethesemodelsperformwell 60%performance. Wefurtherevaluatetherobust-\noncomplextasks,howdotheyfarewithmoreele- nessofthesemodelsandfindthatmorepowerful\nmentaryaspectsofchartunderstanding? Canthey models, suchasGeminiProVisionandGPT-4V,\nreliablyanswerbasicquestionsaboutthechart? areoftensusceptibletothepresenceoftextanno-\n5922\nFindingsoftheAssociationforComputationalLinguistics:EMNLP2024,pages5922–5937\nNovember12-16,2024©2024AssociationforComputationalLinguistics\nmodes in a cost-effective manner. Subsequently,\nweneedtodeterminewhetherthesefailuremodes\npropagatetoreal-worldscenariosandapplications.\nTofacilitatethis,wecreateaprobingdatasetcon-\ntainingtwodistinctsubsetsofplot-questionpairs.\nThefirstsubset,thebasicsyntheticplots,andele-\nmentaryquestionsoffersacontrolledenvironment\ntoscrutinizespecificaspectsofmodelperformance.\nWhatistheminimumxvalueamongthe 4✗ The second subset, the real-world plots, and the",
    "char_length": 1468
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 3,
    "text": "setofpointsinthefigure? same elementary questions consist of real-world\nWhatistheminimumvaluefortherange 0✗ figures,forwhichwehaverandomlyselectedaset\nonthexaxis?\nof plots from real-world images in the ChartQA\n(a)Syntheticplot testset,supplementingthemwithourstraightfor-\nward elementary questions. While the synthetic\nsubset is ideal for in-depth analysis and straight-\nforward to expand, the real-world subset allows\nus to test whether our findings generalize to real-\nworld charts, even though creating this subset re-\nquiresmoreeffortandresourcesduetothemanual\nprocessinvolved. Notethateventhoughournew\ndatasetishuman-generated,itwasgeneratedtoex-\nplicitlycontainsimplequestionsthatonlyrequire\nvisual understanding. This controls for the bias\n(chartQAquestion)Whendoesthe 2014✓ thatcreepsintoveryopen-endedhuman-generated\nlinereachthepeak?\ndatasets. Detailsontheseprobingplotsandques-\n(Ourquestion)Whatistheminimum 0✗ tionsareprovidedintheAppendixA.\nvaluefortherangeonthexaxis?\nModels We evaluate multimodal models that\n(b)Real-worldplot(fromChartQA)\ndemonstratedreasonableperformanceonalready\nFigure1:AnexampleofourevaluationmethodonPaLI- establishedchartunderstandingbenchmarkssuch\n3. ItshowsaquestioninourSyntheticset(top)anda as ChartQA. We include Gemini Pro Vision\nquestionintheChartQAdatasetwithitscorresponding (Gemini-Team et al., 2023), GPT-4V (OpenAI\nquestioninourreal-worldsubsetbelow\net al., 2023), PaLI-3 (Chen et al., 2023), ChartL-\nlama(Hanetal.,2023)andCogVLM(Wangetal.,",
    "char_length": 1492
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 4,
    "text": "tationsovertheactualdatapresentedintheplots,\n2024)modelsinourempiricalanalysis.\nwhichnegativelyaffectstheiraccuracy. Thisstudy\nhighlightscriticallimitationsofcurrentmultimodal Metrics Inourevaluationframework,weemploy\nmodels and underscores the importance of rigor- arelaxedaccuracymeasurefornumericanswersto\nousandthoroughtestingespeciallygivenlimited accommodateminorinaccuraciesfollowingprevi-\npublicknowledgeofthedatausedtotrainthem. 1 ouswork(Methanietal.,2020;Masryetal.,2022;\nLiuetal.,2022,2023a). Specifically,wedeemanu-\n2 Setup mericalanswercorrectifitfallswithin5%relative\nrange of the “gold standard” answer and for non-\nIn the following section, we introduce our evalu-\nnumericalanswerweuseexactmatching. However,\nation method, designed specifically to assess the\nthisaccuracymetricdoesnothaveasymmetricer-\nunderstandingofelementaryfeaturesinbothsyn-\nrorrangeforsmallvslargevalues–forexample,it\ntheticandreal-worldchartandfigureunderstand-\nismuchmorerestrictiveforquestionqueryingthe\ningbymultimodalmodels.\nminimumvaluesincomparisontothosequerying\nthe maximum values. Recognizing that many of\nEvaluationMethod Weproposeatwo-pronged\nour simple questions often pertain to ranges, we\nevaluation approach. Using synthetic data for el-\nadoptarange-basedmetrictoevaluatemodels’an-\nementary questions allows us to identify failure\nswers. This metric, which we term “range-based\n1ThedatasetusedinthispaperisavailableatChart101. accuracy,” allows for a margin of error up to 5%\n5923",
    "char_length": 1486
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 5,
    "text": "Standard Range-Based\nModels\n↓\nAcc CollectiveAcc Acc CollectiveAcc\nGeminiProVision 52.6 0.8% 25.9 1.7% 73.7 0.7% 36.5 1.8%\n± ± ± ±\nGPT-4V 50.0 0.8% 23.8 1.6% 68.4 0.8% 33.4 1.8%\n± ± ± ±\nPaLI-3 31.0 0.8% 8.0 1.0% 43.1 0.8% 21.6 1.7%\n± ± ± ±\nChartLlama 10.6 0.5% 0.1 0.1% 21.3 0.7% 6.8 0.9%\n± ± ± ±\nCogVLM 30.3 0.7% 5.5 0.9% 47.7 0.8% 19.0 1.5%\n± ± ± ±\nTable1: ElementaryQuestionsonSyntheticPlots: Thetabledisplaysaccuracyratesusingstandardmetricsinthe\nleftcolumnsandRange-BasedAccuracyintherightcolumns. Theseresultshighlighttheoveralllowperformance\nofmodelswithsimplechartunderstandingquestions.\nPlotType Models ChartQAQs ElementaryQs\nModel ↓\nbar pie scatter GeminiProV 67.4 2.5% 57.9 1.5%\n± ±\nGPT-4V 64.0 2.6% 58.0 1.5%\nGeminiProVision 53.2 88.5 37.1 ± ±\nPaLI-3 69.7 2.5% 37.7 1.5%\nGPT-4V 42.2 87.2 40.3 ± ±\nChartLlama 30.3 2.4% 25.8 1.3%\nCogVLM 27.3 51.9 23.4 ± ±\nCogVLM 64.0 2.6% 49.8 1.5%\nPaLI-3 26.5 65.8 38.1 ± ±\nChartLlama 9.8 26.9 4.2\nTable 3: Questions on Real Plots: Overall standard\naccuracyonChartQAPlotscomparingoriginalvs. our\nTable 2: Breakdown by Plot Types: Range-Based\nsimplequestions. Thelowperformanceofmodelson\naccuracyonthedifferentforsyntheticplots.\nourelementaryquestionsvs. complexquestionsonthe\nsameplotsrevealsthattheystruggletoanswersimple\nquestions on the same visual data. This discrepancy\nof the entire range under consideration. We also\nhighlightsacriticalgapintheirabilitytoconsistently\ndefinecollectiveaccuracy;thismetricassessesthe\ninterpretvisualinformation.",
    "char_length": 1494
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 6,
    "text": "correctnessofresponsestoafullsetofquestions\nassociatedwithasinglefigureasasinglenumber.\nThis metric underscores the model’s capacity for morechallengingthanpiechartquestions. Thisis\nacomprehensiveandaccurateinterpretationofall likelybecauseanswersforpiechartsareoftenex-\nthebasicvisualfeatureswemeasureforthatfigure. plicitinthefigures(seeFigure6),whereasscatter\nplots require models to interpret min/max values\n3 Results orrangesusinglessexplicitcueslikex-ticks. Fur-\ntheranalysisofchallengingquestiontypesforeach\nModelsstruggletoanswerbasicsyntheticchart\nmodelisinAppendixC\nquestionsreliably. Weinitiallyassessmodelper-\nformanceonoursyntheticsubset. Asdemonstrated Accuracy gap between elementary and com-\nin Table 1, all of our models show poor perfor- plex questions on the same plots. We explore\nmance on both versions of our accuracy metric; whetherthedifficultiesmodelsfacewithbasicchart\nwith the best model Gemini Pro Vision getting understanding questions in synthetic settings are\n52.6%accuracy. However,evenforthismodel,the also evident in real-world scenarios. We ask our\ncollective accuracy of 25.9% indicates a limited elementaryquestionsonasubsetoftheChartQA\ncomprehensiveunderstandingofthesebasicchart test sets. The ChartQA images are chosen inde-\nquestions on the whole chart. Public models per- pendentlyofthequestionspairedwiththeminthe\nformconsiderablyworse,evenChartLlama,which originaldataset. IntherighttwocolumnsofTable",
    "char_length": 1445
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 7,
    "text": "isspecializedexplicitlyforcharts. Thesefindings 3, we compare model performance on these sim-\nhighlight the significance of our straightforward plequestionstotheirperformanceontheoriginal\nbenchmarkinpinpointingthelimitationsofcurrent ChartQAquestions,whichinvolvemorecomplex\nmodels. Weanalyzemodelaccuracybyplottypeto reasoning. Asshown,thereisoftenahighdropin\ninvestigatethechallengeswithdifferentplots. As performance,suchasaround10%forGeminiPro\nTable2shows,modelsfindscatterplotquestions Vision. Thelowperformanceonelementaryques-\n5924\nTypeofTitle GeminiProVision GPT-4V Category RangeBasedAcc.\nCorrectTitle 77.7% 79.4% Original 29.5%\nNoTitle 37.1% 40.3%\nx 25.5%\nMisleading 32.4% 32.5%\nD 28.8%\nMarker\nO 31.5%\nTable 4: Changing Titles in Plots: Range-Based ac-\n+ 26.9%\ncuracyacrossdifferenttitles. Theresultshighlightthe\nimpactoftextualinformationonmodelperformance. Grid 21.3%\nJS-plotly 36.9%\ntionsparticularlyhighlightsconcernsregardingthe Plot JS-highchart 39.7%\nability of models to answer simple questions on JS-amchart 43.7%\nnon-synthetic plots. This is problematic because\nitsuggeststhatthesemodelsmaystruggletohan- Table5: VisualChangesinPlots: PaLI-3Performance\ndlebasictaskseveninreal-worldscenarios,where onfigureswithsameinformationalcontentbutsmallvi-\naccuracyandreliabilityarecrucial. sualchanges. Thevariabilityinperformancehighlights\nalackofrobustnessinchartunderstanding.\n4 RobustnessTests\ntions include altering scatter plot markers, intro-",
    "char_length": 1461
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 8,
    "text": "Onekeyaspectofchartunderstandingisthemod-\nducing grids, or switching the data visualization\nels’resiliencetovisualchangesthatdonotaffect\nlibraryfromMatplotlibtoJavaScript. Resultsare\ntheinformationalcontentbutonlythevisualpresen-\ndisplayedinTable5. Thetoprowshowstherange-\ntation,suchasthechoiceoftheplottinglibraryor\nbasedaccuracyforPaLI-3onthescatterplotsubset\nvariationsinphrasing. Oursyntheticsubsetallows\nat29.5%. Thetablerevealsthatevenslightvisual\nustocomprehensivelyevaluatemodelrobustness\nchangeshighlyimpactthemodel’sperformancein\nagainst these changes. We make targeted visual\nansweringthesamequestion. Forinstance,adding\nmodificationstochartstoassessthemodels’abil-\ngrids to the figures reduces accuracy to 21.3%,\nity to maintain accurate interpretation despite su-\nwhile changing the plot style from Matplotlib’s\nperficialalterations. Thisevaluationiscrucialfor\ndefault to JavaScript-amchart improves accuracy\ndetermining the real-world utility and robustness\nto43.7%. Thesefindingshighlightmodel’slackof\nofmultimodalmodelsinchartcomprehension.\nrobustness, as its performance is greatly affected\nModeldependenceontextualcuesinplots. We bysuchsmallvisualizationchanges.\ncompare three scenarios: 1) plots without any ti-\ntle (baseline), 2) plots with a title containing the 5 RelatedWork\ncorrectanswer,and3)plotswithatitleproviding\nBenchmarksformultimodalreasoning. With\nmisleading,incorrectanswers. Inallcases,models\nrecent advancements in foundation multimodal",
    "char_length": 1474
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 9,
    "text": "are instructed to base their answers on the figure\nmodels,extensiveeffortshavebeenmadetocreate\nitself. The results are presented in Table 4. Our\nvaluableevaluationbenchmarksforassessingmul-\nfindingsrevealthatincludingthecorrectanswerin\ntimodal models in various domains such as math\ntheplottitlelargelyenhancesmodelperformance\nreasoning (Lu et al., 2024; Cherian et al., 2022),\nonourdataset,withanimprovementofover15%\ngeometricreasoning(Kazemietal.,2023;Luetal.,\nobserved for both Gemini Pro Vision and GPT-\n2021), geometric reasoning for coding (Risman-\n4Vmodels. Whencomparingthemisleadingtitle\nchian et al., 2024), visual question answering on\nscenario to the no title scenario, we observe that\nnatural images (Liu et al., 2023b; Agrawal et al.,\nGeminiProVision,inparticular,isswayedbythe\n2016;Gurarietal.,2018),medicalquestionanswer-\npresence of misleading textual information, sug-\ning (Zhang et al., 2023), hallucination detection\ngesting a bias towards text in the figure over an\n(Guanetal.,2024;Lietal.,2023b)andcomprehen-\naccurateunderstandingoftheplotitself.\nsivemultimodalcapabilitiesonreal-worldimages\nModeldependenceonvisualmodifications. We (Yuetal.,2023;AhoandUllman,1972;Fuetal.,\nmake minor visual modifications to the synthetic 2024;Liuetal.,2023c;Lietal.,2023a;Xuetal.,\nfigures while ensuring they convey the same in- 2023). Inthiswork,wedelveintothechartunder-\nformationandthenposeourquestions. Modifica- standing capabilities of foundational multimodal\n5925",
    "char_length": 1478
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 10,
    "text": "models,exploringacriticalandvaluableskillset. modes. Byexposingsubtlelimitations,ourmethod\nlaysthegroundworkformoreeffectivebenchmarks\nChart/Figurereasoningbenchmarks. Specifi-\nthat accurately capture previously hidden model\ncally,therehavebeenvaluableeffortsindeveloping\nweaknesses. Webelievethisworkwillinspirefur-\nbenchmarksforchartunderstanding,rangingfrom\nther innovation in the field, promoting a holistic\nsyntheticbenchmarks(Kafleetal.,2018;Singhand\nandnuancedapproachtomodelevaluation.\nShekhar,2020)featuringyes/noquestions(Kahou\net al., 2018), to those focusing on understanding 7 Limitations\nreal-worldcharts(Masryetal.,2022;Methanietal.,\nThis study emphasizes the value of using direct,\n2020; Xia et al., 2024). In this work, we show a\nunittestingwithreal-worldapplicationevaluations\nmethodthatbridgesthegapbetweenthesynthetic\nformultimodalmodelsinthecontextofchartun-\ncreation of benchmarks for figure understanding\nderstanding. Whileourapproacheffectivelyiden-\nandreal-worldapplicationswithinthebenchmark.\ntifiesclearlimitationsandchallengeswithinthese\nOur evaluation method demonstrates that assess-\nmodels,therearelimitationstoourstudy:\ningmodelsonbasic,fundamentalquestionsabout\nLack of Proposed Solutions: While we identify\nchart understanding can uncover crucial insights\nvariousmodellimitations,ourstudydoesnotoffer\ninto model vulnerabilities. These vulnerabilities\nspecificsolutionstotheseissues. Ourinsightsare\nmightbeoverlookedifevaluationsareconducted",
    "char_length": 1475
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 11,
    "text": "pivotalforpinpointingeffectiveremedies.\nsolelyonsyntheticorreal-worldimages.\nCauses of the Shortcomings: One limitation of\nMultimodalfoundationmodels. Recently,there this study is the ambiguity regarding the precise\nhasbeenasignificantemergenceofgeneralistfoun- causes behind the observed model shortcomings.\ndational multimodal models capable of answer- Althoughwehypothesizethatadistributionalshift\ning questions about images and reasoning upon between the training data and our evaluation set\nthem. Closed-sourcemodelssuchasGeminiPro might play a role, further investigation is needed\nVision(Gemini-Teametal.,2023),GPT-4V(Ope- toconfirmthisandunderstandthis. Weencourage\nnAI et al., 2023) (OpenAI et al., 2023), PaLI-3 continued research and improvement in the field,\n(Chen et al., 2023) stand alongside open-source enhancingtherobustnessandapplicabilityofmul-\ncounterparts like LLaVA-1.5 (Liu et al., 2023b), timodalmodelsacrossvariousreal-worldtasks.\nMini-GPT4(Zhuetal.,2023),InstructBLIP(Dai\n8 Acknowledgment\net al., 2023) and CogVLM (Wang et al., 2024).\nAdditionally, there are models with a specific fo- We would like to thank the members of UCI-\ncusonchartunderstanding,suchasMatCha(Liu NLP, for valuable discussions and feedback on\net al., 2022), ChartLlAMA(Han et al., 2023) and this work. This material is sponsored in part by\nChartVLM (Xia et al., 2024). In this work, we the DARPA MCS program under Contract No.",
    "char_length": 1426
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 12,
    "text": "evaluatealltheaforementionedmodelsthatareac- N660011924033withtheUnitedStatesOfficeOf\ncessibletousandhavedemonstratedevenaslight Naval Research, the NSF CAREER award num-\ncapabilityforchartunderstanding. berIIS-2046873andtheNSFawardIIS-2008956.\nYasaman Razeghi has been doing a part-time in-\n6 Conclusion ternship at Google DeepMind while working on\nthispaper.\nInthispaper, wepresentadiagnosticmethodfor\nevaluatingmultimodalfoundationmodels,witha\nfocusonchartunderstandingcapabilities. Ourap- References\nproach combines the precision of controlled syn-\nAishwaryaAgrawal,JiasenLu,StanislawAntol,Mar-\ntheticevaluationswiththereal-worldrelevanceof\ngaret Mitchell, C. Lawrence Zitnick, Dhruv Batra,\nnaturaldatascenarios. Thisdualstrategyisessen- andDeviParikh.2016. Vqa: Visualquestionanswer-\ntial in the current landscape, where models often ing. Preprint,arXiv:1505.00468.\nlacktransparencyregardingtrainingdataandoper-\nAlfredV.AhoandJeffreyD.Ullman.1972. TheTheory\natebehindAPIs. Ourevaluationmethodcomple- of Parsing, Translation and Compiling, volume 1.\nmentsreal-worlddatasetslikeChartQA,emphasiz- Prentice-Hall,EnglewoodCliffs,NJ.\ning the need to look beyond unified metrics and\nXi Chen, Xiao Wang, Lucas Beyer, Alexander\nthoroughlyassessmodelstoidentifytheirfailure Kolesnikov, Jialin Wu, Paul Voigtlaender, Basil\n5926\nMustafa, Sebastian Goodman, Ibrahim Alabdul- Nithya Attaluri, Jan Balaguer, Jackie Xiang, Pi-",
    "char_length": 1416
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 13,
    "text": "mohsin, Piotr Padlewski, Daniel Salz, Xi Xiong, dong Wang, Zoe Ashwood, Anton Briukhov, Al-\nDanielVlasic,FilipPavetic,KeranRong,TianliYu, bert Webson, Sanjay Ganapathy, Smit Sanghavi,\nDaniel Keysers, Xiaohua Zhai, and Radu Soricut. Ajay Kannan, Ming-Wei Chang, Axel Stjerngren,\n2023. Pali-3visionlanguagemodels: Smaller,faster, JosipDjolonga,YutingSun,AnkurBapna,Matthew\nstronger. Preprint,arXiv:2310.09199. Aitchison, Pedram Pejman, Henryk Michalewski,\nTianheYu,CindyWang,JulietteLove,JunwhanAhn,\nAnoop Cherian, Kuan-Chuan Peng, Suhas Lohit, Dawn Bloxwich, Kehang Han, Peter Humphreys,\nKSmith,andJoshuaBTenenbaum.2022. Aredeep Thibault Sellam, James Bradbury, Varun Godbole,\nneuralnetworkssmarterthansecondgraders?.arxiv. SinaSamangooei,BogdanDamoc,AlexKaskasoli,\nSébastienM.R.Arnold,VijayVasudevan,Shubham\nWenliang Dai, Junnan Li, Dongxu Li, Anthony\nAgrawal,JasonRiesa,DmitryLepikhin,RichardTan-\nMeng Huat Tiong, Junqi Zhao, Weisheng Wang,\nburn, Srivatsan Srinivasan, Hyeontaek Lim, Sarah\nBoyang Li, Pascale Fung, and Steven Hoi.\nHodkinson, Pranav Shyam, Johan Ferret, Steven\n2023. Instructblip: Towardsgeneral-purposevision-\nHand, Ankush Garg, Tom Le Paine, Jian Li, Yu-\nlanguagemodelswithinstructiontuning. Preprint,\njiaLi,MinhGiang,AlexanderNeitz,ZaheerAbbas,\narXiv:2305.06500.\nSarahYork,MachelReid,ElizabethCole,Aakanksha\nChowdhery, Dipanjan Das, Dominika Rogozin´ska,\nChaoyouFu,PeixianChen,YunhangShen,YuleiQin,\nVitalyNikolaev,PabloSprechmann,ZacharyNado,",
    "char_length": 1463
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 14,
    "text": "MengdanZhang,XuLin,JinruiYang,XiawuZheng,\nLukas Zilka, Flavien Prost, Luheng He, Marianne\nKe Li, Xing Sun, Yunsheng Wu, and Rongrong Ji.\nMonteiro,GauravMishra,ChrisWelty,JoshNewlan,\n2024. Mme:Acomprehensiveevaluationbenchmark\nDawei Jia, Miltiadis Allamanis, Clara Huiyi Hu,\nfor multimodal large language models. Preprint,\nRaouldeLiedekerke,JustinGilmer,CarlSaroufim,\narXiv:2306.13394.\nShruti Rijhwani, Shaobo Hou, Disha Shrivastava,\nGemini-Team, Rohan Anil, Sebastian Borgeaud, Anirudh Baddepudi, Alex Goldin, Adnan Ozturel,\nYonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,Radu Albin Cassirer, Yunhan Xu, Daniel Sohn, Deven-\nSoricut, Johan Schalkwyk, Andrew M. Dai, Anja dra Sachan, Reinald Kim Amplayo, Craig Swan-\nHauth, Katie Millican, David Silver, Slav Petrov, son,DessiePetrova,ShashiNarayan,ArthurGuez,\nMelvinJohnson,IoannisAntonoglou,JulianSchrit- SiddharthaBrahma,JessicaLandon,MiteyanPatel,\ntwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Ruizhe Zhao, Kevin Villela, Luyu Wang, Wenhao\nTimothy Lillicrap, Angeliki Lazaridou, Orhan Fi- Jia, Matthew Rahtz, Mai Giménez, Legg Yeung,\nrat, JamesMolloy, MichaelIsard, PaulR.Barham, Hanzhao Lin, James Keeling, Petko Georgiev, Di-\nTomHennigan,BenjaminLee,FabioViola,Malcolm anaMincu,BoxiWu,SalemHaykal,RachelSapu-\nReynolds,YuanzhongXu,RyanDoherty,EliCollins, tro,KiranVodrahalli,JamesQin,ZeynepCankara,\nClemens Meyer, Eliza Rutherford, Erica Moreira, Abhanshu Sharma, Nick Fernando, Will Hawkins,",
    "char_length": 1443
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 15,
    "text": "Kareem Ayoub, Megha Goel, George Tucker, En- Behnam Neyshabur, Solomon Kim, Adrian Hut-\nrique Piqueras, Maxim Krikun, Iain Barr, Nikolay ter, Priyanka Agrawal, Alex Castro-Ros, George\nSavinov,IvoDanihelka,BeccaRoelofs,AnaïsWhite, vandenDriessche,TaoWang,FanYang,Shuoyiin\nAndersAndreassen,TamaravonGlehn,Lakshman Chang,PaulKomarek,RossMcIlroy,MarioLucˇic´,\nYagati, Mehran Kazemi, Lucas Gonzalez, Misha Guodong Zhang, Wael Farhan, Michael Sharman,\nKhalman, Jakub Sygnowski, Alexandre Frechette, Paul Natsev, Paul Michel, Yong Cheng, Yamini\nCharlotteSmith,LauraCulp,LevProleev,YiLuan, Bansal, Siyuan Qiao, Kris Cao, Siamak Shakeri,\nXiChen,JamesLottes,NathanSchucher,Federico Christina Butterfield, Justin Chung, Paul Kishan\nLebron, AlbanRrustemi, NatalieClay, PhilCrone, Rubenstein,ShivaniAgrawal,ArthurMensch,Kedar\nTomasKocisky,JeffreyZhao,BartekPerz,DianYu, Soparkar,KarelLenc,TimothyChung,AedanPope,\nHeidi Howard, Adam Bloniarz, Jack W. Rae, Han Loren Maggiore, Jackie Kay, Priya Jhakra, Shibo\nLu,LaurentSifre,MarcelloMaggioni,FredAlcober, Wang,JoshuaMaynez,MaryPhuong,TaylorTobin,\nDanGarrette,MeganBarnes,ShantanuThakoor,Ja- Andrea Tacchetti, Maja Trebacz, Kevin Robinson,\ncob Austin, Gabriel Barth-Maron, William Wong, Yash Katariya, Sebastian Riedel, Paige Bailey, Ke-\nRishabh Joshi, Rahma Chaabouni, Deeni Fatiha, fan Xiao, Nimesh Ghelani, Lora Aroyo, Ambrose\nArunAhuja,RuiboLiu,YunxuanLi,SarahCogan, Slone, Neil Houlsby, Xuehan Xiong, Zhen Yang,",
    "char_length": 1451
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 16,
    "text": "Jeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang, ElenaGribovskaya,JonasAdler,MateoWirth,Lisa\nJordanGrimstad,AleJakseHartman,MartinChad- Lee,MusicLi,ThaisKagohara,JayPavagadhi,So-\nwick, Gaurav Singh Tomar, Xavier Garcia, Evan phie Bridgers, Anna Bortsova, Sanjay Ghemawat,\nSenter, Emanuel Taropa, Thanumalayan Sankara- ZafaraliAhmed,TianqiLiu,RichardPowell,Vijay\nnarayanaPillai,JacobDevlin,MichaelLaskin,Diego Bolina, Mariko Iinuma, Polina Zablotskaia, James\nde Las Casas, Dasha Valter, Connie Tao, Lorenzo Besley,Da-WoonChung,TimothyDozat,Ramona\nBlanco,AdriàPuigdomènechBadia,DavidReitter, Comanescu,XianceSi,JeremyGreer,GuolongSu,\nMiannaChen,JennyBrennan,ClaraRivera,Sergey Martin Polacek, Raphaël Lopez Kaufman, Simon\nBrin,ShariqIqbal,GabrielaSurita,JaneLabanowski, Tokumine,HexiangHu,ElenaBuchatskaya,Yingjie\nAbhiRao,StephanieWinkler,EmilioParisotto,Yim- Miao,MohamedElhawaty,AdityaSiddhant,Nenad\ning Gu, Kate Olszewska, Yujing Zhang, Ravi Ad- Tomasev,JinweiXing,ChristinaGreer,HelenMiller,\ndanki, Antoine Miech, Annie Louis, Laurent El ShereenAshraf,AurkoRoy,ZizhaoZhang,AdaMa,\nShafey,DenisTeplyashin,GeoffBrown,ElliotCatt, AngelosFilos,MilosBesta,RoryBlevins,TedKli-\n5927\nmenko,Chih-KuanYeh,SoravitChangpinyo,Jiaqi Goedeckemeyer,WilliGierke,MohsenJafari,Meenu\nMu, Oscar Chang, Mantas Pajarskas, Carrie Muir, Gaba,JeremyWiesner,DianaGageWright,Yawen\nVeredCohen,CharlineLeLan,KrishnaHaridasan, Wei,HarshaVashisht,YanaKulizhskaya,JayHoover,",
    "char_length": 1441
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 17,
    "text": "AmitMarathe,StevenHansen,SholtoDouglas,Ra- Maigo Le, Lu Li, Chimezie Iwuanyanwu, Lu Liu,\njkumar Samuel, Mingqiu Wang, Sophia Austin, Kevin Ramirez, Andrey Khorlin, Albert Cui, Tian\nChangLan,JiepuJiang,JustinChiu,JaimeAlonso LIN,MarinGeorgiev,MarcusWu,RicardoAguilar,\nLorenzo, Lars Lowe Sjösund, Sébastien Cevey, KeithPallo,AbhishekChakladar,AlenaRepina,Xi-\nZach Gleicher, Thi Avrahami, Anudhyan Boral, huiWu,TomvanderWeide,PriyaPonnapalli,Car-\nHansa Srinivasan, Vittorio Selo, Rhys May, Kon- oline Kaplan, Jiri Simsa, Shuangfeng Li, Olivier\nstantinosAisopos,LéonardHussenot,LivioBaldini Dousse, Fan Yang, Jeff Piper, Nathan Ie, Minnie\nSoares,KateBaumli,MichaelB.Chang,AdriàRe- Lui, Rama Pasumarthi, Nathan Lintz, Anitha Vi-\ncasens,BenCaine,AlexanderPritzel,FilipPavetic, jayakumar,LamNguyenThiet,DanielAndor,Pedro\nFabio Pardo, Anita Gergely, Justin Frye, Vinay Valenzuela, CosminPaduraru, DaiyiPeng, Kather-\nRamasesh, Dan Horgan, Kartikeya Badola, Nora ineLee,ShuyuanZhang,SomerGreene,DucDung\nKassner, Subhrajit Roy, Ethan Dyer, Víctor Cam- Nguyen, Paula Kurylowicz, Sarmishta Velury, Se-\npos,AlexTomala,YunhaoTang,DaliaElBadawy, bastianKrause,CassidyHardin,LucasDixon,Lili\nElspeth White, Basil Mustafa, Oran Lang, Ab- Janzer, Kiam Choo, Ziqiang Feng, Biao Zhang,\nhishek Jindal, Sharad Vikram, Zhitao Gong, Sergi AchintyaSinghal, TejasiLatkar, MingyangZhang,\nCaelles,RossHemsley,GregoryThornton,Fangxi- QuocLe,ElenaAllicaAbellan,DayouDu,DanMcK-",
    "char_length": 1445
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 18,
    "text": "aoyuFeng,WojciechStokowiec,CeZheng,Phoebe innon,NatashaAntropova,TolgaBolukbasi,Orgad\nThacker, Çag˘lar Ünlü, Zhishuai Zhang, Moham- Keller,DavidReid,DanielFinchelstein,MariaAbi\nmadSaleh,JamesSvensson,MaxBileschi,Piyush Raad,RemiCrocker,PeterHawkins,RobertDadashi,\nPatil,AnkeshAnand,RomanRing,KaterinaTsihlas, ColinGaffney,SidLall,KenFranko,EgorFilonov,\nArpiVezer,MarcoSelvi,TobyShevlane,MikelRo- AnnaBulanova,RémiLeblond,VikasYadav,Shirley\ndriguez, Tom Kwiatkowski, Samira Daruki, Keran Chung, Harry Askham, Luis C. Cobo, Kelvin Xu,\nRong, Allan Dafoe, Nicholas FitzGerald, Keren FelixFischer,JunXu,ChristinaSorokin,ChrisAl-\nGu-Lemberg, Mina Khan, Lisa Anne Hendricks, berti,Chu-ChengLin,ColinEvans,HaoZhou,Alek\nMarie Pellat, Vladimir Feinberg, James Cobon- Dimitriev, Hannah Forbes, Dylan Banarse, Zora\nKerr, Tara Sainath, Maribeth Rauh, Sayed Hadi Tung,JeremiahLiu,MarkOmernick,ColtonBishop,\nHashemi, Richard Ives, Yana Hasson, YaGuang ChintuKumar,RachelSterneck,RyanFoley,Rohan\nLi, Eric Noland, Yuan Cao, Nathan Byrd, Le Hou, Jain,SwaroopMishra,JiaweiXia,TaylorBos,Ge-\nQingzeWang,ThibaultSottiaux,MichelaPaganini, offrey Cideron, Ehsan Amid, Francesco Piccinno,\nJean-BaptisteLespiau,AlexandreMoufarek,Samer Xingyu Wang, Praseem Banzal, Petru Gurita, Hila\nHassan, Kaushik Shivakumar, Joost van Amers- Noga, Premal Shah, Daniel J. Mankowitz, Alex\nfoort, Amol Mandhane, Pratik Joshi, Anirudh Polozov,NateKushman,VictoriaKrakovna,Sasha",
    "char_length": 1434
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 19,
    "text": "Goyal,MatthewTung,AndrewBrock,HannahShea- Brown, MohammadHossein Bateni, Dennis Duan,\nhan, Vedant Misra, Cheng Li, Nemanja Rakic´evic´, Vlad Firoiu, Meghana Thotakuri, Tom Natan, An-\nMostafaDehghani,FangyuLiu,SidMittal,Junhyuk hadMohananey, MatthieuGeist, SidharthMudgal,\nOh,SebNoury,ErenSezener,FantineHuot,Matthew SertanGirgin, HuiLi, JiayuYe, OfirRoval, Reiko\nLamm, Nicola De Cao, Charlie Chen, Gamaleldin Tojo, Michael Kwong, James Lee-Thorp, Christo-\nElsayed,EdChi,MahdisMahdieh,IanTenney,Nan pherYew,QuanYuan,SumitBagri,DanilaSinopal-\nHua,IvanPetrychenko,PatrickKane,DylanScand- nikov,SabelaRamos,JohnMellor,AbhishekSharma,\ninaro,RishubJain,JonathanUesato,RominaDatta, Aliaksei Severyn, Jonathan Lai, Kathy Wu, Heng-\nAdam Sadovsky, Oskar Bunyan, Dominik Rabiej, TzeCheng, DavidMiller, NicolasSonnerat, Denis\nShimuWu,JohnZhang,GautamVasudevan,Edouard Vnukov,RoryGreig,JenniferBeattie,EmilyCave-\nLeurent,MahmoudAlnahlawi,IonutGeorgescu,Nan ness,LibinBai,JulianEisenschlos,AlexKorchem-\nWei, Ivy Zheng, Betty Chan, Pam G Rabinovitch, niy,TomyTsai,MimiJasarevic,WeizeKong,Phuong\nPiotr Stanczyk, Ye Zhang, David Steiner, Subhajit Dao, Zeyu Zheng, Frederick Liu, Fan Yang, Rui\nNaskar, MichaelAzzam, MatthewJohnson, Adam Zhu, Mark Geller, Tian Huey Teh, Jason Sanmiya,\nPaszke, Chung-Cheng Chiu, Jaume Sanchez Elias, EvgenyGladchenko,NejcTrdin,AndreiSozanschi,\nAfroz Mohiuddin, Faizan Muhammad, Jin Miao, DanielToyama,EvanRosen,SasanTavakkol,Lint-",
    "char_length": 1445
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 20,
    "text": "Andrew Lee, Nino Vieillard, Sahitya Potluri, Jane ingXue,ChenElkind,OliverWoodman,JohnCar-\nPark,ElnazDavoodi,JiagengZhang,JeffStanway, penter,GeorgePapamakarios,RupertKemp,Sushant\nDrewGarmon,AbhijitKarmarkar,ZheDong,Jong Kafle, Tanya Grunina, Rishika Sinha, Alice Tal-\nLee,AviralKumar,LuoweiZhou,JonathanEvens, bert,AbhimanyuGoyal,DianeWu,DeneseOwusu-\nWilliam Isaac, Zhe Chen, Johnson Jia, Anselm Afriyie, Cosmo Du, Chloe Thornton, Jordi Pont-\nLevskaya, Zhenkai Zhu, Chris Gorgolewski, Peter Tuset,PradyumnaNarayana,JingLi,SabaerFatehi,\nGrabowski,YuMao,AlbertoMagni,KaishengYao, JohnWieting,OmarAjmeri,BenignoUria,TaoZhu,\nJavier Snaider, Norman Casagrande, Paul Sugan- Yeongil Ko, Laura Knight, Amélie Héliou, Ning\nthan,EvanPalmer,GeoffreyIrving,EdwardLoper, Niu,ShaneGu,ChenxiPang,DustinTran,Yeqing\nManaalFaruqui,IshaArkatkar,NanxinChen,Izhak Li, Nir Levine, Ariel Stolovich, Norbert Kalb, Re-\nShafran,MichaelFink,AlfonsoCastaño,IreneGian- becaSantamaria-Fernandez,SonamGoenka,Wenny\nnoumis, WooyeolKim, MikołajRybin´ski, Ashwin Yustalim,RobinStrudel,AliElqursh,BalajiLaksh-\nSreevatsa,JenniferPrendki,DavidSoergel,Adrian minarayanan,CharlieDeck,ShyamUpadhyay,Hyo\n5928\nLee, Mike Dusenberry, Zonglin Li, Xuezhi Wang, YashGoyal,TejasKhot,DouglasSummers-Stay,Dhruv\nKyle Levin, Raphael Hoffmann, Dan Holtmann- Batra,andDeviParikh.2017. Makingthevinvqa\nRice, Olivier Bachem, Summer Yue, Sho Arora, matter: Elevating the role of image understanding",
    "char_length": 1442
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 21,
    "text": "Eric Malmi, Daniil Mirylenka, Qijun Tan, Christy invisualquestionanswering. InProceedingsofthe\nKoh, Soheil Hassas Yeganeh, Siim Põder, Steven IEEE conference on computer vision and pattern\nZheng, Francesco Pongetti, Mukarram Tariq, Yan- recognition,pages6904–6913.\nhua Sun, Lucian Ionita, Mojtaba Seyedhosseini,\nPouya Tafti, Ragha Kotikalapudi, Zhiyu Liu, An- Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian,\nmolGulati,JasmineLiu,XinyuYe,BartChrzaszcz, ZongxiaLi,XiaoyuLiu,XijunWang,LichangChen,\nLily Wang, Nikhil Sethi, Tianrun Li, Ben Brown, FurongHuang,YaserYacoob,DineshManocha,and\nShreya Singh, Wei Fan, Aaron Parisi, Joe Stanton, TianyiZhou.2024. Hallusionbench: Anadvanced\nChenkaiKuang,VinodKoverkathu,ChristopherA. diagnosticsuiteforentangledlanguagehallucination\nChoquette-Choo,YunjieLi,TJLu,AbeIttycheriah, andvisualillusioninlargevision-languagemodels.\nPrakashShroff,PeiSun,ManiVaradarajan,SanazBa- Preprint,arXiv:2310.14566.\nhargam,RobWilloughby,DavidGaddy,IshitaDas-\ngupta,GuillaumeDesjardins,MarcoCornero,Brona DannaGurari,QingLi,AbigaleJ.Stangl,AnhongGuo,\nRobenek, Bhavishya Mittal, Ben Albrecht, Ashish ChiLin,KristenGrauman,JieboLuo,andJeffreyP.\nShenoy,FedorMoiseev,HenrikJacobsson,Alireza Bigham. 2018. Vizwiz grand challenge: Answer-\nGhaffarkhah,MorganeRivière,AlannaWalton,Clé- ing visual questions from blind people. Preprint,\nment Crepy, Alicia Parrish, Yuan Liu, Zongwei arXiv:1802.08218.\nZhou,ClementFarabet,CareyRadebaugh,Praveen\nYuchengHan,ChiZhang,XinChen,XuYang,Zhibin",
    "char_length": 1497
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 22,
    "text": "Srinivasan, Claudia van der Salm, Andreas Fidje-\nWang,GangYu,BinFu,andHanwangZhang.2023.\nland,SalvatoreScellato,EriLatorre-Chimoto,Hanna\nChartllama: Amultimodalllmforchartunderstand-\nKlimczak-Plucin´ska, David Bridson, Dario de Ce-\ningandgeneration. Preprint,arXiv:2311.16483.\nsare, Tom Hudson, Piermaria Mendolicchio, Lexi\nWalker,AlexMorris,IvoPenchev,MatthewMauger,\nKushal Kafle, Brian Price, Scott Cohen, and Christo-\nAlexeyGuseynov,AlisonReid,SethOdoom,Lucia\npher Kanan. 2018. Dvqa: Understanding data\nLoher,VictorCotruta,MadhaviYenugula,Dominik\nvisualizations via question answering. Preprint,\nGrewe,AnastasiaPetrushkina,TomDuerig,Antonio\narXiv:1801.08163.\nSanchez,SteveYadlowsky,AmyShen,AmirGlober-\nson,AdamKurzrok,LynetteWebb,SahilDua,Dong\nSamira Ebrahimi Kahou, Vincent Michalski, Adam\nLi,PreethiLahoti,SuryaBhupatiraju,DanHurt,Ha-\nAtkinson,AkosKadar,AdamTrischler,andYoshua\nroonQureshi,AnanthAgarwal,TomerShani,Matan\nBengio.2018. Figureqa: Anannotatedfiguredataset\nEyal, Anuj Khare, Shreyas Rammohan Belle, Lei\nforvisualreasoning. Preprint,arXiv:1710.07300.\nWang, Chetan Tekur, Mihir Sanjay Kale, Jinliang\nWei, Ruoxin Sang, Brennan Saeta, Tyler Liechty,\nMehranKazemi,HamidrezaAlvari,AnkitAnand,Jialin\nYiSun,YaoZhao,StephanLee,PanduNayak,Doug\nWu,XiChen,andRaduSoricut.2023. Geomverse:\nFritz,ManishReddyVuyyuru,JohnAslanides,Nidhi\nAsystematicevaluationoflargemodelsforgeomet-\nVyas, Martin Wicke, Xiao Ma, Taylan Bilal, Ev-\nricreasoning. arXivpreprintarXiv:2312.12241.",
    "char_length": 1475
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 23,
    "text": "genii Eltyshev, Daniel Balle, Nina Martin, Hardie\nCate, James Manyika, Keyvan Amiri, Yelin Kim,\nBohaoLi,RuiWang,GuangzhiWang,YuyingGe,Yix-\nXiXiong,KaiKang,FlorianLuisier,NileshTripu-\niaoGe,andYingShan.2023a. Seed-bench: Bench-\nraneni,DavidMadras,MandyGuo,AustinWaters,\nmarking multimodal llms with generative compre-\nOliverWang,JoshuaAinslie,JasonBaldridge,Han\nhension. Preprint,arXiv:2307.16125.\nZhang,GarimaPruthi,JakobBauer,FengYang,Ri-\nham Mansour, Jason Gelman, Yang Xu, George YifanLi,YifanDu,KunZhou,JinpengWang,XinZhao,\nPolovets, Ji Liu, Honglong Cai, Warren Chen, Xi- andJi-RongWen.2023b. Evaluatingobjecthalluci-\nangHaiSheng,EmilyXue,SherjilOzair,AdamsYu, nationinlargevision-languagemodels. InProceed-\nChristofAngermueller,XiaoweiLi,WeirenWang,Ju- ingsofthe2023ConferenceonEmpiricalMethodsin\nliaWiesinger,EmmanouilKoukoumidis,YuanTian, NaturalLanguageProcessing,pages292–305,Sin-\nAnandIyer,MadhuGurumurthy,MarkGoldenson, gapore.AssociationforComputationalLinguistics.\nParashar Shah, MK Blake, Hongkun Yu, Anthony\nUrbanowicz,JennimariaPalomaki,ChrisanthaFer- Fangyu Liu, Julian Eisenschlos, Francesco Piccinno,\nnando, Kevin Brooks, Ken Durden, Harsh Mehta, Syrine Krichene, Chenxi Pang, Kenton Lee, Man-\nNikolaMomchev,ElaheRahimtoroghi,MariaGeor- darJoshi,WenhuChen,NigelCollier,andYasemin\ngaki, Amit Raul, Sebastian Ruder, Morgan Red- Altun.2023a. DePlot: One-shotvisuallanguagerea-\nshaw,JinhyukLee,KomalJalan,DinghuaLi,Ginger soning by plot-to-table translation. In Findings of",
    "char_length": 1490
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 24,
    "text": "Perng,BlakeHechtman,ParkerSchuh,MiladNasr, theAssociationforComputationalLinguistics: ACL\nMiaChen,KieranMilan,VladimirMikulik,Trevor 2023,pages10381–10399,Toronto,Canada.Associ-\nStrohman, JulianaFranco,TimGreen, DemisHas- ationforComputationalLinguistics.\nsabis,KorayKavukcuoglu,JeffreyDean,andOriol\nVinyals.2023. Gemini: Afamilyofhighlycapable Fangyu Liu, Francesco Piccinno, Syrine Krichene,\nmultimodalmodels. Preprint,arXiv:2312.11805. ChenxiPang,KentonLee,MandarJoshi,Yasemin\nAltun,NigelCollier,andJulianMartinEisenschlos.\n5929\n2022. Matcha: Enhancingvisuallanguagepretrain- Cummings, Jeremiah Currier, Yunxing Dai, Cory\ningwithmathreasoningandchartderendering. arXiv Decareaux,ThomasDegry,NoahDeutsch,Damien\npreprintarXiv:2212.09662. Deville, Arka Dhar, David Dohan, Steve Dowl-\ning, Sheila Dunning, Adrien Ecoffet, Atty Eleti,\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Tyna Eloundou, David Farhi, Liam Fedus, Niko\nLee.2023b. Improvedbaselineswithvisualinstruc- Felix, Simón Posada Fishman, Juston Forte, Is-\ntiontuning. Preprint,arXiv:2310.03744. abella Fulford, Leo Gao, Elie Georges, Christian\nGibson, Vik Goel, Tarun Gogineni, Gabriel Goh,\nYuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,\nRapha Gontijo-Lopes, Jonathan Gordon, Morgan\nSongyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi\nGrafstein, ScottGray, RyanGreene, JoshuaGross,\nWang,ConghuiHe,ZiweiLiu,KaiChen,andDahua\nShixiangShaneGu,YufeiGuo,ChrisHallacy,Jesse\nLin.2023c. Mmbench: Isyourmulti-modalmodel",
    "char_length": 1469
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 25,
    "text": "Han, Jeff Harris, Yuchen He, Mike Heaton, Jo-\nanall-aroundplayer? Preprint,arXiv:2307.06281.\nhannesHeidecke,ChrisHesse,AlanHickey,Wade\nHickey,PeterHoeschele,BrandonHoughton,Kenny\nPanLu,HritikBansal,TonyXia,JiachengLiu,Chun-\nHsu,ShengliHu,XinHu,JoostHuizinga,Shantanu\nyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-\nJain,ShawnJain,JoanneJang,AngelaJiang,Roger\nWeiChang,MichelGalley,andJianfengGao.2024.\nJiang,HaozhunJin,DennyJin,ShinoJomoto,Billie\nMathvista: Evaluating mathematical reasoning of\nJonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser,\nfoundation models in visual contexts. In Inter-\nAli Kamali, Ingmar Kanitscheider, Nitish Shirish\nnational Conference on Learning Representations\nKeskar,TabarakKhan,LoganKilpatrick,JongWook\n(ICLR).\nKim, ChristinaKim, YongjikKim, HendrikKirch-\nPanLu,RanGong,ShibiaoJiang,LiangQiu,Siyuan ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,\nHuang,XiaodanLiang,andSong-ChunZhu.2021. Łukasz Kondraciuk, Andrew Kondrich, Aris Kon-\nInter-gps: Interpretable geometry problem solv- stantinidis, Kyle Kosic, Gretchen Krueger, Vishal\ning with formal language and symbolic reasoning. Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan\nPreprint,arXiv:2105.04165. Leike, Jade Leung, Daniel Levy, Chak Ming Li,\nRachel Lim, Molly Lin, Stephanie Lin, Mateusz\nAhmedMasry,XuanLongDo,JiaQingTan,ShafiqJoty, Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue,\nandEnamulHoque.2022. ChartQA:Abenchmark AnnaMakanju,KimMalfacini,SamManning,Todor",
    "char_length": 1447
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 26,
    "text": "forquestionansweringaboutchartswithvisualand Markov, Yaniv Markovski, Bianca Martin, Katie\nlogicalreasoning. InFindingsoftheAssociationfor Mayer,AndrewMayne,BobMcGrew,ScottMayer\nComputationalLinguistics: ACL2022,pages2263– McKinney, Christine McLeavey, Paul McMillan,\n2279,Dublin,Ireland.AssociationforComputational Jake McNeil, David Medina, Aalok Mehta, Jacob\nLinguistics. Menick, Luke Metz, Andrey Mishchenko, Pamela\nMishkin, Vinnie Monaco, Evan Morikawa, Daniel\nRThomasMcCoy,ElliePavlick,andTalLinzen.2019.\nMossing,TongMu,MiraMurati,OlegMurk,David\nRight for the wrong reasons: Diagnosing syntac-\nMély,AshvinNair,ReiichiroNakano,RajeevNayak,\ntic heuristics in natural language inference. arXiv\nArvindNeelakantan,RichardNgo,HyeonwooNoh,\npreprintarXiv:1902.01007.\nLongOuyang,CullenO’Keefe,JakubPachocki,Alex\nPaino, Joe Palermo, Ashley Pantuliano, Giambat-\nNiteshMethani,PrithaGanguly,MiteshM.Khapra,and\ntistaParascandolo,JoelParish,EmyParparita,Alex\nPratyushKumar.2019. Datainterpretationoverplots.\nPassos,MikhailPavlov,AndrewPeng,AdamPerel-\nCoRR,abs/1909.00997.\nman,FilipedeAvilaBelbutePeres,MichaelPetrov,\nNiteshMethani,PrithaGanguly,MiteshMKhapra,and Henrique Ponde de Oliveira Pinto, Michael, Poko-\nPratyushKumar.2020. Plotqa: Reasoningoversci- rny, Michelle Pokrass, Vitchyr Pong, Tolly Pow-\nentificplots. InProceedingsoftheIEEE/CVFWin- ell, Alethea Power, Boris Power, Elizabeth Proehl,\nterConferenceonApplicationsofComputerVision, RaulPuri,AlecRadford,JackRae,AdityaRamesh,",
    "char_length": 1482
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 27,
    "text": "pages1527–1536. CameronRaymond,FrancisReal,KendraRimbach,\nCarl Ross, Bob Rotsted, Henri Roussez, Nick Ry-\nOpenAI,:,JoshAchiam,StevenAdler,SandhiniAgar- der,MarioSaltarelli,TedSanders,ShibaniSanturkar,\nwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoniAle- GirishSastry,HeatherSchmidt,DavidSchnurr,John\nman,DiogoAlmeida,JankoAltenschmidt,SamAlt- Schulman, Daniel Selsam, Kyla Sheppard, Toki\nman,ShyamalAnadkat,RedAvila,IgorBabuschkin, Sherbakov, Jessica Shieh, Sarah Shoker, Pranav\nSuchirBalaji,ValerieBalcom,PaulBaltescu,Haim- Shyam,SzymonSidor,EricSigler,MaddieSimens,\ning Bao, Mo Bavarian, Jeff Belgum, Irwan Bello, JordanSitkin,KatarinaSlama,IanSohl,Benjamin\nJake Berdine, Gabriel Bernadett-Shapiro, Christo- Sokolowsky, Yang Song, Natalie Staudacher, Fe-\npherBerner,LennyBogdonoff,OlegBoiko,Made- lipePetroskiSuch,NatalieSummers,IlyaSutskever,\nlaineBoyd,Anna-LuisaBrakman,GregBrockman, JieTang,NikolasTezak,MadeleineThompson,Phil\nTimBrooks,MilesBrundage,KevinButton,Trevor Tillet, Amin Tootoonchian, Elizabeth Tseng, Pre-\nCai,RosieCampbell,AndrewCann,BrittanyCarey, ston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-\nChelsea Carlson, Rory Carmichael, Brooke Chan, lipeCerónUribe,AndreaVallone,ArunVijayvergiya,\nCheChang,FotisChantzis,DerekChen,SullyChen, ChelseaVoss,CarrollWainwright,JustinJayWang,\nRuby Chen, Jason Chen, Mark Chen, Ben Chess, AlvinWang,BenWang,JonathanWard,JasonWei,\nChesterCho,CaseyChu,HyungWonChung,Dave CJWeinmann,AkilaWelihinda,PeterWelinder,Ji-\n5930",
    "char_length": 1465
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 28,
    "text": "ayiWeng,LilianWeng,MattWiethoff,DaveWillner, A DataCreation\nClemens Winter, Samuel Wolrich, Hannah Wong,\nLauren Workman, Sherwin Wu, Jeff Wu, Michael SyntheticPlot-question Thissubsetcomprises\nWu,KaiXiao,TaoXu,SarahYoo,KevinYu,Qim- a series of scatter plots, bar plots and pie charts\ningYuan,WojciechZaremba,RowanZellers,Chong\neachcreatedusingtheMatplotliborjavascriptli-\nZhang, Marvin Zhang, Shengjia Zhao, Tianhao\nbraries. These plot types are identical to those\nZheng, Juntang Zhuang, William Zhuk, and Bar-\nret Zoph. 2023. Gpt-4 technical report. Preprint, foundintheChartQAdataset,ensuringconsistency\narXiv:2303.08774. andrelevanceinouranalysis. Theseincludescatter\nplots,barcharts,andpiecharts. Wehavethencare-\nMarcoTulioRibeiro,TongshuangWu,CarlosGuestrin,\nand Sameer Singh. 2020. Beyond accuracy: Be- fullyformulatedasetoffundamentalquestionsfor\nhavioraltestingofNLPmodelswithCheckList. In eachplottypeaimedatbasicvisualunderstanding.\nProceedingsofthe58thAnnualMeetingoftheAsso- Wecreateallprimarysubsetsofthesyntheticdata\nciationforComputationalLinguistics,pages4902–\nusingtheMatplotliblibrary. Webeginbyautomat-\n4912, Online. Association for Computational Lin-\nicallygenerating50plotsforeachsubset,followed\nguistics.\nbyamanualreviewofeachplottoensuretheymeet\nSina Rismanchian, Yasaman Razeghi, Sameer Singh,\nourqualitystandardsandarefreefromambiguity.\nandShayanDoroudi.2024. Turtlebench: Avisual\nThenumberofquestionsforeachsubsetisinTable\nprogrammingbenchmarkinturtlegeometry. ArXiv",
    "char_length": 1493
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 29,
    "text": "preprint. 7.\nHrituraj Singh and Sumit Shekhar. 2020. STL-CQA: ScatterPlots Wedepictstraightforwardmathe-\nStructure-based transformers with localization and\nmaticalfunctions,suchasx = yx = 2x,etc.,each\nencodingforchartquestionanswering. InProceed-\nrepresentedusing25defaultbluemarkerpointsin\ningsofthe2020ConferenceonEmpiricalMethods\nin Natural Language Processing (EMNLP), pages scatter plots for the main subset. Each plot is ac-\n3275–3284,Online.AssociationforComputational companied by eight corresponding direct simple\nLinguistics. questionsfocusingontheminimumandmaximum\nvaluesandrangesforthexandyaxes. Theseques-\nWeihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi\nHong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, tionsaredetailedinTable6.\nLeiZhao,XixuanSong,JiazhengXu,BinXu,Juanzi\nLi, Yuxiao Dong, Ming Ding, and Jie Tang. 2024. BarCharts Weautomaticallycreatebarcharts\nCogvlm: Visualexpertforpretrainedlanguagemod- with the default blue Matplotlib library. We ran-\nels. Preprint,arXiv:2311.03079.\ndomly sample the number of bars for each chart,\nRenqiuXia,BoZhang,HanchengYe,XiangchaoYan, rangingfrom1to5,andassignthevaluesofeach\nQiLiu, HongbinZhou, ZijunChen, MinDou, Bo- barrandomlywithintherangeof[-200,200]. Each\ntian Shi, Junchi Yan, and Yu Qiao. 2024. Chartx\nplot is accompanied by five corresponding direct\n&chartvlm: Aversatilebenchmarkandfoundation\nsimple questions focusing on the minimum and\nmodel for complicated chart reasoning. Preprint,",
    "char_length": 1450
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 30,
    "text": "arXiv:2402.12185. maximumvaluesofthebarsandrangesforthey\naxes. ThesequestionsaredetailedinTable6.\nPeng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao,\nShuoLiu,MengLei,FanqingMeng,SiyuanHuang, PieCharts Weautomaticallygeneratepiecharts\nYuQiao,andPingLuo.2023. Lvlm-ehub: Acom-\nwiththenumberofcategoriesrandomlysampled\nprehensive evaluation benchmark for large vision-\nbetween 1 and 10. We ensure that each pie chart\nlanguagemodels. Preprint,arXiv:2306.09265.\nrepresentsatotalsumvalueof100%,whichisthe\nWeihaoYu,ZhengyuanYang,LinjieLi,JianfengWang,\nmostcommonusecaseforpiecharts. Theactual\nKevin Lin, Zicheng Liu, Xinchao Wang, and Li-\nvaluesareexplicitlywrittenwithinthecategories.\njuan Wang. 2023. Mm-vet: Evaluating large mul-\ntimodalmodelsforintegratedcapabilities. Preprint, ThesequestionsaredetailedinTable6.\narXiv:2308.02490.\nReal World Plots In this subset, we randomly\nXiaomanZhang,ChaoyiWu,ZihengZhao,Weixiong\nsampled plots from the ChartQA test set and\nLin,YaZhang,YanfengWang,andWeidiXie.2023.\nadaptedourquestionstotheseplots. Thequestions\nPmc-vqa:Visualinstructiontuningformedicalvisual\nquestionanswering. Preprint,arXiv:2305.10415. wereminimallyeditedtoensureeachquestion’srel-\nevancetothespecificcharttypeandcontext. The\nDeyaoZhu, JunChen, XiaoqianShen, XiangLi, and\ngroundtruthanswerswerethenincluded. During\nMohamedElhoseiny.2023. Minigpt-4: Enhancing\ntheannotationprocess,werandomlyselectedplots\nvision-languageunderstandingwithadvancedlarge",
    "char_length": 1450
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 31,
    "text": "languagemodels. Preprint,arXiv:2304.10592. from the ChartQA test sets and ensured that our\n5931\naddedquestionsmeettwocriteria: 1. Theyinvolve lama, we adjust the temperature to 0.7, based on\nminimal modifications from our set of questions preliminarytestsindicatingoptimizedperformance\nin the synthetic set, and 2. They are devoid of atthissetting. Thismethodensuresthatourevalua-\nambiguity. Thissubsetiscreatedmanually,result- tionreflectsboththerobustnessandthereal-world\ningin218questionson70differentplotsfromthe applicabilityofthesemultimodalmodels.\nChartQA human-annotated test set. For compar-\nB.2 ModelSizes\nison, examples of comparison between our addi-\ntional Questions and ChartQA test questions are WeincludeGeminiProVision(Gemini-Teametal.,\npresentedinFigure4. 2023), GPT-4V (OpenAI et al., 2023), PaLI-3\n(Chenetal.,2023),ChartLlama(Hanetal.,2023)\nRobustnessTestsSubsets Ourprimarydataset,\nandCogVLM(Wangetal.,2024)modelsinourem-\naspreviouslyoutlined,consistsoftwodistinctsub-\npiricalanalysis. WeuseGemini1.0ProVision,and\nsets. These subsets bridge synthetic chart under-\nGPT-4V through their APIs. All experiments for\nstandingquestionswithreal-worldscenarios,aim-\nthesetwo modelsareperformedin thefirstweek\ning to evaluate models’ fundamental abilities to\nof April 2024 (mentioning the data as the mod-\ncomprehend charts. Another fundamental aspect\nels behind APIs can change over time). We use\nofimagecomprehension,especiallywithcharts,is\nChartLlama-13BandCogVLM-17B.ThePaLI-3",
    "char_length": 1492
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 32,
    "text": "theresilienceofmodelstoinvariantvisualchanges\nmodelisofsize5B.\nin the plots. These alterations do not modify the\ncharts’informationalcontentbutsolelyaffecttheir B.3 Prompts\nvisual presentation, such as the libraries used for For all our question-answering tasks, we use the\nplot creation or color variations. Our synthetic prompt\"AnswerthequestionbasedontheFigure\nsubsetspecificallyfacilitatesacomprehensiveeval- + [Question].\" for PaLI-3, CogVLM and Chartl-\nuationofmodelrobustnessagainstthesechanges. Lamma. Fortherobustnesstestofexploringmod-\nByintroducingtargetedmodificationstovisualas- els’dependenceontextualcuesintheplot,wefur-\npectsofcharts,wecanassessmodelperformance theremphasizethefigurebychangingtheprompt\nin maintaining accurate interpretation despite su- to\"Answerthequestiononlybasedonthefigure\nperficialalterations. Thisevaluationiscrucialfor +[Question].\"Forautomatedextractionofthean-\ndetermining the real-world utility and robustness swers, we instruct Gemini Pro Vision and GPT\nofmultimodalmodelsinchartcomprehension. To withanotherpromptaspresentedinFigure2.\nthat end, we create multiple additions to the sub-\nset,changingonespecificvisualpartsofthecharts\nPleaseanswerthefollowingquestionbasedonthe\nto study the models robustness to such changes. plot/figurewiththeresponseinthesameformat:\nTheseeditsincludechangingthechoiceofplotli- \"The answer is ANS. I hope the answer\nis correct.\"InwhichtheANSrepresentsthe\nbrary, changing the markers of the plots, adding",
    "char_length": 1483
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 33,
    "text": "correctanswer. Beconciseandaccurateinyour\nor removing grids, adding misleading text in the reply.\nchartsetc.\nB ExperimentalDetails Figure2: PromptforGeminiandGPTModels\nAlltheexperimentsinthispaperareperformedin\nApril2024. B.4 AutomatedEvaluation\nTofacilitateautomatedevaluation,weinstructall\nB.1 SamplingMethod\nmodelstoformattheirrespon‘sesinaspecificstruc-\nOursamplingmethodforassessingmodelperfor- ture: \"The answer is ANSWER.\" While Gemini\nmance involves querying each model five times ProVisionandGPT-4Vconsistentlyadheretothis\nwith the same set of questions and averaging the format, theothermodelsfrequentlydeviatefrom\nobtainedmetricstomitigatetheeffectsofnonde- it. To address this inconsistency, we employ the\nterminism inherent in model responses. For the GPT-3turbomodeltoreformattheresponsesinto\nmodelsPaLI-3,GeminiProVision,andGPT-4V, therequiredstructurebeforeextractingtheanswers.\nweutilizethedefaulttemperaturesettingtoclosely Thisadditionalstepensuresuniformityinresponse\nmirrortheirtypicalusageinreal-worldapplications. formattingacrossalltestedmodels,enablingmore\nConversely,forthemodelsCogVLMandChartL- accurateautomatedanalysis. PromptisinFigure3\n5932\nSyntheticsubset Questions\nscatterplots Whatisthemaximum/minimumvalueamongthesetofpointsplotted\ninthefigure?\nWhatistheapproximatemaximum/minimumvaluefortherangeon\nthex/y-axis?\nbarchart Whatisthemaximum/minimumvalueamongthesetofbarsplotted\ninthefigure?\nHowmanybarsareinthefigure?",
    "char_length": 1450
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 34,
    "text": "Whatisthemaximum/minimumvaluefortherangeonthey-axis?\npiechart Whatisthemaximum/minimumvalueamongthesetofcategoriesin\nthefigure?\nHowmanycategoriesarepresented?\nTable6: Questiontemplatesforsyntheticdataset\nSynthetic Number of modelresponsesandguidingthedevelopmentof\nsubset Questions effectivesolutions.\nscatter 336\nplots\nbarchart 250\npiechart 141\nTable7: Questiontemplatesforsyntheticdataset\nC MoreAnalysis\nWhat type of questions are the hardest? Fol-\nlowingtheapproachofsegmentingmodelperfor-\nmancebyplottypes,ouruseofsyntheticdatafa-\ncilitatesasimilaranalysisbasedonquestiontypes.\nThisapproachenablesustoevaluateandpinpoint\ntheparticularperformancecharacteristicsofeach\nmodel,allowingforadetailedinvestigationintothe\ndistinctbehavioralpatternsandchallengesmodels\nexhibitwhenrespondingtodifferentkindsofques-\ntions. Theoutcomesofthisquestion-type-specific\nperformanceevaluationarepresentedinFigures9\nand8,whichdetailthedistinctrange-basedaccu-\nracy and response patterns of each model across\nthe range of question types examined. For exam-\nple,wefirstobservedistinctbehaviorsamongthe\nmodels: whileGPT-4Vdemonstratesitsstrongest\nperformanceonquestionsconcerningtheminimum\nrangeonthex-axis,GeminiProVisionandPaLI-3\nstrugglethemostwiththistypeofquestionwhen\ndealing with scatter plots. As discussed earlier,\nthesedetailedinsightsintomodels’specificlimita-\ntions are vital for understanding the reliability of\n5933\nExtracttheconciseanswerfromthemodel’sresponse",
    "char_length": 1457
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 35,
    "text": "as shown in the examples below, making sure the\nanswerisinthisformat: \"The answer is ANS. I\nhope the answer is correct.\"\nExample1:\nQuestion: \"How many food items are\nshowninthebargraph?\"\nModelAnswer:\"<extra_id_0> 0\"\nExtractedAnswer: Theansweris0. I\nhopetheansweriscorrect.\nExample2:\nQuestion: \"How many bars are in the\nfigure?\"\nModelAnswer:\"<extra_id_0> There\nare three bars in the figure.\"\nExtractedAnswer:Theansweristhree.\nIhopetheansweriscorrect.\nExample3:\nQuestion: \"Findmissingdataofthese-\nquence24,_,32,33,42?\"\nModelAnswer:\"<extra_id_0> 33\"\nExtractedAnswer:Theansweris33.I\nhopetheansweriscorrect.\nExample4:\nQuestion: \"Which country has the\nhighest secondary graduation rate in\n2018?\"\nModel Answer: \"<extra_id_0>\nItaly\"\nExtractedAnswer:TheanswerisItaly.\nIhopetheansweriscorrect.\nYourTask:\nGiventhequestionandmodelanswerbe-\nlow,extracttheconciseanswer.\nQuestion:\"{question}\"\nModelAnswer:\"{model_raw_output}\"\nExtractedAnswer:\nFigure3: PromptforExtractionofAnswersforAuto-\nmatedEvaluation\n5934\nFigure 4: More examples of questions from our simplified dataset alongside those from the ChartQA dataset,\nwithcorrespondingquestionsfromourset. ChartQAhuman-writtenquestionsvaryincomplexity,fromthemore\nstraightforward at the bottom of the example to those requiring complex reasoning at the top. In contrast, our\nquestionsareconsistentlystructuredtobesimple.\nFigure5: Examplesofquestionsfromoursyntheticdatasetforthebarcharts\n5935\nFigure6: Examplesofquestionsfromoursyntheticdatasetforthepiecharts",
    "char_length": 1495
  },
  {
    "paper_id": "plot_twist",
    "chunk_id": 36,
    "text": "Figure7: Examplesofquestionsfromoursyntheticdatasetforthescatterplot\n5936\nFigure8: Radarchartdepictingtherange-basedaccuracyofdifferentmodelsinresponsetovariousquestiontypes\ninourpiecharts.\n(a)SyntheticBarChartQuestions (b)SyntheticScatterPlotQuestions\nFigure9: Radarchartdepictingtherange-basedaccuracyofdifferentmodelsinresponsetovariousquestiontypes,\nhighlightingthedistinctlimitationseachmodelexhibitswithrespecttospecifictypesofquestions. ,\n5937",
    "char_length": 451
  }
]